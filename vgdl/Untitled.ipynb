{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e6293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431e9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e9c4dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored interfaces.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: interfaces.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- interfaces.py\t(original)\n",
      "+++ interfaces.py\t(refactored)\n",
      "@@ -13,9 +13,9 @@\n",
      " from pybrain.rl.environments.environment import Environment\n",
      " from pybrain.rl.environments.episodic import EpisodicTask\n",
      " \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite\n",
      "-from stateobs import StateObsHandler \n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite\n",
      "+from .stateobs import StateObsHandler \n",
      " \n",
      "         \n",
      "     \n",
      "@@ -136,7 +136,7 @@\n",
      "         if init_state is not None:\n",
      "             self.setState(init_state)\n",
      "         for a in action_sequence:\n",
      "-            print a, self.getState()\n",
      "+            print(a, self.getState())\n",
      "             if self._isDone()[0]:\n",
      "                 break\n",
      "             self.performAction(a)\n",
      "@@ -181,7 +181,7 @@\n",
      " \n",
      " def testRollout(actions=[0, 0, 2, 2, 0, 3] * 20):        \n",
      "     from examples.gridphysics.mazes import polarmaze_game, maze_level_1\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     game_str, map_str = polarmaze_game, maze_level_1\n",
      "     g = VGDLParser().parseGame(game_str)\n",
      "     g.buildLevel(map_str)    \n",
      "@@ -191,8 +191,8 @@\n",
      "     \n",
      " def testRolloutVideo(actions=[0, 0, 2, 2, 0, 3] * 2):        \n",
      "     from examples.gridphysics.mazes import polarmaze_game, maze_level_1\n",
      "-    from core import VGDLParser\n",
      "-    from tools import makeGifVideo\n",
      "+    from .core import VGDLParser\n",
      "+    from .tools import makeGifVideo\n",
      "     game_str, map_str = polarmaze_game, maze_level_1\n",
      "     g = VGDLParser().parseGame(game_str)\n",
      "     g.buildLevel(map_str)\n",
      "@@ -202,7 +202,7 @@\n",
      " def testInteractions():\n",
      "     from random import randint\n",
      "     from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     from examples.gridphysics.mazes import polarmaze_game, maze_level_1    \n",
      "     from pybrain.rl.agents.agent import Agent\n",
      "     \n",
      "@@ -221,13 +221,13 @@\n",
      "     agent = DummyAgent()\n",
      "     exper = EpisodicExperiment(task, agent)\n",
      "     res = exper.doEpisodes(2)\n",
      "-    print res\n",
      "+    print(res)\n",
      " \n",
      " def testPolicyAgent():\n",
      "     from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     from examples.gridphysics.mazes import polarmaze_game, maze_level_2\n",
      "-    from agents import PolicyDrivenAgent\n",
      "+    from .agents import PolicyDrivenAgent\n",
      "     game_str, map_str = polarmaze_game, maze_level_2\n",
      "     g = VGDLParser().parseGame(game_str)\n",
      "     g.buildLevel(map_str)\n",
      "@@ -239,14 +239,14 @@\n",
      "     env.reset()\n",
      "     exper = EpisodicExperiment(task, agent)\n",
      "     res = exper.doEpisodes(2)\n",
      "-    print res\n",
      "+    print(res)\n",
      "     \n",
      " def testRecordingToGif(human=False):\n",
      "     from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     from examples.gridphysics.mazes import polarmaze_game, maze_level_2\n",
      "-    from agents import PolicyDrivenAgent, InteractiveAgent    \n",
      "-    from tools import makeGifVideo\n",
      "+    from .agents import PolicyDrivenAgent, InteractiveAgent    \n",
      "+    from .tools import makeGifVideo\n",
      "     \n",
      "     game_str, map_str = polarmaze_game, maze_level_2\n",
      "     g = VGDLParser().parseGame(game_str)\n",
      "@@ -259,17 +259,17 @@\n",
      "         agent = PolicyDrivenAgent.buildOptimal(env)\n",
      "     exper = EpisodicExperiment(task, agent)\n",
      "     res = exper.doEpisodes(1)\n",
      "-    print res\n",
      "+    print(res)\n",
      "     \n",
      "     actions = [a for _, a, _ in env._allEvents]\n",
      "-    print actions\n",
      "+    print(actions)\n",
      "     makeGifVideo(env, actions, initstate=env._initstate)\n",
      "     \n",
      " def testAugmented():\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-    from mdpmap import MDPconverter\n",
      "-    from agents import PolicyDrivenAgent    \n",
      "+    from .mdpmap import MDPconverter\n",
      "+    from .agents import PolicyDrivenAgent    \n",
      "     \n",
      "     \n",
      "     zelda_level2 = \"\"\"\n",
      "@@ -290,9 +290,9 @@\n",
      "                           recordingEnabled=True, actionDelay=150)\n",
      "     C = MDPconverter(g, env=env, verbose=True)\n",
      "     Ts, R, _ = C.convert()\n",
      "-    print C.states\n",
      "-    print Ts[0]\n",
      "-    print R\n",
      "+    print(C.states)\n",
      "+    print(Ts[0])\n",
      "+    print(R)\n",
      "     env.reset()\n",
      "     agent = PolicyDrivenAgent.buildOptimal(env)\n",
      "     env.visualize = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored goal_programming.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: goal_programming.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- goal_programming.py\t(original)\n",
      "+++ goal_programming.py\t(refactored)\n",
      "@@ -29,7 +29,7 @@\n",
      "     rle._other_types.append(spriteType)\n",
      "     rle._game.added_sprites.append(s)\n",
      "     if spriteType not in rle.symbolDict:\n",
      "-        idx=len(rle.symbolDict.keys())\n",
      "+        idx=len(list(rle.symbolDict.keys()))\n",
      "         rle.symbolDict[spriteType]=ALNUM[idx]\n",
      "     \n",
      "     for skey in rle._other_types:\n",
      "@@ -44,8 +44,8 @@\n",
      "     max_num = max([int(c[1:]) for c in existing_classes])\n",
      "     class_num = max_num+1\n",
      "     new_class_name = 'c'+str(class_num)\n",
      "-    used_colors = theory.spriteObjects.keys()\n",
      "-    color = next((c for c in colorDict.itervalues() if c not in used_colors), None)\n",
      "+    used_colors = list(theory.spriteObjects.keys())\n",
      "+    color = next((c for c in colorDict.values() if c not in used_colors), None)\n",
      "     return new_class_name, color\n",
      " \n",
      " class GoalAgent(Agent):\n",
      "@@ -97,7 +97,7 @@\n",
      "         h.interactionSet = [rule for rule in h.interactionSet \n",
      "                             if not(rule.slot1 == 'avatar' or rule.slot2 == 'avatar' or\n",
      "                             rule.slot1 == 'EOS' and rule.slot2 == new_name)]\n",
      "-        for (o1, o2) in itertools.product(['avatar'], h.classes.keys()):\n",
      "+        for (o1, o2) in itertools.product(['avatar'], list(h.classes.keys())):\n",
      "             if o2 == new_name:\n",
      "                 continue\n",
      "             kill_rule = InteractionRule('killSprite', o1, o2, {}, set())\n",
      "@@ -112,11 +112,11 @@\n",
      " \n",
      "         newenv = self.initializeVrle(h)\n",
      "         board = numpy.zeros(newenv.outdim)\n",
      "-        for locs in newenv._game.sprite_groups.itervalues():\n",
      "+        for locs in newenv._game.sprite_groups.values():\n",
      "             for loc in locs:\n",
      "                 board[loc.y / 30, loc.x / 30] = 1\n",
      "-        for i in xrange(newenv.outdim[1]):\n",
      "-            for j in xrange(newenv.outdim[0]):\n",
      "+        for i in range(newenv.outdim[1]):\n",
      "+            for j in range(newenv.outdim[0]):\n",
      "                 if board[j][i] == 0:\n",
      "                     addNewSprite(newenv, new_name, (i * 30, j * 30))\n",
      " \n",
      "@@ -178,7 +178,7 @@\n",
      "                 episodes.append((n_level, steps, win, score))\n",
      "                 episodes_played += 1\n",
      "                 if win:\n",
      "-                    print 'won'\n",
      "+                    print('won')\n",
      "                     break\n",
      "                 i += 1\n",
      "             if i < num_episodes_per_level:\n",
      "@@ -192,18 +192,18 @@\n",
      "     def playGoalEpisode(self, n_level, episode_num, flexible_goals=False, win=False,alt_rle=None):\n",
      " \n",
      " \n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "-        print \"INSIDE OF PLAY GOAL EPISODE\"\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      "+        print(\"INSIDE OF PLAY GOAL EPISODE\")\n",
      " \n",
      "         self.initializeEnvironment()\n",
      " \n",
      "@@ -229,7 +229,7 @@\n",
      "             else:\n",
      "                 plan_rle = self.rle\n",
      " \n",
      "-            planner_hyperparameters = dict((k, self.hyperparameters[k]) for k in self.hyperparameters.keys() if k not in ['idx', 'short_horizon', 'first_order_horizon'])\n",
      "+            planner_hyperparameters = dict((k, self.hyperparameters[k]) for k in list(self.hyperparameters.keys()) if k not in ['idx', 'short_horizon', 'first_order_horizon'])\n",
      " \n",
      "             ## Initialize planner\n",
      "             p = WBP(plan_rle, self.gameFilename, theory=self.theory, fakeInteractionRules = self.fakeInteractionRules,\n",
      "@@ -250,11 +250,11 @@\n",
      "                 solution = []\n",
      " \n",
      "             if solution and not p.quitting:\n",
      "-                print \"=============================================\"\n",
      "-                print \"got solution of length\", len(solution)\n",
      "+                print(\"=============================================\")\n",
      "+                print(\"got solution of length\", len(solution))\n",
      "                 for g in p.gameString_array:\n",
      "-                    print colored(g, 'green')\n",
      "-                print \"=============================================\"\n",
      "+                    print(colored(g, 'green'))\n",
      "+                print(\"=============================================\")\n",
      " \n",
      "             if self.shortHorizon:\n",
      "                 if not solution:\n",
      "@@ -264,7 +264,7 @@\n",
      "             else:\n",
      "                 if (not solution) or p.quitting:\n",
      "                     if self.longHorizonObservations<self.longHorizonObservationLimit:\n",
      "-                        print \"Didn't get solution or decided to quit. Observing, then replanning.\"\n",
      "+                        print(\"Didn't get solution or decided to quit. Observing, then replanning.\")\n",
      "                         # embed()\n",
      "                         self.wait(episode_num, num_steps=5)\n",
      "                         solution = [] ## You may have gotten p.quitting but also a solution; make sure you don't try to act on that if the planner decided it wasn't worth it.\n",
      "@@ -273,7 +273,7 @@\n",
      "                         quitting = True\n",
      " \n",
      "             if emptyPlans > self.emptyPlansLimit:\n",
      "-                print \"observing\"\n",
      "+                print(\"observing\")\n",
      "                 # embed()\n",
      "                 self.wait(episode_num, num_steps=5)\n",
      " \n",
      "@@ -300,7 +300,7 @@\n",
      "                 ## You failed the game either because you made a mistake you couldn't recover from or because you timed out in your search.\n",
      "                 ## Search more deeply next time.\n",
      "                 self.max_nodes *= self.max_nodes_annealing\n",
      "-                print \"You got quitting==True from planner. Embedding to debug.\"\n",
      "+                print(\"You got quitting==True from planner. Embedding to debug.\")\n",
      "                 # embed()\n",
      "                 return False, self.rle._game.score, steps\n",
      "         \n",
      "@@ -310,22 +310,22 @@\n",
      "         score = self.rle._game.score\n",
      "         output = \"ended episode. Win={}                   \t\t\t\t\t\t  \".format(win)\n",
      "         if win:\n",
      "-            print colored('________________________________________________________________', 'white', 'on_green')\n",
      "-            print colored('________________________________________________________________', 'white', 'on_green')\n",
      "-\n",
      "-            print colored(output, 'white', 'on_green')\n",
      "-            print colored('________________________________________________________________', 'white', 'on_green')\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "+\n",
      "+            print(colored(output, 'white', 'on_green'))\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "         else:\n",
      "-            print colored('________________________________________________________________', 'white', 'on_red')\n",
      "-            print colored(output, 'white', 'on_red')\n",
      "-            print colored('________________________________________________________________', 'white', 'on_red')\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_red'))\n",
      "+            print(colored(output, 'white', 'on_red'))\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_red'))\n",
      " \n",
      "         return win, score, steps\n",
      " \n",
      "     def executeStep(self, episode_num, action):\n",
      "         self.rle.step(action)\n",
      "-        print \"Game score: {}. Game tick: {}\".format(self.rle._game.score, self.rle._game.time)\n",
      "-        print self.rle.show(color='blue')\n",
      "+        print(\"Game score: {}. Game tick: {}\".format(self.rle._game.score, self.rle._game.time))\n",
      "+        print(self.rle.show(color='blue'))\n",
      "         envReal = self.fastcopy(self.rle)\n",
      "         self.statesEncountered.append(self.rle._game.getFullState())\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored theory_template.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: theory_template.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- theory_template.py\t(original)\n",
      "+++ theory_template.py\t(refactored)\n",
      "@@ -1,17 +1,18 @@\n",
      " from random import choice\n",
      " import itertools, copy, scipy.misc\n",
      " import numpy as np\n",
      "-from sampleVGDLString import *\n",
      "-from class_theory_template import *\n",
      "-from taxonomy import *\n",
      "+from .sampleVGDLString import *\n",
      "+from .class_theory_template import *\n",
      "+from .taxonomy import *\n",
      " from IPython import embed\n",
      "-from ontology import *\n",
      "+from .ontology import *\n",
      " from collections import defaultdict\n",
      " # import ipdb\n",
      " import operator\n",
      " import time, math\n",
      "-from util import factorize, objectsToSymbol\n",
      "-from rlenvironmentnonstatic import createMindEnv\n",
      "+from .util import factorize, objectsToSymbol\n",
      "+from .rlenvironmentnonstatic import createMindEnv\n",
      "+from functools import reduce\n",
      " \n",
      " ALNUM = '0123456789bcdefhijklmnpqrstuvwxyzQWERTYUIOPSDFHJKLZXCVBNM,./;[]<>?:`-=~!@#$%^&*()_+'\n",
      " AvatarTypes = [MovingAvatar, HorizontalAvatar, VerticalAvatar, FlakAvatar, AimedFlakAvatar, OrientedAvatar,\n",
      "@@ -62,7 +63,7 @@\n",
      " \t\tself.rle = rle\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint (self.agentAction, self.agentState, self.events, self.gameState)\n",
      "+\t\tprint((self.agentAction, self.agentState, self.events, self.gameState))\n",
      " \t\treturn (self.agentAction, self.agentState, self.events, self.gameState)\n",
      " \n",
      " \n",
      "@@ -78,7 +79,7 @@\n",
      " \t\tself.negated = False\n",
      " \n",
      " \tdef check(self, dictionary):\n",
      "-\t\tif self.item not in dictionary.keys():\n",
      "+\t\tif self.item not in list(dictionary.keys()):\n",
      " \t\t\tdictionary[self.item] = 0\n",
      " \n",
      " \t\tif self.operator_name == '>':\n",
      "@@ -100,7 +101,7 @@\n",
      " \t\tself.text = 'not '+ self.text\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint self.text\n",
      "+\t\tprint(self.text)\n",
      " \n",
      " \tdef __eq__(self, other):\n",
      " \t\ttry:\n",
      "@@ -128,9 +129,9 @@\n",
      " \n",
      " \tdef display(self):\n",
      " \t\tif not self.preconditions:\n",
      "-\t\t\tprint self.interaction, self.slot1, self.slot2, self.args, \"generic: {}\".format(self.generic)\n",
      "-\t\telse:\n",
      "-\t\t\tprint self.interaction, self.slot1, self.slot2, self.args, [p.text for p in self.preconditions], \"generic: {}\".format(self.generic)\n",
      "+\t\t\tprint(self.interaction, self.slot1, self.slot2, self.args, \"generic: {}\".format(self.generic))\n",
      "+\t\telse:\n",
      "+\t\t\tprint(self.interaction, self.slot1, self.slot2, self.args, [p.text for p in self.preconditions], \"generic: {}\".format(self.generic))\n",
      " \t\treturn\n",
      " \n",
      " \tdef asTuple(self):\n",
      "@@ -184,7 +185,7 @@\n",
      " \t\tself._hash = hash((self.ruleType, limit, win))\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint (self.ruleType, self.termination.limit, self.termination.win)\n",
      "+\t\tprint((self.ruleType, self.termination.limit, self.termination.win))\n",
      " \n",
      " \tdef asTuple(self):\n",
      " \t\treturn (self.ruleType, self.termination.limit, self.termination.win)\n",
      "@@ -201,7 +202,7 @@\n",
      " \t\tself._hash = hash((self.ruleType, s1, s2, win, tuple(args)))\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint self.ruleType, self.termination.s1, self.termination.s2, self.termination.win, self.termination.args\n",
      "+\t\tprint(self.ruleType, self.termination.s1, self.termination.s2, self.termination.win, self.termination.args)\n",
      " \t\treturn\n",
      " \n",
      " \tdef asTuple(self):\n",
      "@@ -216,7 +217,7 @@\n",
      " \t\tself._hash = hash((self.ruleType, limit, stype, win))\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint self.ruleType, self.termination.stype, self.termination.limit, self.termination.win\n",
      "+\t\tprint(self.ruleType, self.termination.stype, self.termination.limit, self.termination.win)\n",
      " \t\treturn\n",
      " \n",
      " \tdef asTuple(self):\n",
      "@@ -229,10 +230,10 @@\n",
      " \t\targList = dict((str(i), stype) for i, stype in enumerate(stypes))\n",
      " \t\tself.termination = MultiSpriteCounter(limit=limit,win=win, **argList)\n",
      " \t\tself.ruleType = \"MultiSpriteCounterRule\"\n",
      "-\t\tself._hash = hash((self.ruleType, limit, win, tuple(sorted(argList.iteritems()))))\n",
      "+\t\tself._hash = hash((self.ruleType, limit, win, tuple(sorted(argList.items()))))\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint self.ruleType, self.termination.stypes, self.termination.limit, self.termination.win\n",
      "+\t\tprint(self.ruleType, self.termination.stypes, self.termination.limit, self.termination.win)\n",
      " \t\treturn\n",
      " \n",
      " \tdef asTuple(self):\n",
      "@@ -256,7 +257,7 @@\n",
      " \n",
      " ## Helper print function\n",
      " def printInteractionSet(interactionSet):\n",
      "-\t\tprint [i.display() for i in interactionSet]\n",
      "+\t\tprint([i.display() for i in interactionSet])\n",
      " \n",
      " class Theory(object):\n",
      " \t\"\"\"\n",
      "@@ -293,7 +294,7 @@\n",
      " \n",
      " \tdef initializeSpriteSet(self, vgdlSpriteParse=False, spriteInductionResult=False):\n",
      " \t\tif not (vgdlSpriteParse or spriteInductionResult):\n",
      "-\t\t\tprint \"You must provide either a vgdlSpriteParse or the result of having performed sprite induction.\"\n",
      "+\t\t\tprint(\"You must provide either a vgdlSpriteParse or the result of having performed sprite induction.\")\n",
      " \t\t\treturn\n",
      " \t\tif vgdlSpriteParse:\n",
      " \t\t\tself.spriteSet = vgdlSpriteParse\n",
      "@@ -321,7 +322,7 @@\n",
      " \t\tdef negBin(k, r, p):\n",
      " \t\t\treturn scipy.misc.comb(k+r-1, k) * p**k * (1-p)**r\n",
      " \n",
      "-\t\tnumClasses, numRules = len(self.classes.keys()), len(self.interactionSet)\n",
      "+\t\tnumClasses, numRules = len(list(self.classes.keys())), len(self.interactionSet)\n",
      " \t\tk = phi(numClasses, numRules, .5)\n",
      " \n",
      " \t\treturn negBin(k,5,.5)\n",
      "@@ -427,7 +428,7 @@\n",
      " \t\traise Exception(\"No corresponding class found for color\")\n",
      " \n",
      " \tdef makeGameStateWithClasses(self, gameState):\n",
      "-\t\tclassGameState = {k: 0 for k in self.classes.keys()}\n",
      "+\t\tclassGameState = {k: 0 for k in list(self.classes.keys())}\n",
      " \t\tfor c in self.classes:\n",
      " \t\t\tfor s in self.classes[c]:\n",
      " \t\t\t\tif s.color in gameState:\n",
      "@@ -627,8 +628,8 @@\n",
      " \t\t# print \"interaction set:\", [i.asTuple() for i in self.interactionSet]\n",
      " \n",
      " \t\t# if verbose:\n",
      "-\t\tif (eventInRules, predictionsHappened) not in failCases.keys():\n",
      "-\t\t\tprint \"weird fail case\"\n",
      "+\t\tif (eventInRules, predictionsHappened) not in list(failCases.keys()):\n",
      "+\t\t\tprint(\"weird fail case\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \t\treturn failCases[(eventInRules, predictionsHappened)][0]\n",
      "@@ -676,7 +677,7 @@\n",
      " \t\t\t## Interactions in ontology.py that have arguments return at most two additional arguments. By convention, 'value' is always the\n",
      " \t\t\t## last of these.\n",
      " \n",
      "-\t\t\tif 'stype' in event[3].keys():\n",
      "+\t\t\tif 'stype' in list(event[3].keys()):\n",
      " \t\t\t\tobj3 = self.spriteObjects[event[3]['stype']]\n",
      " \t\t\t\t# event[3]['stype'] = self.getClass(obj3)\n",
      " \t\t\t\ttmpEvent = copy.deepcopy(event)\n",
      "@@ -733,7 +734,7 @@\n",
      " \n",
      " \t\tclassPair = (self.getClass(obj1), self.getClass(obj2))\n",
      " \t\t# If object classes are currently being modified in the same timestep, obtain the same preconditions as before\n",
      "-\t\tif classPair in self.inModification.keys():\n",
      "+\t\tif classPair in list(self.inModification.keys()):\n",
      " \t\t\tp = self.inModification[classPair]\n",
      " \t\t\tinterpretation = self.interpret(event)\n",
      " \t\t\tinterpretation.addPrecondition(p) #TODO: maybe you should be only doing this if interpreting worked in the line above.\n",
      "@@ -743,7 +744,7 @@\n",
      " \t\telse:\n",
      " \t\t\t# Create possible preconditions\n",
      " \t\t\tconcepts = []\n",
      "-\t\t\tfor k in timestep.agentState.keys():\n",
      "+\t\t\tfor k in list(timestep.agentState.keys()):\n",
      " \t\t\t\tconcepts.extend(self.generateNumberConcepts(k, timestep.agentState[k])) #TODO: Combine generateNumberConcpets and makePreconditions\n",
      " \t\t\tgeneratedPreconditions = self.makePreconditions(concepts)\n",
      " \t\t\tfor p in generatedPreconditions:\n",
      "@@ -817,7 +818,7 @@\n",
      " \t\tself.classes[newSpriteName] = [sprite]\n",
      " \t\tself.spriteSet.append(sprite)\n",
      " \t\tself.spriteObjects[color] = sprite\n",
      "-\t\tfor (o1,o2) in itertools.product([newSpriteName], self.classes.keys()):\n",
      "+\t\tfor (o1,o2) in itertools.product([newSpriteName], list(self.classes.keys())):\n",
      " \t\t\tif o2=='avatar':\n",
      " \t\t\t\trule1 = InteractionRule('killSprite', o1, o2, {}, set(), generic=True)\n",
      " \t\t\telse:\n",
      "@@ -863,7 +864,7 @@\n",
      " \n",
      " \t\t# Check if there is an extra value argument in event. Also if there's an stype arg, get its class.\n",
      " \t\ttry:\n",
      "-\t\t\tif 'stype' in event[3].keys():\n",
      "+\t\t\tif 'stype' in list(event[3].keys()):\n",
      " \t\t\t\tobj3 = self.spriteObjects[event[3]['stype']]\n",
      " \t\t\t\t# event[3]['stype'] = self.getClass(obj3)\n",
      " \t\t\t\ttmpEvent = copy.deepcopy(event)\n",
      "@@ -892,7 +893,7 @@\n",
      " \t\tGiven a list of Interaction rules, will add a negation to each rule, if rule is not in drying paint.\n",
      " \t\t\"\"\"\n",
      " \t\tif len(unfulfilledPredictions) > 0:\n",
      "-\t\t\tprint \"in negatePreconditions\"\n",
      "+\t\t\tprint(\"in negatePreconditions\")\n",
      " \t\t\tembed()\n",
      " \t\t\t# Iterate through relevant rules, negate them if they're not in the drying paint\n",
      " \t\t\tfor r in unfulfilledPredictions:\n",
      "@@ -965,7 +966,7 @@\n",
      " \t\t\tAdds object-class assignments; avoids duplicates.\n",
      " \t\t\t'''\n",
      " \t\t\tc, o = classObjectPair[0], classObjectPair[1]\n",
      "-\t\t\tif c in self.classes.keys():\n",
      "+\t\t\tif c in list(self.classes.keys()):\n",
      " \t\t\t\tif o not in self.classes[c]:\n",
      " \t\t\t\t\tself.classes[c].append(o)\n",
      " \t\t\t\t\treturn True\n",
      "@@ -1033,7 +1034,7 @@\n",
      " \n",
      " \n",
      " \t\tif rle and not rle._isDone()[0]:\n",
      "-\t\t\tknownColors = [sprite[0].color for sprite in self.classes.values()]\n",
      "+\t\t\tknownColors = [sprite[0].color for sprite in list(self.classes.values())]\n",
      " \t\t\tpresentColors = [rle._game.sprite_groups[o][0].colorName for o in rle._game.sprite_groups\n",
      " \t\t\t\t\t\t\t if (len(rle._game.sprite_groups[o]) >\n",
      " \t\t\t\t\t\t\t\tlen([dead_sprite for dead_sprite in rle._game.kill_list if dead_sprite.name==o])) and\n",
      "@@ -1164,7 +1165,7 @@\n",
      " \t\t\"\"\"\n",
      " \t\tObtains the classes of the object; otherwise returns False if class not found.\n",
      " \t\t\"\"\"\n",
      "-\t\tfor classNum, objList in self.classes.iteritems(): #TODO: The issue is here w/ objects not found in the classes list\n",
      "+\t\tfor classNum, objList in self.classes.items(): #TODO: The issue is here w/ objects not found in the classes list\n",
      " \t\t\tfor obj2 in objList:\n",
      " \t\t\t\tif obj == obj2:\n",
      " \t\t\t\t\treturn classNum\n",
      "@@ -1239,7 +1240,7 @@\n",
      " \n",
      " \t\t# Propose new classes and classes with sprites of the same vgdlType\n",
      " \t\telif newClasses > 0:\n",
      "-\t\t\tnumClasses = len(self.classes.keys())\n",
      "+\t\t\tnumClasses = len(list(self.classes.keys()))\n",
      " \t\t\tfor i in range(1, newClasses+1):\n",
      " \t\t\t\tpossibleClasses.append('c'+str(numClasses+i)) # Classes that extend off number of existing classes\n",
      " \t\t\tgotNewClass = True\n",
      "@@ -1303,7 +1304,7 @@\n",
      " \t\t#Get class memberships\n",
      " \t\tclasses = (self.getClassFromColor(pair[0]), self.getClassFromColor(pair[1]))\n",
      " \t\tif False in classes:\n",
      "-\t\t\tprint \"Can't make predictions; theory does not contain {}\".format([el[0] for el in zip(pair, classes) if not el[1]])\n",
      "+\t\t\tprint(\"Can't make predictions; theory does not contain {}\".format([el[0] for el in zip(pair, classes) if not el[1]]))\n",
      " \t\t\treturn False\n",
      " \t\telse:\n",
      " \t\t\tpair = classes\n",
      "@@ -1357,14 +1358,14 @@\n",
      " \n",
      " \t\tremainingPredicates = list(set(predicateList)-set([rule.interaction for rule in self.interactionSet]))\n",
      " \t\tscores = [1./len(remainingPredicates)]*len(remainingPredicates)\n",
      "-\t\treturn zip(remainingPredicates, scores)\n",
      "+\t\treturn list(zip(remainingPredicates, scores))\n",
      " \n",
      " \tdef extrapolateRule(self, pair, tree, beta=1.,softmaxTemp=False):\n",
      " \t\t#returns interactionRules (including preconditions) that are already in the interactionSet\n",
      " \t\t#weighted by their similarity to the provided pair.\n",
      " \t\t#TODO: think about default softmaxTemp.\n",
      " \t\tif len(self.interactionSet)==0:\n",
      "-\t\t\tprint \"Can't extrapolate; our theory has no rules in the interactionSet!\"\n",
      "+\t\t\tprint(\"Can't extrapolate; our theory has no rules in the interactionSet!\")\n",
      " \t\t\treturn\n",
      " \t\tclassPairs = list(set([(rule.slot1, rule.slot2) for rule in self.interactionSet]))\n",
      " \t\tsimilarityScores = [self.pairSimilarity(pair, classPair, tree, beta) for classPair in classPairs]\n",
      "@@ -1373,7 +1374,7 @@\n",
      " \t\t\tsimilarityScores = softmax(similarityScores,softmaxTemp)\n",
      " \n",
      " \n",
      "-\t\tclassSimilarities = zip(classPairs, similarityScores)\n",
      "+\t\tclassSimilarities = list(zip(classPairs, similarityScores))\n",
      " \n",
      " \t\truleClusters = self.findRuleClusters()\n",
      " \t\tfor ruleCluster in ruleClusters:\n",
      "@@ -1502,7 +1503,7 @@\n",
      " \n",
      " \t\t## Check for limits of resources and add count(resource)==limit to the concepts.\n",
      " \t\tfor rule in self.interactionSet:\n",
      "-\t\t\tif 'resource' in rule.args.keys() and rule.args['resource']==item and 'limit' in rule.args.keys() and num>=rule.args['limit']:\n",
      "+\t\t\tif 'resource' in list(rule.args.keys()) and rule.args['resource']==item and 'limit' in list(rule.args.keys()) and num>=rule.args['limit']:\n",
      " \t\t\t\tlimit = rule.args['limit']\n",
      " \t\t\t\t##Also have a concept that is == num:\n",
      " \t\t\t\ttext = item+\">=\"+str(limit)\n",
      "@@ -1518,28 +1519,28 @@\n",
      " \t\treturn False\n",
      " \n",
      " \tdef displayRules(self):\n",
      "-\t\tprint \"\"\n",
      "-\t\tprint \"InteractionSet:\"\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(\"InteractionSet:\")\n",
      " \t\tfor rule in self.interactionSet:\n",
      " \t\t\trule.display()\n",
      " \n",
      " \tdef displayClasses(self):\n",
      "-\t\tprint \"\"\n",
      "-\t\tprint \"Class assignments:\"\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(\"Class assignments:\")\n",
      " \t\tfor c in self.classes:\n",
      " \t\t\tclass_list = [cl.color for cl in self.classes[c]]\n",
      "-\t\t\tprint \"\\t{}: {}: {}\".format(c, class_list, self.spriteObjects[cl.color].vgdlType)\n",
      "+\t\t\tprint(\"\\t{}: {}: {}\".format(c, class_list, self.spriteObjects[cl.color].vgdlType))\n",
      " \t\t#print self.classes\n",
      "-\t\tprint\n",
      "+\t\tprint()\n",
      " \n",
      " \tdef displayTerminationSet(self):\n",
      "-\t\tprint \"\"\n",
      "-\t\tprint \"TerminationSet:\"\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(\"TerminationSet:\")\n",
      " \t\tfor tc in self.terminationSet:\n",
      " \t\t\ttc.display()\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint \"_______\"\n",
      "+\t\tprint(\"_______\")\n",
      " \t\tself.displayRules()\n",
      " \t\tself.displayClasses()\n",
      " \t\tself.displayTerminationSet() #TODO: Figure out why this isn't printing\n",
      "@@ -1600,7 +1601,7 @@\n",
      " \t\tself.nodes_accepted = 0\n",
      " \n",
      " \tdef display(self):\n",
      "-\t\tprint self.theoryCount\n",
      "+\t\tprint(self.theoryCount)\n",
      " \n",
      " \tdef makeSpriteParse(self):\n",
      " \t\ts = SpriteParser()\n",
      "@@ -1616,7 +1617,7 @@\n",
      " \t\t\t\tt.posterior = t.prior()/z\n",
      " \t\t\treturn [t.posterior for t in self.hypothesisSpace]\n",
      " \t\telse:\n",
      "-\t\t\tprint \"Empty hypothesis space; can't give you a posterior.\"\n",
      "+\t\t\tprint(\"Empty hypothesis space; can't give you a posterior.\")\n",
      " \tdef entropy(self, theory):\n",
      " \t\tentropySum = 0\n",
      " \t\tnumSpritesInClasses = float(sum([1 for c in theory.classes for i in c]))\n",
      "@@ -1648,7 +1649,7 @@\n",
      " \t\ttry:\n",
      " \t\t\tprevClassGameStates = [theory.makeGameStateWithClasses(t.gameState['objects']) for t in prevTimeSteps]\n",
      " \t\texcept TypeError:\n",
      "-\t\t\tprint \"TypeError in explainTermination\"\n",
      "+\t\t\tprint(\"TypeError in explainTermination\")\n",
      " \t\t\tembed()\n",
      " \t\tclassGameState = theory.makeGameStateWithClasses(timestep.gameState['objects'])\n",
      " \t\trulesToAdd = []\n",
      "@@ -1757,10 +1758,10 @@\n",
      " \t\tweights = weights/z\n",
      " \t\tpredLists = [prediction[0] for prediction in predictions]\n",
      " \t\tprobs = [[p[1]*weights[i] for p in predLists[i]] for i in range(len(predictions))]\n",
      "-\t\tprint len(weights), len(probs), len(probs[0])\n",
      "+\t\tprint(len(weights), len(probs), len(probs[0]))\n",
      " \t\tsums = [sum([p[i] for p in probs]) for i in range(len(predicates))]\n",
      " \n",
      "-\t\treturn zip(predicates, sums)\n",
      "+\t\treturn list(zip(predicates, sums))\n",
      " \n",
      " \n",
      " \tdef DFSinduction(self, theory, timesteps, maxNumTheories, override=False, verbose=False):\n",
      "@@ -1769,8 +1770,8 @@\n",
      " \t\t\"\"\"\n",
      " \n",
      " \t\tif verbose:\n",
      "-\t\t\tprint \"\\nStart hyp space length:\", len(self.hypothesisSpace)\n",
      "-\t\t\tprint \"running induction on theory\"\n",
      "+\t\t\tprint(\"\\nStart hyp space length:\", len(self.hypothesisSpace))\n",
      "+\t\t\tprint(\"running induction on theory\")\n",
      " \t\t\ttheory.display()\n",
      " \n",
      " \t\t# If still have time to generate more theories\n",
      "@@ -1783,8 +1784,8 @@\n",
      " \t\t\t\treturn\n",
      " \n",
      " \t\t\tif verbose:\n",
      "-\t\t\t\tprint \"Current theory depth: \", ts_index\n",
      "-\t\t\t\tprint \"Explaining event\", timesteps[ts_index].events\n",
      "+\t\t\t\tprint(\"Current theory depth: \", ts_index)\n",
      "+\t\t\t\tprint(\"Explaining event\", timesteps[ts_index].events)\n",
      " \n",
      " \n",
      " \t\t\t# Explain current timestep\n",
      "@@ -1792,7 +1793,7 @@\n",
      " \n",
      " \t\t\tself.nodes_generated += len(newTheories)\n",
      " \t\t\tif verbose:\n",
      "-\t\t\t\tprint \"Possible new theories: \", len(newTheories)\n",
      "+\t\t\t\tprint(\"Possible new theories: \", len(newTheories))\n",
      " \t\t\t\tfor theory in newTheories:\n",
      " \t\t\t\t\ttheory.display()\n",
      " \n",
      "@@ -1807,8 +1808,8 @@\n",
      " \t\t\t\t\t\tnewTheoriesCount += 1\n",
      " \t\t\t\t\t\tself.hypothesisSpace.append(newTheory)\n",
      " \t\t\t\t\telse:\n",
      "-\t\t\t\t\t\tprint \"newTheory didn't explain all events\"\n",
      "-\t\t\t\t\t\tprint \"theory:\"\n",
      "+\t\t\t\t\t\tprint(\"newTheory didn't explain all events\")\n",
      "+\t\t\t\t\t\tprint(\"theory:\")\n",
      " \t\t\t\t\t\tnewTheory.display()\n",
      " \t\t\t\t\t\tself.nodes_eliminated +=1\n",
      " \t\t\t\ttry:\n",
      "@@ -1820,14 +1821,14 @@\n",
      " \t\t\t\texcept IndexError:\n",
      " \t\t\t\t\t# timesteps is an empty list\n",
      " \t\t\t\t\tmax_likelihood = 0\n",
      "-\t\t\t\t\tprint \"WARNING: max_likelihood failed\"\n",
      "+\t\t\t\t\tprint(\"WARNING: max_likelihood failed\")\n",
      " \t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\n",
      " \t\t\t\t\tself.hypothesisSpace = [theory]\n",
      " \t\t\t\tif verbose:\n",
      "-\t\t\t\t\tprint \"New theories that passed likelihood tests: \", newTheoriesCount\n",
      "-\t\t\t\t\tprint \"New hyp space length: \", len(self.hypothesisSpace)\n",
      "-\t\t\t\t\tprint \"Nodes created: {}. Nodes eliminated: {}. Nodes accepted: {}\".format(self.nodes_generated, self.nodes_eliminated, self.nodes_accepted)\n",
      "+\t\t\t\t\tprint(\"New theories that passed likelihood tests: \", newTheoriesCount)\n",
      "+\t\t\t\t\tprint(\"New hyp space length: \", len(self.hypothesisSpace))\n",
      "+\t\t\t\t\tprint(\"Nodes created: {}. Nodes eliminated: {}. Nodes accepted: {}\".format(self.nodes_generated, self.nodes_eliminated, self.nodes_accepted))\n",
      " \n",
      " \n",
      " \t\t\t# If in the middle of timesteps:\n",
      "@@ -1852,10 +1853,10 @@\n",
      " \t\t\t\tnewTheories = self.orderHypotheses(acceptedTheories)\n",
      " \n",
      " \t\t\t\tif verbose:\n",
      "-\t\t\t\t\tprint \"New theories that passed likelihood tests: \", len(newTheories)\n",
      "+\t\t\t\t\tprint(\"New theories that passed likelihood tests: \", len(newTheories))\n",
      " \t\t\t\t\tfor t in newTheories:\n",
      " \t\t\t\t\t\tt.display()\n",
      "-\t\t\t\t\tprint \"Nodes created: {}. Nodes eliminated: {}. Nodes accepted: {}\".format(self.nodes_generated, self.nodes_eliminated, self.nodes_accepted)\n",
      "+\t\t\t\t\tprint(\"Nodes created: {}. Nodes eliminated: {}. Nodes accepted: {}\".format(self.nodes_generated, self.nodes_eliminated, self.nodes_accepted))\n",
      " \n",
      " \t\t\t\tfor t in newTheories:\n",
      " \t\t\t\t\tt.dryingPaint = set()\n",
      "@@ -1927,7 +1928,7 @@\n",
      " \t\teos = [o for o in theory.spriteSet if o.color=='ENDOFSCREEN'][0]\n",
      " \n",
      " \t\ti = len(theory.classes)\n",
      "-\t\tknownColors = [item.color for sublist in theory.classes.values() for item in sublist]\n",
      "+\t\tknownColors = [item.color for sublist in list(theory.classes.values()) for item in sublist]\n",
      " \t\tfor s in spriteSample:\n",
      " \t\t\t## If it's a sprite that's not in our theory, add it to the theory's classes\n",
      " \t\t\t## And intiialize all the generic rules.\n",
      "@@ -2019,9 +2020,9 @@\n",
      " \t\t# \tself.hypothesisSpace = hypothesisSpaceWithTermConditions\n",
      " \n",
      " \t\tif len(self.hypothesisSpace)==0:\n",
      "-\t\t\tprint \"#################################################################\"\n",
      "-\t\t\tprint \"WARNING: no hypotheses. Returning the hypotheses we started with.\"\n",
      "-\t\t\tprint \"#################################################################\"\n",
      "+\t\t\tprint(\"#################################################################\")\n",
      "+\t\t\tprint(\"WARNING: no hypotheses. Returning the hypotheses we started with.\")\n",
      "+\t\t\tprint(\"#################################################################\")\n",
      " \t\t\tself.hypothesisSpace = init_hypotheses\n",
      " \t\t\t# embed()\n",
      " \n",
      "@@ -2083,10 +2084,10 @@\n",
      " \t\t##change this!\n",
      " \t\tif self.spriteInductionResult:\n",
      " \t\t\tT.initializeSpriteSet(vgdlSpriteParse=False, spriteInductionResult=self.spriteInductionResult)\n",
      "-\t\t\tprint \"initialized from sprite induction result\"\n",
      "+\t\t\tprint(\"initialized from sprite induction result\")\n",
      " \t\telif self.vgdlSpriteParse:\n",
      " \t\t\tT.initializeSpriteSet(vgdlSpriteParse=self.vgdlSpriteParse, spriteInductionResult=False)\n",
      "-\t\t\tprint \"initialized from sprite parse\"\n",
      "+\t\t\tprint(\"initialized from sprite parse\")\n",
      " \n",
      " \t\tself.hypothesisSpace = [T]\n",
      " \t\tnewTheories = []\n",
      "@@ -2097,8 +2098,8 @@\n",
      " \t\t\ttimestep = timesteps[i]\n",
      " \n",
      " \t\t\tif verbose:\n",
      "-\t\t\t\tprint \"explaining events {}\".format(timestep.events)\n",
      "-\t\t\t\tprint \"___________________________________________________________________\"\n",
      "+\t\t\t\tprint(\"explaining events {}\".format(timestep.events))\n",
      "+\t\t\t\tprint(\"___________________________________________________________________\")\n",
      " \n",
      " \t\t\t# For every theory\n",
      " \t\t\tfor theory in self.hypothesisSpace:\n",
      "@@ -2132,16 +2133,16 @@\n",
      " \t\t\tself.cleanHypothesisSpace(timesteps[0:i+1], 1) #All timesteps up to now should be fully explained\n",
      " \n",
      " \t\t\tif verbose:\n",
      "-\t\t\t\tprint \"{} hypotheses:\".format(len(self.hypothesisSpace))\n",
      "+\t\t\t\tprint(\"{} hypotheses:\".format(len(self.hypothesisSpace)))\n",
      " \n",
      " \t\t\t# Sort hypotheses (right now by simple length metric), then print.\n",
      "-\t\t\thypotheses = sorted(self.hypothesisSpace, key=lambda x:len(x.interactionSet)*len(x.classes.keys()))\n",
      "+\t\t\thypotheses = sorted(self.hypothesisSpace, key=lambda x:len(x.interactionSet)*len(list(x.classes.keys())))\n",
      " \n",
      " \t\t\tif verbose:\n",
      " \t\t\t\tfor h in hypotheses:\n",
      " \t\t\t\t\th.display()\n",
      "-\t\t\t\tprint \"___________________________________________________________________\"\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tprint(\"___________________________________________________________________\")\n",
      "+\t\t\t\tprint(\"\")\n",
      " \n",
      " \t\t# Termination set induction\n",
      " \t\tif result:\n",
      "@@ -2271,9 +2272,9 @@\n",
      " \n",
      " \tidx = 0\n",
      " \ttry:\n",
      "-\t\tcolors = [colorDict[str(rle._game.sprite_constr[k][1]['color'])] for k in rle._obstypes.keys()]\n",
      "+\t\tcolors = [colorDict[str(rle._game.sprite_constr[k][1]['color'])] for k in list(rle._obstypes.keys())]\n",
      " \texcept:\n",
      "-\t\tprint \"problem with generateSymbolDict\"\n",
      "+\t\tprint(\"problem with generateSymbolDict\")\n",
      " \t\tembed()\n",
      " \ttry:\n",
      " \t\tcolors.append(colorDict[str(rle._game.sprite_constr['avatar'][1]['color'])])\n",
      "@@ -2309,7 +2310,7 @@\n",
      " \t'killIfHasLess': ['resource', 'limit'],\\\n",
      " \t'killOtherHasLess': ['resource', 'limit'],\\\n",
      " \t'wrapAround': ['offset']}\n",
      "-\tif interactionName in ontologyKeywordDict.keys():\n",
      "+\tif interactionName in list(ontologyKeywordDict.keys()):\n",
      " \t\treturn ontologyKeywordDict[interactionName]\n",
      " \telse:\n",
      " \t\treturn []\n",
      "@@ -2328,20 +2329,20 @@\n",
      " \t\ttry:\n",
      " \t\t\tcol = colorDict[str(rle._game.sprite_groups[spriteName][0].color)]\n",
      " \t\t\ttry:\n",
      "-\t\t\t\tclassName = [k for k in theory.classes.keys() if col in [c.color for c in theory.classes[k]]][0]\n",
      "+\t\t\t\tclassName = [k for k in list(theory.classes.keys()) if col in [c.color for c in theory.classes[k]]][0]\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"couldn't find className\"\n",
      "+\t\t\t\tprint(\"couldn't find className\")\n",
      " \t\t\t\tembed()\n",
      " \t\t\treturn className\n",
      " \t\texcept KeyError:\n",
      "-\t\t\tif spriteName in theory.classes.keys():\n",
      "+\t\t\tif spriteName in list(theory.classes.keys()):\n",
      " \t\t\t\treturn spriteName\n",
      " \t\t\telse:\n",
      " \t\t\t\ttry:\n",
      " \t\t\t\t\t## maybe we passed a color, so we should get the class.\n",
      " \t\t\t\t\treturn theory.spriteObjects[spriteName].className\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"failed to get spriteName color. In getClassNameFromSpriteString\"\n",
      "+\t\t\t\t\tprint(\"failed to get spriteName color. In getClassNameFromSpriteString\")\n",
      " \t\t\t\t\tembed()\n",
      " \n",
      " \tdef buildArgsString(interactionRule):\n",
      "@@ -2380,7 +2381,7 @@\n",
      " \t\telse:\n",
      " \t\t\tif interactionRule.args:\n",
      " \t\t\t\targsString = \"\"\n",
      "-\t\t\t\tfor k,v in interactionRule.args.items():\n",
      "+\t\t\t\tfor k,v in list(interactionRule.args.items()):\n",
      " \t\t\t\t\tif k in ['stype', 'strigger']:\n",
      " \t\t\t\t\t\targsString += \" %s=%s\"%(k, getClassNameFromSpriteString(v))\n",
      " \t\t\t\t\telse:\n",
      "@@ -2398,7 +2399,7 @@\n",
      " \n",
      " \tDIRECTION_MAP = {(0,-1):'UP', (0,1):'DOWN', (1,0):'RIGHT', (-1,0):'LEFT'}\n",
      " \n",
      "-\tfor k,v in rle._game.sprite_groups.items():\n",
      "+\tfor k,v in list(rle._game.sprite_groups.items()):\n",
      " \t\tif k!='avatar' and k not in rle._obstypes:\n",
      " \t\t\trle._obstypes[k] = [rle._sprite2state(sprite, oriented=False) for sprite in v if sprite not in rle._game.kill_list]\n",
      " \n",
      "@@ -2414,7 +2415,7 @@\n",
      " \t\t\ttry:\n",
      " \t\t\t\tcolorToSprite[colorDict[str(rle._game.sprite_constr[spriteType][1]['color'])]] = spriteType\n",
      " \t\t\texcept KeyError:\n",
      "-\t\t\t\tprint \"in writeTheoryToTxt, keyError\"\n",
      "+\t\t\t\tprint(\"in writeTheoryToTxt, keyError\")\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \tif goalLoc:\n",
      "@@ -2429,7 +2430,7 @@\n",
      " \t## teleport sprites have to be handled separately, as the spriteType is relational -- it depends on\n",
      " \t## what is in the interactionRules.\n",
      " \tif theory.interactionSet[0].args is not None:\n",
      "-\t\tif any([len(i.args.keys()) for i in theory.interactionSet]):\n",
      "+\t\tif any([len(list(i.args.keys())) for i in theory.interactionSet]):\n",
      " \t\t\t# print \"found args in interactionRule\"\n",
      " \t\t\t# embed()\n",
      " \t\t\tfor interactionRule in theory.interactionSet:\n",
      "@@ -2451,7 +2452,7 @@\n",
      " \tresourcesToAdd = set()\n",
      " \tfor i in theory.interactionSet:\n",
      " \t\tif i.args is not None:\n",
      "-\t\t\tfor k,v in i.args.items():\n",
      "+\t\t\tfor k,v in list(i.args.items()):\n",
      " \t\t\t\tif k=='resource':\n",
      " \t\t\t\t\tresourcesToAdd.add(v)\n",
      " \t\t\t# if \"resource\" in i.args.keys():\n",
      "@@ -2465,7 +2466,7 @@\n",
      " \ttheoryString += \"\\tSpriteSet\\n\"\n",
      " \n",
      " \n",
      "-\tfor c, sprites in theory.classes.items():\n",
      "+\tfor c, sprites in list(theory.classes.items()):\n",
      " \t\tif c == 'EOS':\n",
      " \t\t\tpass\n",
      " \t\telse:\n",
      "@@ -2479,11 +2480,11 @@\n",
      " \t\t\t\t\tif unfilteredType == \"OTHER\":\n",
      " \t\t\t\t\t\tstype = 'ResourcePack'\n",
      " \t\t\t\t\telse:\n",
      "-\t\t\t\t\t\tprint \"writetheorytotxt. stype problem\"\n",
      "+\t\t\t\t\t\tprint(\"writetheorytotxt. stype problem\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\tif s.args:\n",
      "-\t\t\t\t\tfor k,v in s.args.items():\n",
      "+\t\t\t\t\tfor k,v in list(s.args.items()):\n",
      " \t\t\t\t\t\tif k == \"color\":\n",
      " \t\t\t\t\t\t\tcontinue\n",
      " \t\t\t\t\t\telif k == \"orientation\":\n",
      "@@ -2522,7 +2523,7 @@\n",
      " \t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\targsString += \" %s=%s\"%(\"stype\", colorConvertedToSType)\n",
      " \t\t\t\t\texcept KeyError:\n",
      "-\t\t\t\t\t\tprint \"in TheoryToTxt(), search for colorConvertedToSType\"\n",
      "+\t\t\t\t\t\tprint(\"in TheoryToTxt(), search for colorConvertedToSType\")\n",
      " \t\t\t\t\t\t## TODO: If you, say, hypothesize that a missile is a Chaser and that it chases some random color but you don't have that color in your theory yet,\n",
      " \t\t\t\t\t\t## you can end up here.\n",
      " \t\t\t\t\t\t# embed()\n",
      "@@ -2624,11 +2625,11 @@\n",
      " \t\t\t# if c2=='EOS' or c1=='EOS': ## 'EOS stepBack' is always being written at the end. Don't handle it here.\n",
      " \t\t\t# \tcontinue\n",
      " \t\t\tif (c1=='laog' and len(theory.classes[c1])==0) or (c2=='laog' and len(theory.classes[c2])==0):\n",
      "-\t\t\t\tprint \"found laog\"\n",
      "+\t\t\t\tprint(\"found laog\")\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \t\t\tfor s1 in theory.classes[c1]:\n",
      "-\t\t\t\tif c2 not in theory.classes.keys():\n",
      "+\t\t\t\tif c2 not in list(theory.classes.keys()):\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\tfor s2 in theory.classes[c2]:\n",
      " \t\t\t\t\targsString = \"\"\n",
      "@@ -2739,7 +2740,7 @@\n",
      " \t\t\t\t\tsymbol = objectsToSymbol(rle, rle.getObjectsFromNumber(state[r][c]), symbolDict)\n",
      " \t\t\t\t\tmappedState[r][c] = symbol\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"in map\"\n",
      "+\t\t\t\t\tprint(\"in map\")\n",
      " \t\t\t\t\tembed()\n",
      " \n",
      " \t\t\ttry:\n",
      "@@ -2747,7 +2748,7 @@\n",
      " \t\t\t\t\t# an empty square has been selected as the goal\n",
      " \t\t\t\t\tmappedState[r][c] = \"G\"\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"mappedState problem2\"\n",
      "+\t\t\t\tprint(\"mappedState problem2\")\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \tlevelString = 'level=\"\"\"\\n'\n",
      "@@ -2758,14 +2759,14 @@\n",
      " \n",
      " \ttheoryString += \"\\tLevelMapping\\n\"\n",
      " \n",
      "-\tfor colors, symbol in symbolDict.items():\n",
      "+\tfor colors, symbol in list(symbolDict.items()):\n",
      " \t\tif type(colors)==tuple:\n",
      "-\t\t\ttypes = [theory.spriteObjects[c].className for c in colors if c in theory.spriteObjects.keys()]\n",
      "+\t\t\ttypes = [theory.spriteObjects[c].className for c in colors if c in list(theory.spriteObjects.keys())]\n",
      " \t\t\tif len(types)==2:\n",
      " \t\t\t\ttheoryString += \"\\t\\t%s > %s %s\\n\"%(symbol, types[0], types[1])\n",
      " \t\t\telif len(types)==3:\n",
      " \t\t\t\ttheoryString += \"\\t\\t%s > %s %s %s\\n\"%(symbol, types[0], types[1], types[2])\n",
      "-\t\telif type(colors)==str and colors in theory.spriteObjects.keys():\n",
      "+\t\telif type(colors)==str and colors in list(theory.spriteObjects.keys()):\n",
      " \t\t\tc = theory.spriteObjects[colors].className\n",
      " \t\t\ttheoryString += \"\\t\\t%s > %s\\n\"%(symbol, c)\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored planner.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: planner.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- planner.py\t(original)\n",
      "+++ planner.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict, sys\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict, sys\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,14 +14,14 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "-from util import *\n",
      "+from queue import Queue\n",
      "+from .util import *\n",
      " import multiprocessing\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -56,15 +56,15 @@\n",
      " \t\t\tself.immovables = self.rle.immovables\n",
      " \t\t\tself.killerObjects = self.rle.killerObjects\n",
      " \t\t\tself.teleports = self.rle.teleports\n",
      "-\t\t\tprint \"immovables\", self.rle.immovables\n",
      "+\t\t\tprint(\"immovables\", self.rle.immovables)\n",
      " \t\texcept:\n",
      " \t\t\tself.immovables = ['wall', 'poison']\n",
      " \t\t\tself.killerObjects = ['chaser']\n",
      " \t\t\tself.teleports = ['exit1', 'entry1']\n",
      "-\t\t\tprint \"Using defaults as immovables\", self.immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", self.immovables)\n",
      " \n",
      " \t\tfor i in self.immovables:\n",
      "-\t\t\tif i in self.rle._obstypes.keys():\n",
      "+\t\t\tif i in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\t# embed()\n",
      "@@ -165,11 +165,11 @@\n",
      " \t\twhile len(rewardQueue)>0:\n",
      " \t\t\tloc = rewardQueue.popleft()\n",
      " \t\t\tif loc not in processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\ttry:\n",
      " \t\t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"problem with rewardDict\"\n",
      "+\t\t\t\t\tprint(\"problem with rewardDict\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\tprocessed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -204,9 +204,9 @@\n",
      " \t\t\tnextLoc = currentLoc[0]+a[1], currentLoc[1]+a[0] #again, locations are (y,x) and actions are (x,y)\n",
      " \t\telse:\n",
      " \t\t\treturn 0.\n",
      "-\t\tif nextLoc in self.rewardDict.keys():\n",
      "+\t\tif nextLoc in list(self.rewardDict.keys()):\n",
      " \t\t\treturn self.rewardDict[nextLoc]\n",
      "-\t\telif currentLoc in self.rewardDict.keys():\n",
      "+\t\telif currentLoc in list(self.rewardDict.keys()):\n",
      " \t\t\treturn self.rewardDict[currentLoc]\n",
      " \t\telse:\n",
      " \t\t\treturn 0.\n",
      "@@ -237,7 +237,7 @@\n",
      " \tdef findObjectsInState(self, s, objName):\n",
      " \t\t##TODO: Finish last part of this function -- sometimes it can't access objloc[0][0], objloc[1][0]\n",
      " \t\t\n",
      "-\t\tif objName in self.rle._game.sprite_groups.keys():\n",
      "+\t\tif objName in list(self.rle._game.sprite_groups.keys()):\n",
      " \t\t\treturn [self.rle._rect2pos(o.rect) for o in self.rle._game.sprite_groups[objName] if o not in self.rle._game.kill_list]\n",
      " \t\telse:\n",
      " \t\t\treturn None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored rlenvironment.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: rlenvironment.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- rlenvironment.py\t(original)\n",
      "+++ rlenvironment.py\t(refactored)\n",
      "@@ -9,9 +9,9 @@\n",
      " \n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite\n",
      "-from stateobs import StateObsHandler \n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite\n",
      "+from .stateobs import StateObsHandler \n",
      " import argparse\n",
      " \n",
      " OBSERVATION_LOCAL = 'local'\n",
      "@@ -245,19 +245,19 @@\n",
      "     game = _createVGDLGame( *defMaze() )\n",
      "     rle = RLEnvironment( *defMaze() )\n",
      "     if rle.actionSpec() != {'scheme': 'Integer', 'N': 4}:\n",
      "-        print \"FAILED actionSpec\"\n",
      "-        print rle.actionSpec()\n",
      "+        print(\"FAILED actionSpec\")\n",
      "+        print(rle.actionSpec())\n",
      "     if rle.observationSpec() != {'scheme': 'Doubles', 'size': [10, 1]}:\n",
      "-        print \"FAILED observationSpec\"\n",
      "-        print rle.observationSpec()\n",
      "+        print(\"FAILED observationSpec\")\n",
      "+        print(rle.observationSpec())\n",
      " \n",
      " # Verify that observation received matches target observation\n",
      " def _verify( obs, targetObs ):\n",
      "     if obs[\"pcontinue\"] != targetObs[\"pcontinue\"]:\n",
      "-        print \"FAILED pcontinue\"\n",
      "+        print(\"FAILED pcontinue\")\n",
      "         return False\n",
      "     if obs[\"reward\"] != targetObs[\"reward\"]:\n",
      "-        print \"FAILED reward\"\n",
      "+        print(\"FAILED reward\")\n",
      "         return False\n",
      "     match = True\n",
      "     i=0 \n",
      "@@ -267,11 +267,11 @@\n",
      "         i = i+1\n",
      " \n",
      "     if match==False:\n",
      "-        print \"\"\n",
      "-        print \"FAILED observation\"\n",
      "-        print obs[\"observation\"]\n",
      "-        print targetObs[\"observation\"]\n",
      "-        print match\n",
      "+        print(\"\")\n",
      "+        print(\"FAILED observation\")\n",
      "+        print(obs[\"observation\"])\n",
      "+        print(targetObs[\"observation\"])\n",
      "+        print(match)\n",
      "         return False\n",
      "     return True\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP5.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP5.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP5.py\t(original)\n",
      "+++ WBP5.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " ACTIONS = [(1,0), (-1,0), (0,1), (0,-1)]\n",
      "@@ -8,21 +8,21 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename, display):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.noveltyDict = []\n",
      " \t\t# self.trueAtoms = defaultdict(lambda:set()) ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      " \t\tself.vecSize = None\n",
      "-\t\tprint \"If we track tokens we have an additional\", 2**self.phiSize, \"array elements.\"\n",
      "+\t\tprint(\"If we track tokens we have an additional\", 2**self.phiSize, \"array elements.\")\n",
      " \n",
      " \tdef calculateAtoms(self, rle):\n",
      " \t\tlst = {}\n",
      "-\t\tfor k in rle._game.sprite_groups.keys():\n",
      "+\t\tfor k in list(rle._game.sprite_groups.keys()):\n",
      " \t\t\tfor o in rle._game.sprite_groups[k]:\n",
      " \t\t\t\tif o not in rle._game.kill_list:\n",
      " \t\t\t\t\t## turn location into vector posd2[ition (rows appended one after the other.)\n",
      "@@ -45,11 +45,11 @@\n",
      " \t\n",
      " \tdef compareDicts(self, d1,d2):\n",
      " \t\t## only tells us what is in d2 that isn't in d1, as well as differences in values between shared keys\n",
      "-\t\treturn [k for k in d2.keys() if (k not in d1.keys() or d1[k]!=d2[k])]\n",
      "+\t\treturn [k for k in list(d2.keys()) if (k not in list(d1.keys()) or d1[k]!=d2[k])]\n",
      " \n",
      " \tdef delta(self, node1, node2):\n",
      " \t\tif node1 is None:\n",
      "-\t\t\tdiff = node2.state.keys()\n",
      "+\t\t\tdiff = list(node2.state.keys())\n",
      " \t\telse:\n",
      " \t\t\tdiff = self.compareDicts(node1.state, node2.state)\n",
      " \t\treturn set(diff)\n",
      "@@ -61,7 +61,7 @@\n",
      " \t\t# embed()\n",
      " \t\t# if len(self.trueAtoms) > 0:\n",
      " \t\tif len(self.noveltyDict) > 0:\n",
      "-\t\t\ttrueAtoms = node.state.keys()\n",
      "+\t\t\ttrueAtoms = list(node.state.keys())\n",
      " \t\t\toldTrueAtoms = set(trueAtoms)-set(newAtoms)\n",
      " \t\t\tcandidates = []\n",
      " \t\t\tfor i in range(1,k+1):\n",
      "@@ -237,7 +237,7 @@\n",
      " \t\t \telse:\n",
      " \t\t \t\treturn random.choice(bestNodes)\n",
      " \t\telse:\n",
      "-\t\t\tprint \"found 0 nodes in noveltyHeuristic\"\n",
      "+\t\t\tprint(\"found 0 nodes in noveltyHeuristic\")\n",
      " \t\t\tembed()\n",
      " \n",
      " def rewardHeuristic(lst, WBP, k, surrogateCall=False):\n",
      "@@ -251,7 +251,7 @@\n",
      " \t \telse:\n",
      " \t \t\treturn random.choice(bestNodes)\n",
      " \telse:\n",
      "-\t\tprint \"found 0 nodes in rewardHeuristic\"\n",
      "+\t\tprint(\"found 0 nodes in rewardHeuristic\")\n",
      " \t\tembed()\n",
      " \n",
      " #when you expand a node, evaluate its children on both heuristics, then place in the queue.\n",
      "@@ -322,12 +322,12 @@\n",
      " \t\t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\t\t\t# terminal = vrle._isDone()[0]\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t# except:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\t# embed()\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      "@@ -339,8 +339,8 @@\n",
      " \t\t\t\t# terminal = vrle._isDone()[0]\n",
      " \t\t\t\ti += 1\n",
      " \t\tif len(self.actionSeq)>0:\n",
      "-\t\t\tprint self.actionSeq[-1]\n",
      "-\t\tprint vrle.show()\n",
      "+\t\t\tprint(self.actionSeq[-1])\n",
      "+\t\tprint(vrle.show())\n",
      " \t\t# if len(vrle._game.sprite_groups['probe'])==0:\n",
      " \t\t\t# self.WBP.findAvatarInRLE(vrle) == (2,4):\n",
      " \t\t\t# embed()\n",
      "@@ -362,11 +362,11 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\ti+=1\n",
      " \n",
      "@@ -423,15 +423,15 @@\n",
      " \tt1 = time.time()\n",
      " \tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      " \t# last, visited, rejected, visitedStates = BFS2(rle, p, 2)\n",
      "-\tprint time.time()-t1\n",
      "-\tprint len(visited), len(rejected)\n",
      "+\tprint(time.time()-t1)\n",
      "+\tprint(len(visited), len(rejected))\n",
      " \tembed()\n",
      " \tif not hasattr(last, 'actionSeq'):\n",
      "-\t\tprint \"Failed without tracking tokens. re-trying\"\n",
      "+\t\tprint(\"Failed without tracking tokens. re-trying\")\n",
      " \t\tp.trackTokens = True\n",
      " \t\tt1 = time.time()\n",
      " \t\tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      "-\t\tprint time.time()-t1\n",
      "-\t\tprint len(visited), len(rejected)\n",
      "+\t\tprint(time.time()-t1)\n",
      "+\t\tprint(len(visited), len(rejected))\n",
      " \t# embed()\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored ontology_comments.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: ontology_comments.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ontology_comments.py\t(original)\n",
      "+++ ontology_comments.py\t(refactored)\n",
      "@@ -7,8 +7,8 @@\n",
      " from random import choice, random\n",
      " from math import sqrt\n",
      " import pygame\n",
      "-from tools import triPoints, unitVector, vectNorm, oncePerStep\n",
      "-from ai import AStarWorld\n",
      "+from .tools import triPoints, unitVector, vectNorm, oncePerStep\n",
      "+from .ai import AStarWorld\n",
      " from IPython import embed\n",
      " \n",
      " # ---------------------------------------------------------------------\n",
      "@@ -202,7 +202,7 @@\n",
      " # ---------------------------------------------------------------------\n",
      " #     Sprite types\n",
      " # ---------------------------------------------------------------------\n",
      "-from core import VGDLSprite, Resource\n",
      "+from .core import VGDLSprite, Resource\n",
      " '''\n",
      " In updateOptions function, object_info has form \n",
      "     {'position':(ob.rect.left, ob.rect.right), 'features':features, 'type': type_vector}\n",
      "@@ -306,7 +306,7 @@\n",
      "         for option in options:\n",
      " \n",
      "             left, top = self.physics.calculateActiveMovement(self, option)\n",
      "-            if (left, top) in position_options.keys():\n",
      "+            if (left, top) in list(position_options.keys()):\n",
      "                 position_options[(left, top)] += 1.0/len(options) \n",
      "             else:\n",
      "                 position_options[(left, top)] = 1.0/len(options)\n",
      "@@ -445,7 +445,7 @@\n",
      " \n",
      "         for option in options:\n",
      "             pos = self.physics.activeMovement(self, option)\n",
      "-            if pos in position_options.keys():\n",
      "+            if pos in list(position_options.keys()):\n",
      "                 position_options[pos] += 1.0/len(options) \n",
      "             else:\n",
      "                 position_options[pos] = 1.0/len(options)\n",
      "@@ -463,7 +463,7 @@\n",
      " \n",
      "         for option in options:\n",
      "             left, top = self.physics.calculateActiveMovement(self, option)\n",
      "-            if (left, top) in position_options.keys():\n",
      "+            if (left, top) in list(position_options.keys()):\n",
      "                 position_options[(left, top)] += 1.0/len(options) \n",
      "             else:\n",
      "                 position_options[(left, top)] = 1.0/len(options)\n",
      "@@ -603,7 +603,7 @@\n",
      " # ---------------------------------------------------------------------\n",
      " #     Avatars: player-controlled sprite types\n",
      " # ---------------------------------------------------------------------\n",
      "-from core import Avatar\n",
      "+from .core import Avatar\n",
      " \n",
      " class MovingAvatar(VGDLSprite, Avatar):\n",
      "     \"\"\" Default avatar, moves in the 4 cardinal directions. \"\"\"\n",
      "@@ -874,7 +874,7 @@\n",
      " # ---------------------------------------------------------------------\n",
      " #     Termination criteria\n",
      " # ---------------------------------------------------------------------\n",
      "-from core import Termination\n",
      "+from .core import Termination\n",
      " \n",
      " class Timeout(Termination):\n",
      "     def __init__(self, limit=0, win=False):\n",
      "@@ -905,7 +905,7 @@\n",
      "     def __init__(self, limit=0, win=True, **kwargs):\n",
      "         self.limit = limit\n",
      "         self.win = win\n",
      "-        self.stypes = kwargs.values()\n",
      "+        self.stypes = list(kwargs.values())\n",
      " \n",
      "     def isDone(self, game):\n",
      "         if sum([game.numSprites(st) for st in self.stypes]) == self.limit:\n",
      "@@ -1270,7 +1270,7 @@\n",
      "             left, top = current_sprite.physics.calculateActiveMovement(current_sprite, option) #TODO: Check why this calculation isn't correct\n",
      " \n",
      "             ## was rect.left, rect.top\n",
      "-            if (left, top) in position_options.keys():\n",
      "+            if (left, top) in list(position_options.keys()):\n",
      "                 position_options[(left, top)] += 1.0/len(options) \n",
      "             else:\n",
      "                 position_options[(left, top)] = 1.0/len(options)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored rlenvironmentnonstatic.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: rlenvironmentnonstatic.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- rlenvironmentnonstatic.py\t(original)\n",
      "+++ rlenvironmentnonstatic.py\t(refactored)\n",
      "@@ -9,21 +9,22 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame\n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic\n",
      " from collections import defaultdict\n",
      " import argparse\n",
      " from IPython import embed\n",
      " import random\n",
      " import math\n",
      " import importlib\n",
      "-from colors import *\n",
      "-from util import factorize, objectsToSymbol\n",
      "+from .colors import *\n",
      "+from .util import factorize, objectsToSymbol\n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      " from termcolor import colored\n",
      " import time\n",
      "-import cPickle\n",
      "+import pickle\n",
      "+from functools import reduce\n",
      " # from line_profiler import LineProfiler\n",
      " \n",
      " OBSERVATION_LOCAL = 'local'\n",
      "@@ -99,7 +100,7 @@\n",
      "         idx = 0\n",
      "         OLD_GOAL = \"oldGl\"\n",
      "         # embed()\n",
      "-        for s in self._obstypes.keys():\n",
      "+        for s in list(self._obstypes.keys()):\n",
      "             # colorMapping[s] = colorDict[str(self._game.sprite_constr[s][1]['color'])].lower()\n",
      "             if not s == \"goal\":\n",
      "                 inverseMapping[s] = alnum[idx]\n",
      "@@ -124,7 +125,7 @@\n",
      "         \"\"\"        \n",
      "         mappedState = np.ones((self.outdim[1]*self.outdim[0]))\n",
      "         kl_set = set(self._game.kill_list)\n",
      "-        for k, lst in self._game.sprite_groups.items():\n",
      "+        for k, lst in list(self._game.sprite_groups.items()):\n",
      "             if k!= thingWeShoot:\n",
      "                 for sprite in lst:\n",
      "                     if sprite not in kl_set:\n",
      "@@ -148,13 +149,13 @@\n",
      "         else:\n",
      "             mappedState = [[' ' for x in range(self.outdim[1])] for y in range(self.outdim[0])]\n",
      " \n",
      "-        for lst in self._game.sprite_groups.values():\n",
      "+        for lst in list(self._game.sprite_groups.values()):\n",
      "             for sprite in lst:\n",
      "                 if sprite not in self._game.kill_list:\n",
      "                     y,x = sprite.rect.top/30, sprite.rect.left/30\n",
      "                     locs[(y,x)].append(sprite.name)\n",
      " \n",
      "-        for k,v in locs.iteritems():\n",
      "+        for k,v in locs.items():\n",
      "             if binary:\n",
      "                 symbol = 0\n",
      "             else:\n",
      "@@ -379,7 +380,7 @@\n",
      "         ## getState (defined in stateobsnonstatic) uses it to populate getState, getSensors, etc.\n",
      "         for k in self._game.sprite_groups:\n",
      "             for sprite in self._game.sprite_groups[k]:\n",
      "-                if (k, self._rect2pos(sprite.rect)) not in self._gravepoints.keys():\n",
      "+                if (k, self._rect2pos(sprite.rect)) not in list(self._gravepoints.keys()):\n",
      "                     self._gravepoints[(k, self._rect2pos(sprite.rect))] = True\n",
      "         # print \"after adding gravepoints\"\n",
      "         # embed()\n",
      "@@ -445,11 +446,11 @@\n",
      "             self._game.keystate[k] = False\n",
      " \n",
      "         self._game.positionDict = dict()\n",
      "-        for k,v in self._game.sprite_groups.items():\n",
      "+        for k,v in list(self._game.sprite_groups.items()):\n",
      "             for sprite in v:\n",
      "                 if sprite not in self._game.kill_list:\n",
      "                     loc = (sprite.rect.left, sprite.rect.top)\n",
      "-                    if loc in self._game.positionDict.keys():\n",
      "+                    if loc in list(self._game.positionDict.keys()):\n",
      "                         self._game.positionDict[loc].append(sprite)\n",
      "                     else:\n",
      "                         self._game.positionDict[loc] = [sprite]\n",
      "@@ -463,8 +464,8 @@\n",
      " ## the game in the agent's 'head'\n",
      " def defTheoryTest():\n",
      "     from examples.gridphysics.theorytest import game, level\n",
      "-    print level[0]\n",
      "-    print game[0]\n",
      "+    print(level[0])\n",
      "+    print(game[0])\n",
      "     return (game, level)\n",
      " \n",
      " def defVirtualGame():\n",
      "@@ -527,7 +528,7 @@\n",
      " def natsort_key(s):\n",
      "     \"Used internally to get a tuple by which s is sorted.\"\n",
      "     import re\n",
      "-    return map(try_int, re.findall(r'(\\d+|\\D+)', s))\n",
      "+    return list(map(try_int, re.findall(r'(\\d+|\\D+)', s)))\n",
      " \n",
      " def natcmp(a, b):\n",
      "     \"Natural string comparison, case sensitive.\"\n",
      "@@ -539,7 +540,7 @@\n",
      " \n",
      " def defInputGame(filename, randomize=False, index=None):\n",
      "     game_file = importlib.import_module(filename)\n",
      "-    levels = [k for k in game_file.__dict__.keys() if 'level' in k]\n",
      "+    levels = [k for k in list(game_file.__dict__.keys()) if 'level' in k]\n",
      "     levels.sort(natcasecmp)\n",
      "     # print levels\n",
      "     if randomize:\n",
      "@@ -547,7 +548,7 @@\n",
      "         return (game_file.game, game_file.__dict__[level])\n",
      "     elif index>=0:\n",
      "         if index<len(levels):\n",
      "-            print index\n",
      "+            print(index)\n",
      "             level = levels[index]\n",
      "         else:\n",
      "             level = random.choice(levels)\n",
      "@@ -600,19 +601,19 @@\n",
      "     game = _createVGDLGame( *defMaze() )\n",
      "     rle = RLEnvironmentNonStatic( *defMaze() )\n",
      "     if rle.actionSpec() != {'scheme': 'Integer', 'N': 4}:\n",
      "-        print \"FAILED actionSpec\"\n",
      "-        print rle.actionSpec()\n",
      "+        print(\"FAILED actionSpec\")\n",
      "+        print(rle.actionSpec())\n",
      "     if rle.observationSpec() != {'scheme': 'Doubles', 'size': [10, 1]}:\n",
      "-        print \"FAILED observationSpec\"\n",
      "-        print rle.observationSpec()\n",
      "+        print(\"FAILED observationSpec\")\n",
      "+        print(rle.observationSpec())\n",
      " \n",
      " # Verify that observation received matches target observation\n",
      " def _verify( obs, targetObs ):\n",
      "     if obs[\"pcontinue\"] != targetObs[\"pcontinue\"]:\n",
      "-        print \"FAILED pcontinue\"\n",
      "+        print(\"FAILED pcontinue\")\n",
      "         return False\n",
      "     if obs[\"reward\"] != targetObs[\"reward\"]:\n",
      "-        print \"FAILED reward\"\n",
      "+        print(\"FAILED reward\")\n",
      "         return False\n",
      "     match = True\n",
      "     i=0\n",
      "@@ -622,11 +623,11 @@\n",
      "         i = i+1\n",
      " \n",
      "     if match==False:\n",
      "-        print \"\"\n",
      "-        print \"FAILED observation\"\n",
      "-        print obs[\"observation\"]\n",
      "-        print targetObs[\"observation\"]\n",
      "-        print match\n",
      "+        print(\"\")\n",
      "+        print(\"FAILED observation\")\n",
      "+        print(obs[\"observation\"])\n",
      "+        print(targetObs[\"observation\"])\n",
      "+        print(match)\n",
      "         return False\n",
      "     return True\n",
      " \n",
      "@@ -639,8 +640,8 @@\n",
      " \n",
      " def createMindEnv(game, level, output=False, obsType=OBSERVATION_GLOBAL ):\n",
      "     if output:\n",
      "-        print game\n",
      "-        print level\n",
      "+        print(game)\n",
      "+        print(level)\n",
      "     return RLEnvironmentNonStatic( game, level, observationType=obsType )\n",
      " \n",
      " def createRLVirtualGame( obsType=OBSERVATION_GLOBAL ):\n",
      "@@ -755,7 +756,7 @@\n",
      "             rle = createRLSimpleGame1( obsType )\n",
      " \n",
      "         # rle = createRLSimpleGame1( obsType )\n",
      "-        print \"in testSimpleGame1\"\n",
      "+        print(\"in testSimpleGame1\")\n",
      "         embed()\n",
      " \n",
      " \n",
      "@@ -777,7 +778,7 @@\n",
      "             rle = createRLFrogs( obsType )\n",
      " \n",
      "         # rle = createRLSimpleGame1( obsType )\n",
      "-        print \"in testFrogs\"\n",
      "+        print(\"in testFrogs\")\n",
      "         embed()\n",
      " \n",
      " def testSimpleGame_missile(numEpisodes, numJogOnSpot, verify, reuseGame, obsType):\n",
      "@@ -797,7 +798,7 @@\n",
      "             rle = createRLSimpleGame1( obsType )\n",
      " \n",
      "         # rle = createRLSimpleGame1( obsType )\n",
      "-        print \"in testSimpleGame1\"\n",
      "+        print(\"in testSimpleGame1\")\n",
      "         embed()\n",
      " \n",
      " def testAliens(numEpisodes, numJogOnSpot, verify, reuseGame, obsType):\n",
      "@@ -816,7 +817,7 @@\n",
      "             # Re-create the game.\n",
      "             rle = createRLAliens( obsType )\n",
      " \n",
      "-        print \"in testAliens\"\n",
      "+        print(\"in testAliens\")\n",
      "         embed()\n",
      " \n",
      "         # res = rle.step(0) #up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored test_continuous.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: test_continuous.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- test_continuous.py\t(original)\n",
      "+++ test_continuous.py\t(refactored)\n",
      "@@ -1,8 +1,8 @@\n",
      "-from theory_template import Game, TimeStep\n",
      "-from sampleVGDLString import *\n",
      "-from taxonomy import *\n",
      "-from class_theory_template import *\n",
      "-from induction import runInduction_DFS\n",
      "+from .theory_template import Game, TimeStep\n",
      "+from .sampleVGDLString import *\n",
      "+from .taxonomy import *\n",
      "+from .class_theory_template import *\n",
      "+from .induction import runInduction_DFS\n",
      " from IPython import embed\n",
      " import time, ast\n",
      " \n",
      "@@ -29,16 +29,16 @@\n",
      " \tend = time.time()\n",
      " \n",
      " \t# Useful for quick view of test results\n",
      "-\tprint \"########################\"\n",
      "-\tprint \"Checking {}...\".format(name)\n",
      "-\tprint \"TIME TO RUN: {}\".format(end-start)\n",
      "-\tprint \"Expected number of hypotheses {} = actual number of hypotheses {}? {}\".format(expectedHypotheses, len(hypotheses), expectedHypotheses==len(hypotheses))\n",
      "+\tprint(\"########################\")\n",
      "+\tprint(\"Checking {}...\".format(name))\n",
      "+\tprint(\"TIME TO RUN: {}\".format(end-start))\n",
      "+\tprint(\"Expected number of hypotheses {} = actual number of hypotheses {}? {}\".format(expectedHypotheses, len(hypotheses), expectedHypotheses==len(hypotheses)))\n",
      " \t\n",
      " \tif expectedHypotheses==len(hypotheses): \t# TODO: Are there other parameters which we want to check?\n",
      "-\t\tprint \">>>> PASS!\"\n",
      "+\t\tprint(\">>>> PASS!\")\n",
      " \telse:\n",
      "-\t\tprint \">>>> FAIL :(\"\n",
      "-\tprint \"\\n########################\\n\\n\\n\\n\\n\\n\\n\"\n",
      "+\t\tprint(\">>>> FAIL :(\")\n",
      "+\tprint(\"\\n########################\\n\\n\\n\\n\\n\\n\\n\")\n",
      " \n",
      " \treturn hypotheses\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Can't parse mcts_old.py: ParseError: bad input: type=6, value='', context=('\\n\\n', (508, 0))\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: There was 1 error:\n",
      "RefactoringTool: Can't parse mcts_old.py: ParseError: bad input: type=6, value='', context=('\\n\\n', (508, 0))\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored core_old.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: core_old.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- core_old.py\t(original)\n",
      "+++ core_old.py\t(refactored)\n",
      "@@ -5,10 +5,10 @@\n",
      " '''\n",
      " import pygame\n",
      " from random import choice\n",
      "-from tools import Node, indentTreeParser\n",
      "+from .tools import Node, indentTreeParser\n",
      " from collections import defaultdict\n",
      "-from tools import roundedPoints\n",
      "-from colors import *\n",
      "+from .tools import roundedPoints\n",
      "+from .colors import *\n",
      " import os, shutil\n",
      " import datetime\n",
      " import uuid\n",
      "@@ -68,9 +68,9 @@\n",
      "     @staticmethod\n",
      "     def playSubjectiveGame(game_str, map_str):\n",
      "         from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-        from interfaces import GameTask\n",
      "-        from subjective import SubjectiveGame\n",
      "-        from agents import InteractiveAgent, UserTiredException\n",
      "+        from .interfaces import GameTask\n",
      "+        from .subjective import SubjectiveGame\n",
      "+        from .agents import InteractiveAgent, UserTiredException\n",
      "         g = VGDLParser().parseGame(game_str)\n",
      "         g.buildLevel(map_str)\n",
      "         senv = SubjectiveGame(g, actionDelay=100, recordingEnabled=True)\n",
      "@@ -105,7 +105,7 @@\n",
      "         \"\"\" Whatever is visible in the global namespace (after importing the ontologies)\n",
      "         can be used in the VGDL, and is evaluated.\n",
      "         \"\"\"\n",
      "-        from ontology import * # @UnusedWildImport\n",
      "+        from .ontology import * # @UnusedWildImport\n",
      "         return eval(estr)\n",
      " \n",
      "     def parseInteractions(self, inodes):\n",
      "@@ -116,7 +116,7 @@\n",
      "                 class1, class2 = [x.strip() for x in pair.split(\" \") if len(x)>0]\n",
      "                 self.game.collision_eff.append(tuple([class1, class2, eclass, args]))\n",
      "                 if self.verbose:\n",
      "-                    print \"Collision\", pair, \"has effect:\", edef\n",
      "+                    print(\"Collision\", pair, \"has effect:\", edef)\n",
      "         #print self.game.collision_eff\n",
      " \n",
      "     def parseTerminations(self, tnodes):\n",
      "@@ -126,7 +126,7 @@\n",
      "         for tn in tnodes:\n",
      "             sclass, args = self._parseArgs(tn.content)\n",
      "             if self.verbose:\n",
      "-                print \"Adding:\", sclass, args\n",
      "+                print(\"Adding:\", sclass, args)\n",
      "             self.game.terminations.append(sclass(**args))\n",
      " \n",
      "     def parseConditions(self, cnodes):\n",
      "@@ -151,7 +151,7 @@\n",
      " \n",
      "             if len(sn.children) == 0:\n",
      "                 if self.verbose:\n",
      "-                    print \"Defining:\", key, sclass, args, stypes\n",
      "+                    print(\"Defining:\", key, sclass, args, stypes)\n",
      "                 self.game.sprite_constr[key] = (sclass, args, stypes)\n",
      "                 # self.game.sprite_groups[key] = [] ##Added 4/30\n",
      "                 if key in self.game.sprite_order:\n",
      "@@ -168,7 +168,7 @@\n",
      "             # a char can map to multiple sprites\n",
      "             keys = [x.strip() for x in val.split(\" \") if len(x)>0]\n",
      "             if self.verbose:\n",
      "-                print \"Mapping\", c, keys\n",
      "+                print(\"Mapping\", c, keys)\n",
      "             self.game.char_mapping[c] = keys\n",
      " \n",
      "     def _parseArgs(self, s,  sclass=None, args=None):\n",
      "@@ -221,13 +221,13 @@\n",
      "     load_save_enabled = True\n",
      " \n",
      "     def __init__(self, **kwargs):\n",
      "-        from ontology import Immovable, DARKGRAY, BLACK, MovingAvatar, GOLD\n",
      "-        for name, value in kwargs.iteritems():\n",
      "+        from .ontology import Immovable, DARKGRAY, BLACK, MovingAvatar, GOLD\n",
      "+        for name, value in kwargs.items():\n",
      "             # print \"NAME: \", name\n",
      "             if hasattr(self, name):\n",
      "                 self.__dict__[name] = value\n",
      "             else:\n",
      "-                print \"WARNING: undefined parameter '%s' for game! \"%(name)\n",
      "+                print(\"WARNING: undefined parameter '%s' for game! \"%(name))\n",
      " \n",
      "         # contains mappings to constructor (just a few defaults are known)\n",
      "         self.sprite_constr = {'wall': (Immovable, {'color': DARKGRAY}, ['wall']),\n",
      "@@ -280,9 +280,9 @@\n",
      "         self.all_killed=[] # All items that have been killed\n",
      " \n",
      "     def buildLevel(self, lstr):\n",
      "-        from ontology import stochastic_effects\n",
      "+        from .ontology import stochastic_effects\n",
      "         lines = [l for l in lstr.split(\"\\n\") if len(l)>0]\n",
      "-        lengths = map(len, lines)\n",
      "+        lengths = list(map(len, lines))\n",
      "         assert min(lengths)==max(lengths), \"Inconsistent line lengths.\"\n",
      "         self.width = lengths[0]\n",
      "         self.height = len(lines)\n",
      "@@ -295,7 +295,7 @@\n",
      "         self.screensize = (self.width*self.block_size, self.height*self.block_size)\n",
      " \n",
      "         # set up resources\n",
      "-        for res_type, (sclass, args, _) in self.sprite_constr.iteritems():\n",
      "+        for res_type, (sclass, args, _) in self.sprite_constr.items():\n",
      "             if issubclass(sclass, Resource):\n",
      "                 if 'res_type' in args:\n",
      "                     res_type = args['res_type']\n",
      "@@ -352,7 +352,7 @@\n",
      " \n",
      "         for key in keys:\n",
      "             if self.num_sprites > self.MAX_SPRITES:\n",
      "-                print \"Sprite limit reached.\"\n",
      "+                print(\"Sprite limit reached.\")\n",
      "                 return\n",
      "             sclass, args, stypes = self.sprite_constr[key]\n",
      "             # verify the singleton condition\n",
      "@@ -390,7 +390,7 @@\n",
      "             self.screen = pygame.display.set_mode((1,1))\n",
      "             self.background = pygame.Surface(size)\n",
      "         else:\n",
      "-            from ontology import LIGHTGRAY\n",
      "+            from .ontology import LIGHTGRAY\n",
      "             pygame.init()\n",
      "             self.screen = pygame.display.set_mode(size)\n",
      "             self.background = pygame.Surface(size)\n",
      "@@ -427,7 +427,7 @@\n",
      "     def getAvatars(self):\n",
      "         \"\"\" The currently alive avatar(s) \"\"\"\n",
      "         res = []\n",
      "-        for ss in self.sprite_groups.values():\n",
      "+        for ss in list(self.sprite_groups.values()):\n",
      "             if ss and isinstance(ss[0], Avatar):\n",
      "                 res.extend([s for s in ss if s not in self.kill_list])\n",
      "         return res\n",
      "@@ -494,7 +494,7 @@\n",
      "                     ss[str(pos)] = attrs\n",
      "                 else:\n",
      "                     ss[pos] = attrs\n",
      "-                for a, val in s.__dict__.iteritems():\n",
      "+                for a, val in s.__dict__.items():\n",
      "                     if a not in ias:\n",
      "                         attrs[a] = val\n",
      "                 if s.resources:\n",
      "@@ -511,17 +511,17 @@\n",
      "         self.reset()\n",
      "         self.score = fs['score']\n",
      "         self.ended = fs['ended']\n",
      "-        for key, ss in fs['objects'].iteritems():\n",
      "+        for key, ss in fs['objects'].items():\n",
      "             self.sprite_groups[key] = [] ## Added 4/31/17\n",
      "-            for ID, attrs in ss.iteritems():\n",
      "+            for ID, attrs in ss.items():\n",
      "                 try:\n",
      "                     p = attrs['x'], attrs['y']\n",
      "                 except:\n",
      "                     p = attrs[x], attrs[y]\n",
      "                 s = self._createSprite_cheap(key, p)\n",
      "-                for a, val in attrs.iteritems():\n",
      "+                for a, val in attrs.items():\n",
      "                     if a == 'resources':\n",
      "-                        for r, v in val.iteritems():\n",
      "+                        for r, v in val.items():\n",
      "                             s.resources[r] = v\n",
      "                     else:\n",
      "                         s.__setattr__(a, val)\n",
      "@@ -657,7 +657,7 @@\n",
      " \n",
      "                         if dim:\n",
      "                             sprites = self.getSprites(classprite1)\n",
      "-                            spritesFiltered = filter(lambda sprite: sprite.__dict__[dim] == sprite2.__dict__[dim], sprites)\n",
      "+                            spritesFiltered = [sprite for sprite in sprites if sprite.__dict__[dim] == sprite2.__dict__[dim]]\n",
      "                             for sC in spritesFiltered:\n",
      "                                 new_effects.append((effect, sprite1, sC, self, kwargs))\n",
      "                                 spritesActedOn.add(sprite1)\n",
      "@@ -749,9 +749,9 @@\n",
      "         sprite_output = \"output/{}_{}_sprites.txt\".format(name,timestamp)\n",
      " \n",
      "         # --------- Game-play ------------\n",
      "-        from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-        from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution\n",
      "-        from ontology import spriteInduction\n",
      "+        from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+        from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution\n",
      "+        from .ontology import spriteInduction\n",
      "         # from theory_template import *\n",
      "         finalEventList = []\n",
      "         agentStatePrev = {}\n",
      "@@ -783,7 +783,7 @@\n",
      "             try:\n",
      "                 self.setFullState(self.playback_states[self.playback_index])\n",
      "             except:\n",
      "-                print \"playback is failing\"\n",
      "+                print(\"playback is failing\")\n",
      "                 embed()\n",
      " \n",
      "             # Save the event and agent state\n",
      "@@ -829,11 +829,11 @@\n",
      "         if win:\n",
      "             self.score += 1\n",
      "             self.win = True\n",
      "-            print \"Game won, with score %s\" % self.score\n",
      "+            print(\"Game won, with score %s\" % self.score)\n",
      "         else:\n",
      "             self.score -= 1\n",
      "             self.win = False\n",
      "-            print \"Playback is incomplete, or game is lost. Score=%s\" % self.score\n",
      "+            print(\"Playback is incomplete, or game is lost. Score=%s\" % self.score)\n",
      " \n",
      "         # ipdb.set_trace()\n",
      " \n",
      "@@ -873,9 +873,9 @@\n",
      "         sprite_output = \"output/{}_{}_sprites.txt\".format(name,timestamp)\n",
      " \n",
      "         # --------- Game-play ------------\n",
      "-        from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-        from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution\n",
      "-        from ontology import spriteInduction\n",
      "+        from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+        from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution\n",
      "+        from .ontology import spriteInduction\n",
      "         # from theory_template import *\n",
      "         finalEventList = []\n",
      "         agentStatePrev = {}\n",
      "@@ -890,7 +890,7 @@\n",
      "         self.all_objects = self.getObjects() # Save all objects, some which may be killed in game\n",
      " \n",
      "         ##figure out keypress type:\n",
      "-        disableContinuousKeyPress = all([self.all_objects[k]['sprite'].physicstype.__name__=='GridPhysics' for k in self.all_objects.keys()])\n",
      "+        disableContinuousKeyPress = all([self.all_objects[k]['sprite'].physicstype.__name__=='GridPhysics' for k in list(self.all_objects.keys())])\n",
      " \n",
      "         objects = self.getObjects()\n",
      "         self.spriteDistribution = {}\n",
      "@@ -933,7 +933,7 @@\n",
      "                         #     self.keystate = tuple(self.keystate)\n",
      "                         #     self.playback_index += 1\n",
      " \n",
      "-                        if lastKeyPress.index(1) in keyPresses.keys():\n",
      "+                        if lastKeyPress.index(1) in list(keyPresses.keys()):\n",
      "                             keyPressType = keyPresses[lastKeyPress.index(1)]\n",
      "                             # print keyPressType\n",
      " \n",
      "@@ -992,18 +992,18 @@\n",
      "                         #     self.score = 1\n",
      " \n",
      "                         self.win = True\n",
      "-                        print time.time()-t1, len(self.actions), win, self.score\n",
      "-                        print \"Game won, with score %s\" % self.score\n",
      "+                        print(time.time()-t1, len(self.actions), win, self.score)\n",
      "+                        print(\"Game won, with score %s\" % self.score)\n",
      "                     else:\n",
      "                         self.win = False\n",
      "                         self.score -=1 ## Added 3/16/17\n",
      "-                        print time.time()-t1, len(self.actions), win, self.score\n",
      "-                        print \"Game lost. Score=%s\" % self.score\n",
      "+                        print(time.time()-t1, len(self.actions), win, self.score)\n",
      "+                        print(\"Game lost. Score=%s\" % self.score)\n",
      "                     np.save(\"temp_data.npy\", [time.time()-t1, len(self.actions), self.win, self.score])\n",
      "                     allStates.append(self.getFullState())\n",
      " \n",
      "                     pygame.time.wait(10)\n",
      "-                    print len(self.actions), win, self.score\n",
      "+                    print(len(self.actions), win, self.score)\n",
      "                     return win, self.score\n",
      "                     # pygame.quit()\n",
      "                     # sys.exit()\n",
      "@@ -1048,7 +1048,7 @@\n",
      "             allStates.append(self.getFullState())\n",
      " \n",
      "         if(persist_movie):\n",
      "-            print \"Creating Movie\"\n",
      "+            print(\"Creating Movie\")\n",
      "             self.video_file = \"./videos/\" +  str(self.uiud) + \".mp4\"\n",
      "             subprocess.call([\"ffmpeg\",\"-y\",  \"-r\", \"30\", \"-b\", \"800\", \"-i\", tmpl, self.video_file ])\n",
      "             [os.remove(f) for f in glob.glob(tmp_dir + \"*\" + str(self.uiud) + \"*\")]\n",
      "@@ -1072,19 +1072,19 @@\n",
      "                 # self.score = 1\n",
      "             self.score +=1 # Added 3/16/17\n",
      "             self.win = True\n",
      "-            print \"Game won, with score %s\" % self.score\n",
      "+            print(\"Game won, with score %s\" % self.score)\n",
      "             np.save(\"temp_data.npy\", [time.time()-t1, len(self.actions), self.win, self.score])\n",
      " \n",
      "         else:\n",
      "             self.win = False\n",
      "             self.score -=1 # Added 3/16/17\n",
      "-            print \"Game lost. Score=%s\" % self.score\n",
      "+            print(\"Game lost. Score=%s\" % self.score)\n",
      "             np.save(\"temp_data.npy\", [time.time()-t1, len(self.actions), self.win, self.score])\n",
      " \n",
      " \n",
      "         # pause a few frames for the player to see the final screen.\n",
      "         pygame.time.wait(10)\n",
      "-        print len(self.actions), win, self.score\n",
      "+        print(len(self.actions), win, self.score)\n",
      "         return win, self.score\n",
      " \n",
      " \n",
      "@@ -1092,7 +1092,7 @@\n",
      "         return self.getAvatars()[0].declare_possible_actions()\n",
      " \n",
      "     def startGameExternalPlayer(self, headless, persist_movie, movie_dir):\n",
      "-        print \"in startgameexternalplayer\"\n",
      "+        print(\"in startgameexternalplayer\")\n",
      "         embed()\n",
      "         self._initScreen(self.screensize, headless)\n",
      "         pygame.display.flip()\n",
      "@@ -1171,7 +1171,7 @@\n",
      "     shrinkfactor=0\n",
      " \n",
      "     def __init__(self, pos, size=(10,10), color=None, speed=None, cooldown=None, physicstype=None, **kwargs):\n",
      "-        from ontology import GridPhysics\n",
      "+        from .ontology import GridPhysics\n",
      "         self.rect = pygame.Rect(pos, size)\n",
      "         self.x = pos[0]\n",
      "         self.y = pos[1]\n",
      "@@ -1193,11 +1193,11 @@\n",
      " \n",
      " \n",
      "         #self.color = color or self.color or (choice(self.COLOR_DISC), choice(self.COLOR_DISC), choice(self.COLOR_DISC))\n",
      "-        for name, value in kwargs.iteritems():\n",
      "+        for name, value in kwargs.items():\n",
      "             try:\n",
      "                 self.__dict__[name] = value\n",
      "             except:\n",
      "-                print \"WARNING: undefined parameter '%s' for sprite '%s'! \"%(name, self.__class__.__name__)\n",
      "+                print(\"WARNING: undefined parameter '%s' for sprite '%s'! \"%(name, self.__class__.__name__))\n",
      "         # how many timesteps ago was the last move?\n",
      "         self.lastmove = 0\n",
      " \n",
      "@@ -1237,7 +1237,7 @@\n",
      "         return (self.rect[0]-self.lastrect[0], self.rect[1]-self.lastrect[1])\n",
      " \n",
      "     def _draw(self, game):\n",
      "-        from ontology import LIGHTGREEN\n",
      "+        from .ontology import LIGHTGREEN\n",
      "         screen = game.screen\n",
      "         if self.shrinkfactor != 0:\n",
      "             shrunk = self.rect.inflate(-self.rect.width*self.shrinkfactor,\n",
      "@@ -1262,7 +1262,7 @@\n",
      " \n",
      "     def _drawResources(self, game, screen, rect):\n",
      "         \"\"\" Draw progress bars on the bottom third of the sprite \"\"\"\n",
      "-        from ontology import BLACK\n",
      "+        from .ontology import BLACK\n",
      "         tot = len(self.resources)\n",
      "         barheight = rect.height/3.5/tot\n",
      "         offset = rect.top+2*rect.height/3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP4.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP4.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP4.py\t(original)\n",
      "+++ WBP4.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " ACTIONS = [(1,0), (-1,0), (0,1), (0,-1)]\n",
      "@@ -8,12 +8,12 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename, display):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      " \n",
      "@@ -87,7 +87,7 @@\n",
      " \t\tif n%2==1:\n",
      " \t\t\tdecomposition.append(0)\n",
      " \t\t\tn = n-1\n",
      "-\t\ti = len(rle._obstypes.keys())\n",
      "+\t\ti = len(list(rle._obstypes.keys()))\n",
      " \t\twhile i>0:\n",
      " \t\t\tif n>=2**i:\n",
      " \t\t\t\tdecomposition.append(i)\n",
      "@@ -113,7 +113,7 @@\n",
      " \t\treturn mergedList\n",
      " \n",
      " \tdef factorizeBoolean(self, rle, n):\n",
      "-\t\tlistLen = len(rle._obstypes.keys())+1\n",
      "+\t\tlistLen = len(list(rle._obstypes.keys()))+1\n",
      " \t\treturn self.indicesToBooleans(self.T, self.factorize(rle, n))\n",
      " \n",
      " \tdef findTrueTuples(self, node, k):\n",
      "@@ -155,7 +155,7 @@\n",
      " \t\t\tcandidates = [frozenset(p) for p in list(itertools.combinations(newAtoms, k))]\n",
      " \n",
      " \t\tnode.candidates = candidates\n",
      "-\t\tprint len(candidates)\n",
      "+\t\tprint(len(candidates))\n",
      " \t\tnewTuples = set()\n",
      " \t\tfor aT in candidates:\n",
      " \t\t\tif aT not in self.trueAtoms:\n",
      "@@ -209,7 +209,7 @@\n",
      " \t\t \telse:\n",
      " \t\t \t\treturn random.choice(bestNodes)\n",
      " \t\telse:\n",
      "-\t\t\tprint \"found 0 nodes in noveltyHeuristic\"\n",
      "+\t\t\tprint(\"found 0 nodes in noveltyHeuristic\")\n",
      " \t\t\tembed()\n",
      " \n",
      " def rewardHeuristic(lst, WBP, k, surrogateCall=False):\n",
      "@@ -223,7 +223,7 @@\n",
      " \t \telse:\n",
      " \t \t\treturn random.choice(bestNodes)\n",
      " \telse:\n",
      "-\t\tprint \"found 0 nodes in rewardHeuristic\"\n",
      "+\t\tprint(\"found 0 nodes in rewardHeuristic\")\n",
      " \t\tembed()\n",
      " \n",
      " #when you expand a node, evaluate its children on both heuristics, then place in the queue.\n",
      "@@ -293,12 +293,12 @@\n",
      " \t\t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\t\t\t# terminal = vrle._isDone()[0]\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t# except:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\t# embed()\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      "@@ -310,8 +310,8 @@\n",
      " \t\t\t\t# terminal = vrle._isDone()[0]\n",
      " \t\t\t\ti += 1\n",
      " \t\tif len(self.actionSeq)>0:\n",
      "-\t\t\tprint self.actionSeq[-1]\n",
      "-\t\tprint vrle.show()\n",
      "+\t\t\tprint(self.actionSeq[-1])\n",
      "+\t\tprint(vrle.show())\n",
      " \t\t# if len(vrle._game.sprite_groups['probe'])==0:\n",
      " \t\t\t# self.WBP.findAvatarInRLE(vrle) == (2,4):\n",
      " \t\t\t# embed()\n",
      "@@ -333,11 +333,11 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\ti+=1\n",
      " \n",
      "@@ -390,14 +390,14 @@\n",
      " \tt1 = time.time()\n",
      " \tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      " \t# last, visited, rejected, visitedStates = BFS2(rle, p, 2)\n",
      "-\tprint time.time()-t1\n",
      "-\tprint len(visited), len(rejected)\n",
      "+\tprint(time.time()-t1)\n",
      "+\tprint(len(visited), len(rejected))\n",
      " \tif not hasattr(last, 'actionSeq'):\n",
      "-\t\tprint \"Failed without tracking tokens. re-trying\"\n",
      "+\t\tprint(\"Failed without tracking tokens. re-trying\")\n",
      " \t\tp.trackTokens = True\n",
      " \t\tt1 = time.time()\n",
      " \t\tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      "-\t\tprint time.time()-t1\n",
      "-\t\tprint len(visited), len(rejected)\n",
      "+\t\tprint(time.time()-t1)\n",
      "+\t\tprint(len(visited), len(rejected))\n",
      " \tembed()\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored mcts_pseudoreward_heuristic.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: mcts_pseudoreward_heuristic.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mcts_pseudoreward_heuristic.py\t(original)\n",
      "+++ mcts_pseudoreward_heuristic.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,14 +14,14 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from qlearner import *\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .qlearner import *\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -45,7 +45,7 @@\n",
      " \tdef __init__(self, existing_rle=False, game = None, level = None, partitionWeights=[1,0,1],\\\n",
      " \t\t         rleCreateFunc=False, obsType = OBSERVATION_GLOBAL, decay_factor=.8, num_workers=1):\n",
      " \t\tif not existing_rle and not rleCreateFunc:\n",
      "-\t\t\tprint \"You must pass either an existing rle or an rleCreateFunc\"\n",
      "+\t\t\tprint(\"You must pass either an existing rle or an rleCreateFunc\")\n",
      " \t\t\treturn\n",
      " \t\t# assumption: not starting on terminal state\n",
      " \t\t\"\"\"\n",
      "@@ -102,7 +102,7 @@\n",
      " \t\tgoal_loc = np.where(np.reshape(self.rle._getSensors(), self.outdim)==goal_code)\n",
      " \t\tgoal_loc = goal_loc[0][0], goal_loc[1][0] #(y,x)\n",
      " \n",
      "-\t\tif 'avatar' in self._obstypes.keys():\n",
      "+\t\tif 'avatar' in list(self._obstypes.keys()):\n",
      " \t\t\tinverted_avatar_loc=self._obstypes['avatar'][0]\n",
      " \t\t\tavatar_loc = (inverted_avatar_loc[1], inverted_avatar_loc[0])\n",
      " \t\t\tself.avatar_code = np.reshape(self.rle._getSensors(), self.outdim)[avatar_loc[0]][avatar_loc[1]]\n",
      "@@ -171,10 +171,10 @@\n",
      " \t\t\t# print \"immovables\", immovables\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall', 'poison']\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self._obstypes.keys():\n",
      "+\t\t\tif i in list(self._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -207,7 +207,7 @@\n",
      " \t\twhile len(self.rewardQueue)>0:\n",
      " \t\t\tloc = self.rewardQueue.popleft()\n",
      " \t\t\tif loc not in self.processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tself.processed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -251,8 +251,8 @@\n",
      " \t\t\tVrle = copy.deepcopy(VRLE)\n",
      " \n",
      " \t\t\tif i%100==0 and len(rewards)>0:\n",
      "-\t\t\t\tprint \"Training cycle: %i\"%i\n",
      "-\t\t\t\tprint \"avg. rewards for last group of 100\", np.mean(rewards[-100:])\n",
      "+\t\t\t\tprint(\"Training cycle: %i\"%i)\n",
      "+\t\t\t\tprint(\"avg. rewards for last group of 100\", np.mean(rewards[-100:]))\n",
      " \n",
      " \t\t\tif defaultPolicySolveStep:\n",
      " \t\t\t\treward, v, iters = self.treePolicy(self.root, Vrle, step_horizon, \\\n",
      "@@ -288,7 +288,7 @@\n",
      " \tdef getBestActionsForPlayout(self, partitionWeights, debug=False):\n",
      " \t\tv = self.root\n",
      " \t\tactions = []\n",
      "-\t\twhile v and not v.terminal and len(v.children.keys())>0:\n",
      "+\t\twhile v and not v.terminal and len(list(v.children.keys()))>0:\n",
      " \t\t\ta, v = self.bestChild(v,partitionWeights, debug=debug)\n",
      " \t\t\tactions.append(a)\n",
      " \t\treturn actions\n",
      "@@ -308,24 +308,24 @@\n",
      " \t\tcntr=0\n",
      " \t\tv = self.root\n",
      " \t\tif output:\n",
      "-\t\t\tprint \"current state\"\n",
      "-\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "+\t\t\tprint(\"current state\")\n",
      "+\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      " \t\tactions, nodes = [], []\n",
      " \t\twhile v and not v.terminal and cntr<numActions:\n",
      " \t\t\tif output:\n",
      "-\t\t\t\tprint \"options\"\n",
      "-\t\t\t\tprint [(ACTIONS[k],c.qVal) for k,c in v.children.iteritems()]\n",
      "+\t\t\t\tprint(\"options\")\n",
      "+\t\t\t\tprint([(ACTIONS[k],c.qVal) for k,c in v.children.items()])\n",
      " \t\t\ta, v = self.bestChild(v,(1,0,0))\n",
      " \n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tnodes.append(v)\n",
      " \t\t\tif output:\n",
      " \t\t\t\tif v:\n",
      "-\t\t\t\t\tprint \"selected\"\n",
      "-\t\t\t\t\tprint ACTIONS[a]\n",
      "-\t\t\t\t\tprint \"resulted in\"\n",
      "-\t\t\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "-\t\t\t\t\tprint \"\"\n",
      "+\t\t\t\t\tprint(\"selected\")\n",
      "+\t\t\t\t\tprint(ACTIONS[a])\n",
      "+\t\t\t\t\tprint(\"resulted in\")\n",
      "+\t\t\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      "+\t\t\t\t\tprint(\"\")\n",
      " \t\t\tcntr+=1\n",
      " \t\t# if v.terminal:\n",
      " \t\t# \tdistance = 0\n",
      "@@ -366,7 +366,7 @@\n",
      " \t\t\t\tterminal = rle._isDone()[0]\n",
      " \n",
      " \t\t\t\tif terminal != v.terminal or not np.array_equal(v.state, rle._getSensors()):\n",
      "-\t\t\t\t\tprint \"inconsistency in node and rle\"\n",
      "+\t\t\t\t\tprint(\"inconsistency in node and rle\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\t# terminal = (not res['pcontinue']) or (rle._avatar is None)\n",
      " \t\t\t\tif terminal:\n",
      "@@ -392,7 +392,7 @@\n",
      " \t\t\taction_choices = self.actions\n",
      " \n",
      " \t\tfor a in action_choices:\n",
      "-\t\t\tif a not in v.children.keys():\n",
      "+\t\t\tif a not in list(v.children.keys()):\n",
      " \t\t\t\texpand_action = a\n",
      " \t\t\t\tres = rle.step(a)\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      "@@ -417,13 +417,13 @@\n",
      " \tdef maxChild(self, v):\n",
      " \t\ttmp = np.where(np.reshape(v.state, self.outdim)==1)\n",
      " \t\tavatar_loc = tmp[0][0], tmp[1][0]\n",
      "-\t\tqVals = [v.children[a].qVal for a in v.children.keys()]\n",
      "-\t\tif len(qVals)>0 and avatar_loc in self.neighborDict.keys() and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      "+\t\tqVals = [v.children[a].qVal for a in list(v.children.keys())]\n",
      "+\t\tif len(qVals)>0 and avatar_loc in list(self.neighborDict.keys()) and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      " \t\t\t\tmaxVal = max(qVals)\n",
      "-\t\t\t\tchoices = [(a,c) for (a,c) in v.children.items() if c.qVal==maxVal]\n",
      "-\t\t\t\tfor (a,c) in v.children.items():\n",
      "-\t\t\t\t\tprint a, c.qVal\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tchoices = [(a,c) for (a,c) in list(v.children.items()) if c.qVal==maxVal]\n",
      "+\t\t\t\tfor (a,c) in list(v.children.items()):\n",
      "+\t\t\t\t\tprint(a, c.qVal)\n",
      "+\t\t\t\tprint(\"\")\n",
      " \t\t\t\treturn random.choice(choices)\n",
      " \t\telse:\n",
      " \t\t\treturn (None, None)\n",
      "@@ -464,7 +464,7 @@\n",
      " \t\t\tself.printexplorationweight = exploration_coefficient\n",
      " \t\t\t# print \"heuristic coefficient is now\", heuristic_coefficient\n",
      " \n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tcontinue\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -473,7 +473,7 @@\n",
      " \t\t\t\tif self.avatar_code not in [l%2 for l in c.state]: ##we're in a terminal losing state\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t## Don't give pseudoreward\n",
      " \t\t\t\t\tif not c.terminal:\n",
      "-\t\t\t\t\t\tprint \"terminal state but avatar not in c.state\"\n",
      "+\t\t\t\t\t\tprint(\"terminal state but avatar not in c.state\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\t\t# vLoc = np.where(np.reshape(v.state, self.outdim)%2==self.avatar_code)\n",
      "@@ -503,15 +503,15 @@\n",
      " \t\t\t\t\t\tsumVisitCount += abs(math.sqrt(2*math.log(v.visitCount)/c.visitCount))\n",
      " \t\t\t\t\t\tsumPseudoReward += abs(transform(loc))/c.visitCount\n",
      " \t\t\t\t\texcept TypeError:\n",
      "-\t\t\t\t\t\tprint \"loc is weird type\"\n",
      "+\t\t\t\t\t\tprint(\"loc is weird type\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \n",
      " \n",
      " \t\t# print \"\"\n",
      " \t\tif debug:\n",
      "-\t\t\tprint np.reshape(v.state, self.outdim)\n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\t\tprint(np.reshape(v.state, self.outdim))\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tfuncVal = -float('inf')\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -566,18 +566,18 @@\n",
      " \t\t\t\t\t        + exploration_coefficient*math.sqrt(2*math.log(v.visitCount)/c.visitCount)/sumVisitCount \\\n",
      " \t\t\t\t\t        + heuristic_coefficient*(transform(loc)/c.visitCount)/sumPseudoReward\t\n",
      " \t\t\t\tif debug:\n",
      "-\t\t\t\t\tprint a, funcVal\n",
      "+\t\t\t\t\tprint(a, funcVal)\n",
      " \n",
      " \t\t\tif funcVal > maxFuncVal:\n",
      " \t\t\t\tmaxFuncVal = funcVal\n",
      " \t\t\t\tbestAction = a\n",
      " \t\t\t\tbestChild = c\n",
      " \t\tif bestChild == None:\t## Tiebreaker\n",
      "-\t\t\tbestAction = random.choice(v.children.keys())\n",
      "+\t\t\tbestAction = random.choice(list(v.children.keys()))\n",
      " \t\t\tbestChild = v.children[bestAction]\n",
      " \n",
      " \t\tif debug:\n",
      "-\t\t\tprint \"\"\n",
      "+\t\t\tprint(\"\")\n",
      " \t\treturn bestAction, bestChild\n",
      " \n",
      " \tdef defaultPolicy(self, v, rle, step_horizon, domain_knowledge=False):\n",
      "@@ -598,7 +598,7 @@\n",
      " \n",
      " \t\t\titers += 1\n",
      " \t\t\t\n",
      "-\t\t\tif domain_knowledge and avatar_loc in self.actionDict.keys():\n",
      "+\t\t\tif domain_knowledge and avatar_loc in list(self.actionDict.keys()):\n",
      " \t\t\t\tsample = random.choice(self.actionDict[avatar_loc])\n",
      " \t\t\telse:\n",
      " \t\t\t\tsample = random.choice([(-1,0), (1,0), (0,-1), (0,1)])\n",
      "@@ -676,12 +676,12 @@\n",
      " \t\telif len(event)==2:\n",
      " \t\t\toutlist.append((event[0], getObjectColor(event[1])))\n",
      " \tif len(outlist)>0:\n",
      "-\t\tprint outlist\n",
      "+\t\tprint(outlist)\n",
      " \treturn outlist\n",
      " \n",
      " \n",
      " def observe(rle, obsSteps):\n",
      "-\tprint \"observing\"\n",
      "+\tprint(\"observing\")\n",
      " \tfor i in range(obsSteps):\n",
      " \t\tspriteInduction(rle._game, step=1)\n",
      " \t\tspriteInduction(rle._game, step=2)\n",
      "@@ -728,7 +728,7 @@\n",
      " \t\t\telif plannerType=='QLearning':\n",
      " \t\t\t\tplanner = QLearner(vrle, gameString=game, levelString=level)\n",
      " \t\t\t\tsubgoals = planner.getSubgoals(subgoal_path_threshold=10)\n",
      "-\t\t\tprint \"subgoals\", subgoals\n",
      "+\t\t\tprint(\"subgoals\", subgoals)\n",
      " \t\t\ttotal_steps = 0\n",
      " \t\t\tfor subgoal in subgoals:\n",
      " \t\t\t\tif not theory_change_flag and not goal_achieved:\n",
      "@@ -752,9 +752,9 @@\n",
      " \t\t\t\t\t\t\tterminal = rle._isDone()[0]\t\t\t\t\n",
      " \t\t\t\t\t\t\teffects = translateEvents(res['effectList'], all_objects)\n",
      " \t\t\t\t\t\t\tif symbolDict: \n",
      "-\t\t\t\t\t\t\t\tprint rle.show()\n",
      "+\t\t\t\t\t\t\t\tprint(rle.show())\n",
      " \t\t\t\t\t\t\telse:\n",
      "-\t\t\t\t\t\t\t\tprint np.reshape(new_state, rle.outdim)\n",
      "+\t\t\t\t\t\t\t\tprint(np.reshape(new_state, rle.outdim))\n",
      " \t\t\t\t\t\t\t# Save the event and agent state\n",
      " \t\t\t\t\t\t\ttry:\n",
      " \t\t\t\t\t\t\t\tagentState = dict(rle._game.getAvatars()[0].resources)\n",
      "@@ -775,7 +775,7 @@\n",
      " \t\t\t\t\t\t\t\t# \tgoal_achieved = True\n",
      " \t\t\t\t\t\t\t\tfor e in effects:\n",
      " \t\t\t\t\t\t\t\t\tif 'DARKBLUE' in e and colorDict[str(object_goal.color)] in e:\n",
      "-\t\t\t\t\t\t\t\t\t\tprint \"goal achieved\"\n",
      "+\t\t\t\t\t\t\t\t\t\tprint(\"goal achieved\")\n",
      " \t\t\t\t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\t\t\t\t\tgoal_achieved = True\n",
      " \n",
      "@@ -785,11 +785,11 @@\n",
      " \t\t\t\t\t\t\t\t\n",
      " \t\t\t\t\t\t\t\t## TODO: This crashed. Get it working again, then incorporate the sprite induction result.\n",
      " \t\t\t\t\t\t\t\tif len(rle._game.spriteDistribution)==0:\n",
      "-\t\t\t\t\t\t\t\t\tprint \"before step3\"\n",
      "+\t\t\t\t\t\t\t\t\tprint(\"before step3\")\n",
      " \t\t\t\t\t\t\t\t\tembed()\n",
      " \t\t\t\t\t\t\t\tspriteInduction(rle._game, step=3)\n",
      " \t\t\t\t\t\t\t\tif len(rle._game.spriteDistribution)==0:\n",
      "-\t\t\t\t\t\t\t\t\tprint \"after step3\"\n",
      "+\t\t\t\t\t\t\t\t\tprint(\"after step3\")\n",
      " \t\t\t\t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\t\t\t\t\t# if not sample:\n",
      "@@ -824,10 +824,10 @@\n",
      " \t\t\t\t\t\t\t\t\tfor e in finalEventList[-1]['effectList']:\n",
      " \t\t\t\t\t\t\t\t\t\tif e[1] == 'DARKBLUE':\n",
      " \t\t\t\t\t\t\t\t\t\t\tcandidate_new_colors.append(e[2])\n",
      "-\t\t\t\t\t\t\t\t\t\t\tprint \"appending\", e[2], \"to candidate_new_colors\"\n",
      "+\t\t\t\t\t\t\t\t\t\t\tprint(\"appending\", e[2], \"to candidate_new_colors\")\n",
      " \t\t\t\t\t\t\t\t\t\tif e[2] == 'DARKBLUE':\n",
      " \t\t\t\t\t\t\t\t\t\t\tcandidate_new_colors.append(e[1])\n",
      "-\t\t\t\t\t\t\t\t\t\t\tprint \"appending\", e[1], \"to candidate_new_colors\"\n",
      "+\t\t\t\t\t\t\t\t\t\t\tprint(\"appending\", e[1], \"to candidate_new_colors\")\n",
      " \n",
      " \t\t\t\t\t\t\t\t\tcandidate_new_colors = list(set(candidate_new_colors))\n",
      " \t\t\t\t\t\t\t\t\t# print \"candidate new colors\", candidate_new_colors\n",
      "@@ -836,7 +836,7 @@\n",
      " \t\t\t\t\t\t\t\t\tgame, level, symbolDict, immovables = writeTheoryToTxt(rle, hypotheses[0], symbolDict, \\\n",
      " \t\t\t\t\t\t\t\t\t\t\"./examples/gridphysics/theorytest.py\", goalLoc=(rle._rect2pos(object_goal.rect)[1], rle._rect2pos(object_goal.rect)[0]))\n",
      " \n",
      "-\t\t\t\t\t\t\t\t\tprint \"updating internal theory\"\n",
      "+\t\t\t\t\t\t\t\t\tprint(\"updating internal theory\")\n",
      " \t\t\t\t\t\t\t\t\tvrle = createMindEnv(game, level, output=False)\n",
      " \t\t\t\t\t\t\t\t\tvrle.immovables = immovables\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      " \t\t\t\t\t\t\t\telse:\n",
      "@@ -852,7 +852,7 @@\n",
      " \t\t\t\t\t\t\tif terminal:\n",
      " \t\t\t\t\t\t\t\treturn rle, hypotheses, finalEventList, candidate_new_colors, states_encountered, game_object\n",
      " \n",
      "-\t\t\t\t\tprint \"executed all actions.\"\n",
      "+\t\t\t\t\tprint(\"executed all actions.\")\n",
      " \t\t\ttotal_steps += steps\n",
      " \treturn rle, hypotheses, finalEventList, candidate_new_colors, states_encountered, game_object\n",
      " \n",
      "@@ -862,7 +862,7 @@\n",
      " \trle = rleCreateFunc(OBSERVATION_GLOBAL)\n",
      " \tgame, level = defInputGame(filename)\n",
      " \toutdim = rle.outdim\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \t\n",
      " \tterminal = rle._isDone()[0]\n",
      " \t\n",
      "@@ -883,11 +883,11 @@\n",
      " \n",
      " \t\tfor j in range(min(len(actions), max_actions_per_plan)):\n",
      " \t\t\tif actions[j] is not None and not terminal:\n",
      "-\t\t\t\tprint ACTIONS[actions[j]]\n",
      "+\t\t\t\tprint(ACTIONS[actions[j]])\n",
      " \t\t\t\tres = rle.step(actions[j])\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      " \t\t\t\tterminal = not res['pcontinue']\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\t\tfinalStates.append(rle._game.getFullState())\n",
      " \n",
      " \t\ti+=1\n",
      "@@ -907,9 +907,9 @@\n",
      " \tVrle = createMindEnv(theoryString, levelString, output=False)\n",
      " \tVrle.immovables = immovables\n",
      " \n",
      "-\tprint \"mental map with subgoal:\", subgoal\n",
      "-\tprint Vrle.show()\n",
      "-\tprint \"planner type\", plannerType\n",
      "+\tprint(\"mental map with subgoal:\", subgoal)\n",
      "+\tprint(Vrle.show())\n",
      "+\tprint(\"planner type\", plannerType)\n",
      " \tif plannerType=='mcts':\n",
      " \t\tmcts = Basic_MCTS(existing_rle=Vrle, game=theoryString, level=levelString, partitionWeights=partitionWeights)\n",
      " \t\t# print \"made mcts for subgoal,\", subgoal\n",
      "@@ -920,11 +920,11 @@\n",
      " \t\tplanner = QLearner(Vrle, gameString=theoryString, levelString=levelString)\n",
      " \t\tsteps = planner.learn(500, satisfice=True)\n",
      " \t\tactions = planner.getBestActionsForPlayout()\n",
      "-\tprint \"Found plan to subgoal. Actions\", actions\n",
      "+\tprint(\"Found plan to subgoal. Actions\", actions)\n",
      " \tif act:\n",
      " \t\tfor a in actions:\n",
      " \t\t\trle.step(a)\n",
      "-\t\t\tprint rle.show()\n",
      "+\t\t\tprint(rle.show())\n",
      " \treturn rle, actions, steps\n",
      " \n",
      " \n",
      "@@ -934,7 +934,7 @@\n",
      " \tgame, level = defInputGame(filename)\n",
      " \toutdim = rle.outdim\n",
      " \tsymbolDict = generateSymbolDict(rle)\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \n",
      " \tgoal_loc = np.where(np.reshape(rle._getSensors(), rle.outdim)==8)\n",
      " \tgoal_loc = goal_loc[0][0], goal_loc[1][0]\n",
      "@@ -954,7 +954,7 @@\n",
      " \n",
      " \tmcts = Basic_MCTS(existing_rle=rle, game=game, level=level, partitionWeights=[5,2,3])\n",
      " \tsubgoals = mcts.getSubgoals(subgoal_path_threshold=3)\n",
      "-\tprint \"subgoals\", subgoals\n",
      "+\tprint(\"subgoals\", subgoals)\n",
      " \n",
      " \t\n",
      " \ttotal_steps = 0\n",
      "@@ -963,16 +963,16 @@\n",
      " \tfor subgoal in subgoals:\n",
      " \t\trle, actions, steps = getToWaypoint(rle, subgoal, symbolDict, defaultPolicyMaxSteps, partitionWeights=[10,2,4])\n",
      " \t\tnumActions += len(actions)\n",
      "-\t\tprint steps, \"steps\"\n",
      "+\t\tprint(steps, \"steps\")\n",
      " \t\ttotal_steps += steps\n",
      " \t\tif total_steps > maxEpisodes:\n",
      " \t\t\tsolved = False\n",
      " \t\t\tbreak\n",
      " \n",
      " \tif solved:\n",
      "-\t\tprint \"Found and executed plan using\", total_steps, \"epiosodes of MCTS.\"\n",
      "+\t\tprint(\"Found and executed plan using\", total_steps, \"epiosodes of MCTS.\")\n",
      " \telse:\n",
      "-\t\tprint \"didn't solve game even using %i episodes of MCTS\"%total_steps\n",
      "+\t\tprint(\"didn't solve game even using %i episodes of MCTS\"%total_steps)\n",
      " \n",
      " \treturn mcts, total_steps, solved, numActions\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Can't parse VGDL_ontology.dot: ParseError: bad input: type=1, value='digraph', context=(' ', (1, 7))\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: There was 1 error:\n",
      "RefactoringTool: Can't parse VGDL_ontology.dot: ParseError: bad input: type=1, value='digraph', context=(' ', (1, 7))\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored metaplanner.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: metaplanner.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- metaplanner.py\t(original)\n",
      "+++ metaplanner.py\t(refactored)\n",
      "@@ -1,8 +1,8 @@\n",
      "-from ontology import distributionInitSetup\n",
      "-from WBP import *\n",
      "-from mcts import *\n",
      "-from qlearner import *\n",
      "-from aStar import *\n",
      "+from .ontology import distributionInitSetup\n",
      "+from .WBP import *\n",
      "+from .mcts import *\n",
      "+from .qlearner import *\n",
      "+from .aStar import *\n",
      " import time\n",
      " from termcolor import colored\n",
      " \n",
      "@@ -16,14 +16,14 @@\n",
      " \t\t\treturn None\n",
      " \t\telif objectID == 'EOS':\n",
      " \t\t\treturn 'ENDOFSCREEN'\n",
      "-\t\telif objectID in all_objects.keys():\n",
      "+\t\telif objectID in list(all_objects.keys()):\n",
      " \t\t\treturn all_objects[objectID]['type']['color']\n",
      "-\t\telif objectID in rle._game.getObjects().keys():\n",
      "+\t\telif objectID in list(rle._game.getObjects().keys()):\n",
      " \t\t\treturn rle._game.getObjects()[objectID]['type']['color']\n",
      "-\t\telif objectID in [colorDict[k] for k in colorDict.keys()]:\n",
      "+\t\telif objectID in [colorDict[k] for k in list(colorDict.keys())]:\n",
      " \t\t\t# If we were passed a color to begin with (i.e., in the case of EOS)\n",
      " \t\t\treturn objectID\n",
      "-\t\telif objectID in rle._game.sprite_groups.keys():\n",
      "+\t\telif objectID in list(rle._game.sprite_groups.keys()):\n",
      " \t\t\treturn colorDict[str(rle._game.sprite_groups[objectID][0].color)]\n",
      " \t\telif objectID in [obj.ID for obj in rle._game.kill_list]:\n",
      " \t\t\tobjectColor = [obj.color for obj in rle._game.kill_list\n",
      "@@ -32,7 +32,7 @@\n",
      " \t\telse:\n",
      " \t\t\t# for some reason we haven't been passed an ID but rather a sprite object\n",
      " \t\t\tobjectName = objectID.name\n",
      "-\t\t\tcolor = [all_objects[k]['type']['color'] for k in all_objects.keys() if all_objects[k]['sprite'].name==objectName][0]\n",
      "+\t\t\tcolor = [all_objects[k]['type']['color'] for k in list(all_objects.keys()) if all_objects[k]['sprite'].name==objectName][0]\n",
      " \t\t\treturn color\n",
      " \n",
      " \toutlist = []\n",
      "@@ -44,7 +44,7 @@\n",
      " \t\t\t# print 'in translateEvents', event\n",
      " \t\t\tif len(event) > 3:\n",
      " \t\t\t\ttmp = [event[0], getObjectColor(event[1]), getObjectColor(event[2])]\n",
      "-\t\t\t\tfor k in event[3].keys():\n",
      "+\t\t\t\tfor k in list(event[3].keys()):\n",
      " \t\t\t\t\tif k=='stype':\n",
      " \t\t\t\t\t\tevent[3][k] = getObjectColor(event[3][k])\n",
      " \t\t\t\ttmp.extend(event[3:])\n",
      "@@ -54,7 +54,7 @@\n",
      " \t\t\telif len(event)==2:\n",
      " \t\t\t\toutlist.append((event[0], getObjectColor(event[1])))\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"translateEvents failed\"\n",
      "+\t\t\tprint(\"translateEvents failed\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \t#Make sure events in timestep are unique (don't want to double-count things)\n",
      "@@ -63,13 +63,13 @@\n",
      " \t\tif o not in uniqueEventList:\n",
      " \t\t\tuniqueEventList.append(o)\n",
      " \tif len(uniqueEventList)>0:\n",
      "-\t\tprint uniqueEventList\n",
      "+\t\tprint(uniqueEventList)\n",
      " \treturn uniqueEventList\n",
      " \n",
      " \n",
      " def observe(rle, obsSteps, bestSpriteTypeDict, display=False):\n",
      " \tif display:\n",
      "-\t\tprint \"observing for {} steps\".format(obsSteps)\n",
      "+\t\tprint(\"observing for {} steps\".format(obsSteps))\n",
      " \tif obsSteps>0:\n",
      " \t\tfor i in range(obsSteps):\n",
      " \t\t\t# print rle.show()\n",
      "@@ -81,11 +81,11 @@\n",
      " \t\t\t# print \"step 2 took {} seconds\".format(time.time()-t1)\n",
      " \t\t\trle.step((0,0))\n",
      " \t\t\tif display:\n",
      "-\t\t\t\tprint \"score: {}, game tick: {}\".format(rle._game.score, rle._game.time)\n",
      "-\t\t\t\tprint rle.show(color='blue')\n",
      "+\t\t\t\tprint(\"score: {}, game tick: {}\".format(rle._game.score, rle._game.time))\n",
      "+\t\t\t\tprint(rle.show(color='blue'))\n",
      " \n",
      " \t\t\trle._game.nextPositions = {}\n",
      "-\t\t\tfor k, v in rle._game.all_objects.iteritems():\n",
      "+\t\t\tfor k, v in rle._game.all_objects.items():\n",
      " \t\t\t\trle._game.nextPositions[k] = (int(rle._game.all_objects[k]['sprite'].rect.x), int(rle._game.all_objects[k]['sprite'].rect.y))\n",
      " \t\t\t\ttry:\n",
      " \t\t\t\t\tif rle._game.previousPositions[k] != rle._game.nextPositions[k]:\n",
      "@@ -113,7 +113,7 @@\n",
      " \trle = rleCreateFunc(OBSERVATION_GLOBAL)\n",
      " \tgame, level = defInputGame(filename)\n",
      " \toutdim = rle.outdim\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \n",
      " \tterminal = rle._isDone()[0]\n",
      " \n",
      "@@ -134,11 +134,11 @@\n",
      " \n",
      " \t\tfor j in range(min(len(actions), max_actions_per_plan)):\n",
      " \t\t\tif actions[j] is not None and not terminal:\n",
      "-\t\t\t\tprint ACTIONS[actions[j]]\n",
      "+\t\t\t\tprint(ACTIONS[actions[j]])\n",
      " \t\t\t\tres = rle.step(actions[j])\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      " \t\t\t\tterminal = not res['pcontinue']\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\t\tfinalStates.append(rle._game.getFullState())\n",
      " \n",
      " \t\ti+=1\n",
      "@@ -155,7 +155,7 @@\n",
      " \tgame, level = defInputGame(filename)\n",
      " \toutdim = rle.outdim\n",
      " \tsymbolDict = generateSymbolDict(rle)\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \n",
      " \tgoal_loc = np.where(np.reshape(rle._getSensors(), rle.outdim)==8)\n",
      " \tgoal_loc = goal_loc[0][0], goal_loc[1][0]\n",
      "@@ -175,7 +175,7 @@\n",
      " \n",
      " \tmcts = Basic_MCTS(existing_rle=rle, game=game, level=level, partitionWeights=[5,2,3])\n",
      " \tsubgoals = mcts.getSubgoals(subgoal_path_threshold=3)\n",
      "-\tprint \"subgoals\", subgoals\n",
      "+\tprint(\"subgoals\", subgoals)\n",
      " \n",
      " \n",
      " \ttotal_steps = 0\n",
      "@@ -184,16 +184,16 @@\n",
      " \tfor subgoal in subgoals:\n",
      " \t\trle, actions = getToWaypoint(rle, subgoal, symbolDict, defaultPolicyMaxSteps, partitionWeights=[10,2,4])\n",
      " \t\tnumActions += len(actions)\n",
      "-\t\tprint steps, \"steps\"\n",
      "+\t\tprint(steps, \"steps\")\n",
      " \t\t# total_steps += steps\n",
      " \t\tif total_steps > maxEpisodes:\n",
      " \t\t\tsolved = False\n",
      " \t\t\tbreak\n",
      " \n",
      " \tif solved:\n",
      "-\t\tprint \"Found and executed plan using\", total_steps, \"epiosodes of MCTS.\"\n",
      "+\t\tprint(\"Found and executed plan using\", total_steps, \"epiosodes of MCTS.\")\n",
      " \telse:\n",
      "-\t\tprint \"didn't solve game even using %i episodes of MCTS\"%total_steps\n",
      "+\t\tprint(\"didn't solve game even using %i episodes of MCTS\"%total_steps)\n",
      " \n",
      " \treturn mcts, total_steps, solved, numActions\n",
      " \n",
      "@@ -247,7 +247,7 @@\n",
      " \n",
      " \ttheory = generateTheoryFromGame(rle)\n",
      " \n",
      "-\tprint \"in getToWayPoint\"\n",
      "+\tprint(\"in getToWayPoint\")\n",
      " \t# embed()\n",
      " \t# print \"making RLE in getToWaypoint, after generateTheoryFromGame()\"\n",
      " \ttheoryString, levelString, inverseMapping, immovables, killerObjects =\\\n",
      "@@ -255,9 +255,9 @@\n",
      " \tVrle = createMindEnv(theoryString, levelString, output=False)\n",
      " \tVrle.immovables, Vrle.killerObjects = immovables, killerObjects\n",
      " \n",
      "-\tprint \"mental map with subgoal\", subgoal\n",
      "-\tprint Vrle.show()\n",
      "-\tprint \"planner type\", plannerType\n",
      "+\tprint(\"mental map with subgoal\", subgoal)\n",
      "+\tprint(Vrle.show())\n",
      "+\tprint(\"planner type\", plannerType)\n",
      " \tif plannerType=='IW':\n",
      " \t\tplanner = IW(rle=Vrle, gameString=theoryString, levelString=levelString, gameFilename=Vrle.game_name, k=2)\n",
      " \t\tplanner.BFS(Vrle)\n",
      "@@ -275,11 +275,11 @@\n",
      " \telif plannerType=='AStar':\n",
      " \t\tplanner = AStar(Vrle, gameString=theoryString, levelString=levelString)\n",
      " \t\tpath, actions = planner.search()\n",
      "-\tprint \"Found plan to subgoal. Actions\", actions\n",
      "+\tprint(\"Found plan to subgoal. Actions\", actions)\n",
      " \tif act:\n",
      " \t\tfor a in actions:\n",
      " \t\t\trle.step(a)\n",
      "-\t\t\tprint rle.show()\n",
      "+\t\t\tprint(rle.show())\n",
      " \treturn rle, actions\n",
      " \n",
      " def objectGoalReached(effects, object_goal):\n",
      "@@ -287,7 +287,7 @@\n",
      " \tgoal_achieved = False\n",
      " \tfor e in effects:\n",
      " \t\tif 'DARKBLUE' in e and colorDict[str(object_goal.color)] in e:\n",
      "-\t\t\tprint \"goal achieved\"\n",
      "+\t\t\tprint(\"goal achieved\")\n",
      " \t\t\tgoal_achieved = True\n",
      " \t\t\tbreak\n",
      " \treturn goal_achieved\n",
      "@@ -346,8 +346,8 @@\n",
      " \n",
      " \t## Add newly-seen objects.\n",
      " \tcurrent_objects = rle._game.getObjects()\n",
      "-\tfor k in current_objects.keys():\n",
      "-\t\tif k not in all_objects.keys():\n",
      "+\tfor k in list(current_objects.keys()):\n",
      "+\t\tif k not in list(all_objects.keys()):\n",
      " \t\t\tall_objects[k] = current_objects[k]\n",
      " \n",
      " \tstates_encountered = [rle._game.getFullState()]\n",
      "@@ -373,7 +373,7 @@\n",
      " \t\t\telif plannerType=='AStar':\n",
      " \t\t\t\tplanner = AStar(vrle, gameString=game, levelString=level)\n",
      " \t\t\t\tsubgoals = planner.getSubgoals(subgoal_path_threshold=100) ##This finds subgoals by searching the entire space, so do this only once and then use the path.\n",
      "-\t\t\tprint \"subgoals\", subgoals\n",
      "+\t\t\tprint(\"subgoals\", subgoals)\n",
      " \n",
      " \t\t\t## if you can't find subgoals that get you to the goal, exit\n",
      " \t\t\tif len(subgoals)==0:\n",
      "@@ -385,7 +385,7 @@\n",
      " \t\t\t\tif not theory_change_flag and not goal_achieved and not resetSubgoals:\n",
      " \n",
      " \t\t\t\t\t## write subgoal to theory; initialize VRLE.\n",
      "-\t\t\t\t\tprint \"at top of metaplanner loop -- making theory\"\n",
      "+\t\t\t\t\tprint(\"at top of metaplanner loop -- making theory\")\n",
      " \t\t\t\t\tgame, level, symbolDict, immovables, killerObjects = writeTheoryToTxt(rle, hypotheses[0], symbolDict, \\\n",
      " \t\t\t\t\t\t\"./examples/gridphysics/theorytest.py\", subgoal)\n",
      " \t\t\t\t\tvrle = createMindEnv(game, level, output=False)\n",
      "@@ -414,18 +414,18 @@\n",
      " \t\t\t\t\t\t\texcept Exception as e:\t# TODO: how to process changes in resources that led to termination state?\n",
      " \t\t\t\t\t\t\t\t# agentState = defaultdict(lambda: 0)\n",
      " \t\t\t\t\t\t\t\tagentState = rle.agentStatePrev\n",
      "-\t\t\t\t\t\t\t\tprint \"didn't find agentState resources\"\n",
      "+\t\t\t\t\t\t\t\tprint(\"didn't find agentState resources\")\n",
      " \t\t\t\t\t\t\t\tembed()\n",
      " \n",
      " \n",
      "-\t\t\t\t\t\t\tprint \"agentState\", agentState\n",
      "+\t\t\t\t\t\t\tprint(\"agentState\", agentState)\n",
      " \t\t\t\t\t\t\tres = rle.step(noise(action))\n",
      " \n",
      " \n",
      " \t\t\t\t\t\t\t## Add newly-seen objects.\n",
      " \t\t\t\t\t\t\tcurrent_objects = rle._game.getObjects()\n",
      "-\t\t\t\t\t\t\tfor k in current_objects.keys():\n",
      "-\t\t\t\t\t\t\t\tif k not in all_objects.keys():\n",
      "+\t\t\t\t\t\t\tfor k in list(current_objects.keys()):\n",
      "+\t\t\t\t\t\t\t\tif k not in list(all_objects.keys()):\n",
      " \t\t\t\t\t\t\t\t\tall_objects[k] = current_objects[k]\n",
      " \t\t\t\t\t\t\t\t\tdistributionInitSetup(rle._game, k)\n",
      " \t\t\t\t\t\t\t\t\trle._game.ignoreList.append(k) ## this is a hack -- the point is to prevent spriteInduction from\n",
      "@@ -446,14 +446,14 @@\n",
      " \t\t\t\t\t\t\t# \tembed()\n",
      " \t\t\t\t\t\t\teffects = translateEvents(res['effectList'], all_objects, rle)\n",
      " \n",
      "-\t\t\t\t\t\t\tk = random.choice(rle._game.spriteDistribution.keys())\n",
      "+\t\t\t\t\t\t\tk = random.choice(list(rle._game.spriteDistribution.keys()))\n",
      " \n",
      " \t\t\t\t\t\t\tspriteInduction(rle._game, step=3)\n",
      " \n",
      " \t\t\t\t\t\t\tif symbolDict:\n",
      "-\t\t\t\t\t\t\t\tprint rle.show()\n",
      "+\t\t\t\t\t\t\t\tprint(rle.show())\n",
      " \t\t\t\t\t\t\telse:\n",
      "-\t\t\t\t\t\t\t\tprint np.reshape(new_state, rle.outdim)\n",
      "+\t\t\t\t\t\t\t\tprint(np.reshape(new_state, rle.outdim))\n",
      " \n",
      " \t\t\t\t\t\t\t## If there were collisions, update history and perform interactionSet induction if the collisions were novel.\n",
      " \t\t\t\t\t\t\tif effects:\n",
      "@@ -479,7 +479,7 @@\n",
      " \t\t\t\t\t\t\t\tall_effects = [item for sublist in [e['effectList'] for e in finalEventList] for item in sublist]\n",
      " \t\t\t\t\t\t\t\tif not all([e in all_effects for e in effects]):## TODO: make sure you write this so that it works with simultaneous effects.\n",
      " \n",
      "-\t\t\t\t\t\t\t\t\tprint \"new effects\", [e for e in effects if not e in all_effects]\n",
      "+\t\t\t\t\t\t\t\t\tprint(\"new effects\", [e for e in effects if not e in all_effects])\n",
      " \n",
      " \t\t\t\t\t\t\t\t\tfinalEventList.append(event)\n",
      " \t\t\t\t\t\t\t\t\tterminationCondition = {'ended': False, 'win':False, 'time':rle._game.time}\n",
      "@@ -494,13 +494,13 @@\n",
      " \t\t\t\t\t\t\t\t\t# hypotheses[0].display()\n",
      " \t\t\t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\t\t\t\tif len(hypotheses)>1:\n",
      "-\t\t\t\t\t\t\t\t\t\tprint \"more than one hypothesis\"\n",
      "+\t\t\t\t\t\t\t\t\t\tprint(\"more than one hypothesis\")\n",
      " \t\t\t\t\t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\t\t\t\t\t\tcandidate_new_colors = updateCandidateColors(hypotheses, finalEventList)\n",
      " \n",
      " \n",
      "-\t\t\t\t\t\t\t\t\tprint \"updating internal theory\"\n",
      "+\t\t\t\t\t\t\t\t\tprint(\"updating internal theory\")\n",
      " \t\t\t\t\t\t\t\t\t# print \"avatarLoc\", planner.findAvatarInRLE(rle)\n",
      " \t\t\t\t\t\t\t\t\t## update to incorporate what we've learned, keep the same subgoal for now; this will update at the top of the next loop.\n",
      " \t\t\t\t\t\t\t\t\tgame, level, symbolDict, immovables, killerObjects = writeTheoryToTxt(rle, hypotheses[0], symbolDict, \\\n",
      "@@ -518,7 +518,7 @@\n",
      " \t\t\t\t\t\t\t\t\t# hypotheses[0].display()\n",
      " \t\t\t\t\t\t\t\t\t# print \"\"\n",
      " \t\t\t\t\t\t\t\telse:\n",
      "-\t\t\t\t\t\t\t\t\tprint \"no new effects\", effects\n",
      "+\t\t\t\t\t\t\t\t\tprint(\"no new effects\", effects)\n",
      " \t\t\t\t\t\t\t\t\tfinalEventList.append(event)\n",
      " \t\t\t\t\t\t\t\t\tterminationCondition = {'ended': False, 'win':False, 'time':rle._game.time}\n",
      " \t\t\t\t\t\t\t\t\ttrace = ([TimeStep(e['agentAction'], e['agentState'], e['effectList'], e['gameState']) for e in finalEventList], terminationCondition)\n",
      "@@ -529,7 +529,7 @@\n",
      " \t\t\t\t\t\t\tif terminal:\n",
      " \t\t\t\t\t\t\t\treturn rle, hypotheses, finalEventList, candidate_new_colors, states_encountered, game_object\n",
      " \n",
      "-\t\t\t\t\tprint \"executed all actions.\"\n",
      "+\t\t\t\t\tprint(\"executed all actions.\")\n",
      " \t\t\t\t\t## If you finish all actions, vrle needs to reflect most recent state.\n",
      " \t\t\t\t\t## goalLoc will be overwritten once you find new subgoals at the top.\n",
      " \t\t\t\t\tgame, level, symbolDict, immovables, killerObjects = writeTheoryToTxt(rle, hypotheses[0], symbolDict, \\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored nonparallel_planning.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: nonparallel_planning.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- nonparallel_planning.py\t(original)\n",
      "+++ nonparallel_planning.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from pathos.multiprocessing import ProcessingPool\n",
      "-from main_agent import Agent\n",
      "+from .main_agent import Agent\n",
      " import time\n",
      " import dill\n",
      " \n",
      "@@ -47,7 +47,7 @@\n",
      " \n",
      "         def gen_color():\n",
      "         \tfrom vgdl.colors import colorDict\n",
      "-        \tcolor_list = colorDict.values()\n",
      "+        \tcolor_list = list(colorDict.values())\n",
      "         \tcolor_list = [c for c in color_list if c not in ['UUWSWF']]\n",
      "         \tfor color in color_list:\n",
      "         \t\tyield color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to youtube.py\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: youtube.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Can't parse oles.java: ParseError: bad input: type=1, value='controllers', context=(' ', (1, 8))\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: There was 1 error:\n",
      "RefactoringTool: Can't parse oles.java: ParseError: bad input: type=1, value='controllers', context=(' ', (1, 8))\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored agent2.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: agent2.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- agent2.py\t(original)\n",
      "+++ agent2.py\t(refactored)\n",
      "@@ -1,12 +1,12 @@\n",
      "-from util import *\n",
      "-from core import colorDict, VGDLParser, makeVideo, sys\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, \\\n",
      "+from .util import *\n",
      "+from .core import colorDict, VGDLParser, makeVideo, sys\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, \\\n",
      " MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt, generateSymbolDict\n",
      " # from metaplanner import *\n",
      " import importlib\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " \n",
      " ## For now, only implementing version of agent that can deal with single goals.\n",
      "@@ -77,7 +77,7 @@\n",
      " \t\ttry:\n",
      " \t\t\treturn self.getSpriteColor(rle._game.sprite_groups[spriteName][0])\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"getSpriteNameColor failed\"\n",
      "+\t\t\tprint(\"getSpriteNameColor failed\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \tdef objectSelectionPhase(self, unknownColors, allColors, rle):\n",
      "@@ -86,25 +86,25 @@\n",
      " \t\tepsilon = .1\n",
      " \t\t## With probability 1-epsilon, select known goal if it's known, otherwise unkown object.\n",
      " \t\tif len(self.goalColor)>0 and \\\n",
      "-\t\tlen([k for k in rle._game.sprite_groups.keys() if len(rle._game.sprite_groups[k])>0 and \\\n",
      "+\t\tlen([k for k in list(rle._game.sprite_groups.keys()) if len(rle._game.sprite_groups[k])>0 and \\\n",
      " \t\t\tself.getSpriteNameColor(k, rle) in self.goalColor])>0 and \\\n",
      " \t\t\trandom.random()>epsilon:\n",
      "-\t\t\tkey = random.choice([k for k in rle._game.sprite_groups.keys() if len(rle._game.sprite_groups[k])>0\\\n",
      "+\t\t\tkey = random.choice([k for k in list(rle._game.sprite_groups.keys()) if len(rle._game.sprite_groups[k])>0\\\n",
      " \t\t\t and self.getSpriteNameColor(k, rle) in self.goalColor])\n",
      " \t\t\tobjectGoal = rle._game.sprite_groups[key][0]\n",
      " \t\t\t# actualGoal = objectGoal\n",
      " \t\t\t# objectGoalLocation = rle._rect2posFlipCoords(objectGoal.rect)\n",
      "-\t\t\tprint \"Selecting from known goals\", list(set(self.goalColor))\n",
      "-\t\t\tprint \"\"\n",
      "+\t\t\tprint(\"Selecting from known goals\", list(set(self.goalColor)))\n",
      "+\t\t\tprint(\"\")\n",
      " \t\telse:\n",
      " \t\t\ttry:\n",
      " \t\t\t\tobjectGoal = selectObjectGoal(rle, unknownColors, allColors, self.killerObjects, method=\"random_then_nearest\")\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tprint(\"\")\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"no unknown objects and no goal? Embedding so you can debug.\"\n",
      "+\t\t\t\tprint(\"no unknown objects and no goal? Embedding so you can debug.\")\n",
      " \t\t\t\tembed()\n",
      " \t\tobjectGoalLocation = rle._rect2posFlipCoords(objectGoal.rect)\n",
      "-\t\tprint \"object goal is\", self.getSpriteColor(objectGoal), \"at location\", objectGoalLocation\n",
      "+\t\tprint(\"object goal is\", self.getSpriteColor(objectGoal), \"at location\", objectGoalLocation)\n",
      " \t\treturn objectGoal, objectGoalLocation\n",
      " \n",
      " \tdef initializeVrle(self, hypothesis, objectGoalLocation, rle):\n",
      "@@ -112,7 +112,7 @@\n",
      " \t\tgameString, levelString, symbolDict, immovables, killerObjects = writeTheoryToTxt(rle, hypothesis, self.symbolDict,\\\n",
      " \t\t\t\t \"./examples/gridphysics/theorytest.py\", objectGoalLocation)\n",
      " \n",
      "-\t\tprint \"Initializing mental theory *with* object goal\"\n",
      "+\t\tprint(\"Initializing mental theory *with* object goal\")\n",
      " \t\tVrle = createMindEnv(gameString, levelString, output=True)\n",
      " \t\tVrle.immovables, Vrle.killerObjects = immovables, killerObjects\n",
      " \t\treturn Vrle\n",
      "@@ -120,7 +120,7 @@\n",
      " \tdef VrleInitPhase(self, objectGoalLocation, rle):\n",
      " \t\t## Initialize multiple VRLEs, each corresponding to one hypothesis in self.hypotheses\n",
      " \t\tVRLEs = []\n",
      "-\t\tprint \"in VrleInitPhase.\", len(self.hypotheses), \"hypotheses\"\n",
      "+\t\tprint(\"in VrleInitPhase.\", len(self.hypotheses), \"hypotheses\")\n",
      " \t\tfor hypothesis in self.hypotheses:\n",
      " \t\t\tVRLEs.append(self.initializeVrle(hypothesis, objectGoalLocation, rle))\n",
      " \t\treturn VRLEs\n",
      "@@ -132,17 +132,17 @@\n",
      " \n",
      " \t\tgameObject = None\n",
      " \t\tfor i in range(numEpisodes):\n",
      "-\t\t\tprint \"Starting episode\", i\n",
      "+\t\t\tprint(\"Starting episode\", i)\n",
      " \t\t\tself.pickNewLevel(index=i)\n",
      " \t\t\tgameObject, won, eventList, statesEncountered = self.playEpisode(gameObject, finalEventList)\n",
      " \t\t\tfinalEventList.extend(eventList)\n",
      " \t\t\tVGDLParser.playGame(self.gameString, self.levelString, statesEncountered, persist_movie=True, make_images=True, make_movie=False, movie_dir=\"videos/\"+self.gameName, padding=10)\n",
      " \t\t\ttotalStatesEncountered.append(statesEncountered)\n",
      " \t\t\ttally.append(won)\n",
      "-\t\t\tprint \"Episode ended. Won:\", won\n",
      "-\t\t\tprint \"Have won\", sum(tally), \"out of\", len(tally), \"episodes\"\n",
      "+\t\t\tprint(\"Episode ended. Won:\", won)\n",
      "+\t\t\tprint(\"Have won\", sum(tally), \"out of\", len(tally), \"episodes\")\n",
      " \t\t\n",
      "-\t\tprint \"Won\", sum(tally), \"out of \", len(tally), \"episodes.\"\n",
      "+\t\tprint(\"Won\", sum(tally), \"out of \", len(tally), \"episodes.\")\n",
      " \t\tmakeVideo(movie_dir=\"videos/\"+self.gameName)\n",
      " \t\t# empty image directory\n",
      " \t\t# shutil.rmtree(\"images/tmp\")\n",
      "@@ -164,12 +164,12 @@\n",
      " \t\t# embed()\n",
      " \t\t# allColors = [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in rle._game.sprite_groups.keys()]\n",
      " \t\t##select only non-moving objects as goals. Avoids chasing, which takes forever at the moment.\n",
      "-\t\tallColors = [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in rle._game.sprite_groups.keys() if len(rle._game.getSprites(k))>0 and rle._game.sprite_groups[k][0].speed is None ]\n",
      "+\t\tallColors = [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in list(rle._game.sprite_groups.keys()) if len(rle._game.getSprites(k))>0 and rle._game.sprite_groups[k][0].speed is None ]\n",
      " \t\tallColors = [c for c in allColors if c!='DARKBLUE']\n",
      " \t\tunknownColors = [c for c in allColors if c not in self.knownColors]\n",
      " \n",
      "-\t\tprint \"unknown colors:\", unknownColors\n",
      "-\t\tprint \"Known colors:\", self.knownColors\n",
      "+\t\tprint(\"unknown colors:\", unknownColors)\n",
      "+\t\tprint(\"Known colors:\", self.knownColors)\n",
      " \n",
      " \t\tended, won = rle._isDone()\n",
      " \t\t## Start storing encountered states.\n",
      "@@ -179,10 +179,10 @@\n",
      " \t\t## initialize theory if necessary.\n",
      " \t\tif len(self.hypotheses) == 0:\n",
      " \t\t\tgameObject = self.initializeHypotheses(rle, allObjects, learnSprites=True)\n",
      "-\t\t\tprint \"initializing hypotheses\"\n",
      "+\t\t\tprint(\"initializing hypotheses\")\n",
      " \t\telse:\n",
      " \t\t\tgameObject = self.completeHypotheses(rle, allObjects)\n",
      "-\t\t\tprint \"had hypotheses -- completing them.\"\n",
      "+\t\t\tprint(\"had hypotheses -- completing them.\")\n",
      " \n",
      " \t\twhile not ended:\n",
      " \n",
      "@@ -197,7 +197,7 @@\n",
      " \t\t\t\t## VRLEs, hypothesis-selection-method .....\n",
      " \t\t\t\t## figures out plan determined as above\n",
      " \t\t\t\t## carries out plan.\n",
      "-\t\t\tprint \"calling getToObjecGoal\"\n",
      "+\t\t\tprint(\"calling getToObjecGoal\")\n",
      " \t\t\trle, self.hypotheses, finalEventList, candidateNewColors, statesEncountered, gameObject = \\\n",
      " \t\t\t\tgetToObjectGoal(rle, VRLEs[0], self.plannerType, gameObject, self.hypotheses[0], self.gameString, self.levelString, \\\n",
      " \t\t\t\t\tobjectGoal, allObjects, finalEventList, symbolDict=self.symbolDict)\n",
      "@@ -216,10 +216,10 @@\n",
      " \t\t\tfor col in candidateNewColors:\n",
      " \t\t\t\tif col not in self.knownColors:\n",
      " \t\t\t\t\tself.knownColors.append(col)\n",
      "-\t\t\t\t\tprint \"added\", col, \"to knownColors\"\n",
      "-\t\t\tprint \"updated known colors\", self.knownColors\n",
      "+\t\t\t\t\tprint(\"added\", col, \"to knownColors\")\n",
      "+\t\t\tprint(\"updated known colors\", self.knownColors)\n",
      " \t\t\tunknownColors = [c for c in unknownColors if c not in self.knownColors]\n",
      "-\t\t\tprint \"updated unknownColors\", unknownColors\n",
      "+\t\t\tprint(\"updated unknownColors\", unknownColors)\n",
      " \t\treturn gameObject, won, finalEventList, totalStatesEncountered\n",
      " \n",
      " if __name__ == \"__main__\":\n",
      "@@ -256,10 +256,10 @@\n",
      " \tplannerType = \"IW\"\n",
      " \t# plannerType = \"QLearning\"\n",
      " \t# plannerType = \"AStar\"\n",
      "-\tprint \"\"\n",
      "-\tprint \"Playing {} with {}\".format(filename, plannerType)\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"Playing {} with {}\".format(filename, plannerType))\n",
      " \tagent = Agent(filename, plannerType)\n",
      " \tt1 = time.time()\n",
      " \tnumEpisodes = 5\n",
      " \tagent.playMultipleEpisodes(numEpisodes)\n",
      "-\tprint \"Ended {} episodes of {} with planner {} in {} seconds\".format(numEpisodes, filename, plannerType, time.time()-t1)\n",
      "+\tprint(\"Ended {} episodes of {} with planner {} in {} seconds\".format(numEpisodes, filename, plannerType, time.time()-t1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xfa in position 9: invalid start byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored theoryTests.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: theoryTests.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- theoryTests.py\t(original)\n",
      "+++ theoryTests.py\t(refactored)\n",
      "@@ -1,7 +1,7 @@\n",
      "-from theory_template import *\n",
      "-from rlenvironmentnonstatic import defInputGame, createRLInputGame, createRLInputGameFromStrings\n",
      "-from metaplanner import observe\n",
      "-from ontology import *\n",
      "+from .theory_template import *\n",
      "+from .rlenvironmentnonstatic import defInputGame, createRLInputGame, createRLInputGameFromStrings\n",
      "+from .metaplanner import observe\n",
      "+from .ontology import *\n",
      " if __name__ == \"__main__\":\n",
      " \n",
      " \tfilename = \"examples.gridphysics.simpleGame_preconditions\"\n",
      "@@ -49,7 +49,7 @@\n",
      " \n",
      " \thypotheses = list(gameObject.runInduction(gameObject.spriteInductionResult, trace, 20, verbose=False)) ##if you resample or run sprite induction, this \n",
      " \n",
      "-\tprint \"found\", len(hypotheses), \"hypotheses\"\n",
      "+\tprint(\"found\", len(hypotheses), \"hypotheses\")\n",
      " \n",
      " \tembed()\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored similarity.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: similarity.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- similarity.py\t(original)\n",
      "+++ similarity.py\t(refactored)\n",
      "@@ -52,5 +52,5 @@\n",
      " \t\t\tcount += 1\n",
      " \tfor i in range(len(to_remove)):\n",
      " \t\ts2.remove(to_remove[i])\n",
      "-\tprint 'count', count\n",
      "+\tprint('count', count)\n",
      " \treturn 1./(1+count)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored mcts_clean.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: mcts_clean.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mcts_clean.py\t(original)\n",
      "+++ mcts_clean.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,14 +14,14 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from qlearner import *\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .qlearner import *\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -31,7 +31,7 @@\n",
      " \tdef __init__(self, existing_rle=False, game = None, level = None, partitionWeights=[1,0,1],\\\n",
      " \t\t         rleCreateFunc=False, obsType = OBSERVATION_GLOBAL, decay_factor=.8):\n",
      " \t\tif not existing_rle and not rleCreateFunc:\n",
      "-\t\t\tprint \"You must pass either an existing rle or an rleCreateFunc\"\n",
      "+\t\t\tprint(\"You must pass either an existing rle or an rleCreateFunc\")\n",
      " \t\t\treturn\n",
      " \n",
      " \t\tif existing_rle:\n",
      "@@ -79,10 +79,10 @@\n",
      " \t\t\timmovables = self.rle.immovables\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall', 'poison']\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self._obstypes.keys():\n",
      "+\t\t\tif i in list(self._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -114,7 +114,7 @@\n",
      " \t\twhile len(self.rewardQueue)>0:\n",
      " \t\t\tloc = self.rewardQueue.popleft()\n",
      " \t\t\tif loc not in self.processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tself.processed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -133,8 +133,8 @@\n",
      " \t\t\tVrle = copy.deepcopy(VRLE)\n",
      " \n",
      " \t\t\tif i%100==0 and len(rewards)>0:\n",
      "-\t\t\t\tprint \"Training cycle: %i\"%i\n",
      "-\t\t\t\tprint \"avg. rewards for last group of 100\", np.mean(rewards[-100:])\n",
      "+\t\t\t\tprint(\"Training cycle: %i\"%i)\n",
      "+\t\t\t\tprint(\"avg. rewards for last group of 100\", np.mean(rewards[-100:]))\n",
      " \n",
      " \t\t\tif defaultPolicySolveStep:\n",
      " \t\t\t\treward, v, iters = self.treePolicy(self.root, Vrle, step_horizon, \\\n",
      "@@ -178,11 +178,11 @@\n",
      " \n",
      " \t\t\t\tres = rle.step(a)\n",
      " \t\t\t\t\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\t\tterminal = rle._isDone()[0]\n",
      " \n",
      " \t\t\t\tif terminal != v.terminal or not np.array_equal(v.state, rle._getSensors()):\n",
      "-\t\t\t\t\tprint \"inconsistency in node and rle\"\n",
      "+\t\t\t\t\tprint(\"inconsistency in node and rle\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\t# terminal = (not res['pcontinue']) or (rle._avatar is None)\n",
      " \t\t\t\tif terminal:\n",
      "@@ -196,7 +196,7 @@\n",
      " \tdef getBestActionsForPlayout(self, partitionWeights):\n",
      " \t\tv = self.root\n",
      " \t\tactions = []\n",
      "-\t\twhile v and not v.terminal and len(v.children.keys())>0:\n",
      "+\t\twhile v and not v.terminal and len(list(v.children.keys()))>0:\n",
      " \t\t\ta, v = self.bestChild(v,partitionWeights)\n",
      " \t\t\tactions.append(a)\n",
      " \t\treturn actions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored induction.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: induction.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- induction.py\t(original)\n",
      "+++ induction.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " import sys, ast, time\n",
      "-from theory_template import Game, TimeStep\n",
      "+from .theory_template import Game, TimeStep\n",
      " from IPython import embed\n",
      " \n",
      " \n",
      "@@ -31,14 +31,14 @@\n",
      "     ## but we need a string that is an ID that maps to each type.)\n",
      "     def getObjectType(timestep, objectID, timesteps):\n",
      "         objType = None\n",
      "-        for k in timestep.gameState['objects'].keys():\n",
      "-            if objectID in [o['ID'] for o in timestep.gameState['objects'][k].values()]:\n",
      "+        for k in list(timestep.gameState['objects'].keys()):\n",
      "+            if objectID in [o['ID'] for o in list(timestep.gameState['objects'][k].values())]:\n",
      "                 objType = k\n",
      "                 return objType\n",
      "         if objType==None:\n",
      "             for t in timesteps:\n",
      "-                for k in t.gameState['objects'].keys():\n",
      "-                    if objectID in [o['ID'] for o in t.gameState['objects'][k].values()]:\n",
      "+                for k in list(t.gameState['objects'].keys()):\n",
      "+                    if objectID in [o['ID'] for o in list(t.gameState['objects'][k].values())]:\n",
      "                         objType = k\n",
      "                         return objType\n",
      "     #trace[0] contains all timesteps\n",
      "@@ -46,7 +46,7 @@\n",
      "         timestep = trace[0][i]\n",
      "         for j in range(len(timestep.events)):\n",
      "             event = timestep.events[j]\n",
      "-            print event\n",
      "+            print(event)\n",
      "             if len(event)==3:\n",
      "                 timestep.events[j] = (event[0], getObjectType(timestep, event[1], trace[0]), getObjectType(timestep, event[2], trace[0]))\n",
      "             elif len(event)==2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to Untitled.ipynb\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: Untitled.ipynb\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored util.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: util.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- util.py\t(original)\n",
      "+++ util.py\t(refactored)\n",
      "@@ -3,7 +3,7 @@\n",
      " import os\n",
      " import random\n",
      " import csv\n",
      "-import cPickle\n",
      "+import pickle\n",
      " \n",
      " ALNUM = '0123456789bcdefhijklmnpqrstuvwxyzQWERTYUIOPSDFHJKLZXCVBNM,./;[]<>?:`-=~!@#$%^&*()_+'\n",
      " CHARS = 'bcdefhijklmnpqrstuvwxyzQWERTYUIOPSDFHJKLZXCVBNM'\n",
      "@@ -35,7 +35,7 @@\n",
      " \tif n%2==1:\n",
      " \t\tdecomposition.append(0)\n",
      " \t\tn = n-1\n",
      "-\ti = len(rle._obstypes.keys())\n",
      "+\ti = len(list(rle._obstypes.keys()))\n",
      " \twhile i>0:\n",
      " \t\tif n>=2**i:\n",
      " \t\t\tdecomposition.append(i)\n",
      "@@ -52,22 +52,22 @@\n",
      " \tobjects = [rle._game.sprite_groups[o][0].colorName for o in objects]\n",
      " \ttry:\n",
      " \t\tif len(objects)==1:\n",
      "-\t\t\tif objects[0] not in symbolDict.keys():\n",
      "-\t\t\t\tidx = len(symbolDict.keys())\n",
      "+\t\t\tif objects[0] not in list(symbolDict.keys()):\n",
      "+\t\t\t\tidx = len(list(symbolDict.keys()))\n",
      " \t\t\t\tsymbolDict[objects[0]] = ALNUM[idx]\n",
      " \t\t\treturn symbolDict[objects[0]]\n",
      " \t\telse:\n",
      " \t\t\tfor item in itertools.permutations(objects):\n",
      "-\t\t\t\tif tuple(item) in symbolDict.keys():\n",
      "+\t\t\t\tif tuple(item) in list(symbolDict.keys()):\n",
      " \t\t\t\t\treturn symbolDict[tuple(item)]\n",
      " \n",
      "-\t\tif not any([tuple(k) in symbolDict.keys() for k in list(itertools.permutations(objects))]):\n",
      "-\t\t\tidx = len(symbolDict.keys())\n",
      "+\t\tif not any([tuple(k) in list(symbolDict.keys()) for k in list(itertools.permutations(objects))]):\n",
      "+\t\t\tidx = len(list(symbolDict.keys()))\n",
      " \t\t\tsymbolDict[tuple(objects)] = ALNUM[idx]\n",
      " \t\t\treturn ALNUM[idx]\n",
      " \texcept:\n",
      " \t\t# import ipdb; ipdb.set_trace()\n",
      "-\t\tprint \"objectsToSymbol problem.\"\n",
      "+\t\tprint(\"objectsToSymbol problem.\")\n",
      " \t\tembed()\n",
      " \n",
      " def getObjectColor(objectID, all_objects, game, colorDict):\n",
      "@@ -75,14 +75,14 @@\n",
      " \t\treturn None\n",
      " \telif objectID == 'EOS':\n",
      " \t\treturn 'ENDOFSCREEN'\n",
      "-\telif objectID in all_objects.keys():\n",
      "+\telif objectID in list(all_objects.keys()):\n",
      " \t\treturn all_objects[objectID]['type']['color']\n",
      "-\telif objectID in game.getObjects().keys():\n",
      "+\telif objectID in list(game.getObjects().keys()):\n",
      " \t\treturn game.getObjects()[objectID]['type']['color']\n",
      "-\telif objectID in [colorDict[k] for k in colorDict.keys()]:\n",
      "+\telif objectID in [colorDict[k] for k in list(colorDict.keys())]:\n",
      " \t\t# If we were passed a color to begin with (i.e., in the case of EOS)\n",
      " \t\treturn objectID\n",
      "-\telif objectID in game.sprite_groups.keys():\n",
      "+\telif objectID in list(game.sprite_groups.keys()):\n",
      " \t\treturn colorDict[str(game.sprite_groups[objectID][0].color)]\n",
      " \telif objectID in [obj.ID for obj in game.kill_list]:\n",
      " \t\tobjectColor = [obj.color for obj in ame.kill_list\n",
      "@@ -91,16 +91,16 @@\n",
      " \telse:\n",
      " \t\t# for some reason we haven't been passed an ID but rather a sprite object\n",
      " \t\tobjectName = objectID.name\n",
      "-\t\tcolor = [all_objects[k]['type']['color'] for k in all_objects.keys() if all_objects[k]['sprite'].name==objectName][0]\n",
      "+\t\tcolor = [all_objects[k]['type']['color'] for k in list(all_objects.keys()) if all_objects[k]['sprite'].name==objectName][0]\n",
      " \t\treturn color\n",
      " \n",
      " def extendColorDict(num):\n",
      " \tfor i in range(num):\n",
      " \t\tcolorName = make_random_name(CAPCHARS)\n",
      "-\t\tcolor = (random.choice(range(256)), random.choice(range(256)), random.choice(range(256)))\n",
      "-\t\tprint colorName + '=' + str(color)\n",
      "+\t\tcolor = (random.choice(list(range(256))), random.choice(list(range(256))), random.choice(list(range(256))))\n",
      "+\t\tprint(colorName + '=' + str(color))\n",
      " \t\tcolorDict[str(color)] = colorName\n",
      "-\tprint colorDict\n",
      "+\tprint(colorDict)\n",
      " \n",
      " def make_random_name(chars):\n",
      " \timport random\n",
      "@@ -133,7 +133,7 @@\n",
      " \t\twriter.writerow((game['modelType'], game['condition'], game['gameName'], levels_won, steps, planner_steps, score))\n",
      " \tf.close()\n",
      " def ccopy(obj):\n",
      "-\treturn cPickle.loads(cPickle.dumps(obj))\n",
      "+\treturn pickle.loads(pickle.dumps(obj))\n",
      " \n",
      " def str2bool(v):\n",
      "     if v.lower() in ('yes', 'true', 't', 'y', '1'):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored class_theory_template.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: class_theory_template.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- class_theory_template.py\t(original)\n",
      "+++ class_theory_template.py\t(refactored)\n",
      "@@ -1,7 +1,7 @@\n",
      " import pygame\n",
      " import sys\n",
      " sys.path.insert(0, '../')\n",
      "-from tools import Node, indentTreeParser\n",
      "+from .tools import Node, indentTreeParser\n",
      " from collections import defaultdict\n",
      " import os\n",
      " import uuid\n",
      "@@ -9,9 +9,9 @@\n",
      " import glob\n",
      " # import ipdb\n",
      " from IPython import embed\n",
      "-from core import *\n",
      "-from tools import roundedPoints\n",
      "-from ontology import colorDict\n",
      "+from .core import *\n",
      "+from .tools import roundedPoints\n",
      "+from .ontology import colorDict\n",
      " # from vgdl.core import *\n",
      " # from vgdl.tools import roundedPoints\n",
      " # from vgdl.ontology import colorDict\n",
      "@@ -29,7 +29,7 @@\n",
      " \n",
      "     # TODO: Should enforce proper syntax for properties\n",
      "     def display(self):\n",
      "-        print (self.vgdlType, self.color, self.className, self.args)\n",
      "+        print((self.vgdlType, self.color, self.className, self.args))\n",
      " \n",
      "     def __eq__(self, other):\n",
      "         return all([\n",
      "@@ -51,7 +51,7 @@\n",
      "         \"\"\" Whatever is visible in the global namespace (after importing the ontologies)\n",
      "         can be used in the VGDL, and is evaluated.\n",
      "         \"\"\"\n",
      "-        from ontology import * #@UnusedWildImport\n",
      "+        from .ontology import * #@UnusedWildImport\n",
      "         return eval(estr)\n",
      " \n",
      "     def _parseArgs(self, s,  sclass=None, args=None):\n",
      "@@ -81,7 +81,7 @@\n",
      "             if c.content == \"SpriteSet\":\n",
      "                 self.parseSprites(c.children)\n",
      "         #Return list of sprite types.\n",
      "-        return self.sprite_types.values()\n",
      "+        return list(self.sprite_types.values())\n",
      " \n",
      " \n",
      "     def parseSprites(self, snodes, parentclass=None, parentargs={}, parenttypes=[]):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored plotting.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: plotting.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- plotting.py\t(original)\n",
      "+++ plotting.py\t(refactored)\n",
      "@@ -18,7 +18,7 @@\n",
      "     \n",
      "     Black corresponds to non-state positions. \"\"\"\n",
      "     \n",
      "-    from ontology import LEFT, RIGHT, UP, DOWN\n",
      "+    from .ontology import LEFT, RIGHT, UP, DOWN\n",
      "     if len(states[0]) > 3:\n",
      "         polar = True\n",
      "         M = ones((size[0] * 2, size[1] * 2))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored tools.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: tools.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- tools.py\t(original)\n",
      "+++ tools.py\t(refactored)\n",
      "@@ -51,7 +51,7 @@\n",
      "     return [(p[0], p[1]) for p in [p1, p2a, p2b]]\n",
      " \n",
      " def roundedPoints(rect):    \n",
      "-    from ontology import BASEDIRS\n",
      "+    from .ontology import BASEDIRS\n",
      "     size = rect.size[0]\n",
      "     assert rect.size[1]==size, \"Assumes square shape.\"\n",
      "     size = size*0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored ai.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: ai.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ai.py\t(original)\n",
      "+++ ai.py\t(refactored)\n",
      "@@ -1,6 +1,6 @@\n",
      " \n",
      " import math\n",
      "-import core\n",
      "+from . import core\n",
      " from IPython import embed\n",
      " import pygame\n",
      " #from tools import logToFile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored mcts_pseudoreward_heuristic_b.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: mcts_pseudoreward_heuristic_b.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mcts_pseudoreward_heuristic_b.py\t(original)\n",
      "+++ mcts_pseudoreward_heuristic_b.py\t(refactored)\n",
      "@@ -1,23 +1,23 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      " import math\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " from threading import Thread\n",
      " from collections import defaultdict, deque\n",
      " import time\n",
      " import copy\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectSubgoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectSubgoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -40,7 +40,7 @@\n",
      " class Basic_MCTS:\n",
      " \tdef __init__(self, existing_rle=False, game = None, level = None, partitionWeights=[1,0,1], waypoint=None, rleCreateFunc=False, obsType = OBSERVATION_GLOBAL, decay_factor=.95, num_workers=1):\n",
      " \t\tif not existing_rle and not rleCreateFunc:\n",
      "-\t\t\tprint \"You must pass either an existing rle or an rleCreateFunc\"\n",
      "+\t\t\tprint(\"You must pass either an existing rle or an rleCreateFunc\")\n",
      " \t\t\treturn\n",
      " \t\t# assumption: not starting on terminal state\n",
      " \t\t\"\"\"\n",
      "@@ -92,7 +92,7 @@\n",
      " \t\tgoal_loc = np.where(np.reshape(self.rle._getSensors(), self.outdim)==goal_code)\n",
      " \t\tgoal_loc = goal_loc[0][0], goal_loc[1][0]\n",
      " \n",
      "-\t\tif 'avatar' in self._obstypes.keys():\n",
      "+\t\tif 'avatar' in list(self._obstypes.keys()):\n",
      " \t\t\tinverted_avatar_loc=self._obstypes['avatar'][0]\n",
      " \t\t\tavatar_loc = (inverted_avatar_loc[1], inverted_avatar_loc[0])\n",
      " \t\t\tself.avatar_code = np.reshape(self.rle._getSensors(), self.outdim)[avatar_loc[0]][avatar_loc[1]]\n",
      "@@ -117,9 +117,9 @@\n",
      " \t\tself.rewardDict = {goal_loc:self.maxPseudoReward}\n",
      " \t\tself.processed = [goal_loc]\n",
      " \n",
      "-\t\tprint \"maxPseudoreward\", self.maxPseudoReward\n",
      "-\t\tprint \"rewardScaling\", self.rewardScaling\n",
      "-\t\tprint \"partitionWeights\", self.partitionWeights\n",
      "+\t\tprint(\"maxPseudoreward\", self.maxPseudoReward)\n",
      "+\t\tprint(\"rewardScaling\", self.rewardScaling)\n",
      "+\t\tprint(\"partitionWeights\", self.partitionWeights)\n",
      " \t\tself.scanDomainForMovementOptions()\n",
      " \t\tself.propagateRewards(goal_loc)\n",
      " \n",
      "@@ -137,13 +137,13 @@\n",
      " \t\ttry:\n",
      " \t\t\timmovables = self.rle.immovables\n",
      " \t\t\t# immovables = ['wall']\n",
      "-\t\t\tprint \"immovables\", immovables\n",
      "+\t\t\tprint(\"immovables\", immovables)\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall']\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self._obstypes.keys():\n",
      "+\t\t\tif i in list(self._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -176,7 +176,7 @@\n",
      " \t\twhile len(self.rewardQueue)>0:\n",
      " \t\t\tloc = self.rewardQueue.popleft()\n",
      " \t\t\tif loc not in self.processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tself.processed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -186,10 +186,10 @@\n",
      " \n",
      " \tdef startTrainingPhase(self, numTrainingCycles, step_horizon, VRLE, mark_solution=False, solution_limit=20):\n",
      " \n",
      "-\t\tprint \"defaultPolicy steps\", step_horizon\n",
      "+\t\tprint(\"defaultPolicy steps\", step_horizon)\n",
      " \t\tif mark_solution:\n",
      "-\t\t\tprint \"will stop once we find\", solution_limit, \"solutions\"\n",
      "-\t\tprint \"\"\n",
      "+\t\t\tprint(\"will stop once we find\", solution_limit, \"solutions\")\n",
      "+\t\tprint(\"\")\n",
      " \t\t#track total iterations spent in treePolicy\n",
      " \t\ttree_policy_iters, default_policy_iters = 0, 0\n",
      " \t\trewards = []\n",
      "@@ -199,9 +199,9 @@\n",
      " \t\t\tVrle = copy.deepcopy(VRLE)\n",
      " \n",
      " \t\t\tif i%10==0 and len(rewards)>0:\n",
      "-\t\t\t\tprint \"Training cycle: %i\"%i\n",
      "-\t\t\t\tprint \"avg. rewards for last group of 10\", np.mean(rewards[-10:])\n",
      "-\t\t\t\tprint \"Partition weights\", [self.partitionWeights[0], self.printexplorationweight, self.printheuristicweight]\n",
      "+\t\t\t\tprint(\"Training cycle: %i\"%i)\n",
      "+\t\t\t\tprint(\"avg. rewards for last group of 10\", np.mean(rewards[-10:]))\n",
      "+\t\t\t\tprint(\"Partition weights\", [self.partitionWeights[0], self.printexplorationweight, self.printheuristicweight])\n",
      " \t\t\ttry:\n",
      " \t\t\t\tif defaultPolicySolveStep:\n",
      " \t\t\t\t\treward, v, iters = self.treePolicy(self.root, Vrle, step_horizon, \\\n",
      "@@ -210,7 +210,7 @@\n",
      " \t\t\t\t\treward, v, iters = self.treePolicy(self.root, Vrle, step_horizon)\n",
      " \n",
      " \t\t\texcept TypeError:\n",
      "-\t\t\t\tprint \"typeError in startTrainingphase\"\n",
      "+\t\t\t\tprint(\"typeError in startTrainingphase\")\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \t\t\ttree_policy_iters += iters\n",
      "@@ -218,7 +218,7 @@\n",
      " \t\t\t\treward, dPiters = self.defaultPolicy(v, Vrle, step_horizon, domain_knowledge=True)\n",
      " \t\t\t\tif reward >0 and not defaultPolicySolveStep:\n",
      " \t\t\t\t\tdefaultPolicySolveStep = i\n",
      "-\t\t\t\t\tprint \"found reward at step\", defaultPolicySolveStep\n",
      "+\t\t\t\t\tprint(\"found reward at step\", defaultPolicySolveStep)\n",
      " \n",
      " \n",
      " \t\t\t\tloc = np.where(np.reshape(v.state, self.outdim)==self.avatar_code)\n",
      "@@ -233,11 +233,11 @@\n",
      " \t\t\t\tself.num_solutions_found += 1\n",
      " \t\t\t\t# print \"found solution\"\n",
      " \t\t\t\tif self.num_solutions_found > solution_limit:\n",
      "-\t\t\t\t\tprint \"\"\n",
      "-\t\t\t\t\tprint VRLE.show()\n",
      "-\t\t\t\t\tprint \"found solution\", solution_limit, \"times in \", i, \"rounds.\"\n",
      "+\t\t\t\t\tprint(\"\")\n",
      "+\t\t\t\t\tprint(VRLE.show())\n",
      "+\t\t\t\t\tprint(\"found solution\", solution_limit, \"times in \", i, \"rounds.\")\n",
      " \t\t\t\t\tactions = self.getBestActionsForPlayout((1,0,0))\n",
      "-\t\t\t\t\tprint \"greedy path:\", actions\n",
      "+\t\t\t\t\tprint(\"greedy path:\", actions)\n",
      " \t\t\t\t\treturn self\n",
      " \t\t\trewards.append(reward)\n",
      " \t\t\tself.backup(v, reward)\n",
      "@@ -247,7 +247,7 @@\n",
      " \tdef getBestActionsForPlayout(self, partitionWeights):\n",
      " \t\tv = self.root\n",
      " \t\tactions = []\n",
      "-\t\twhile v and not v.terminal and len(v.children.keys())>0:\n",
      "+\t\twhile v and not v.terminal and len(list(v.children.keys()))>0:\n",
      " \t\t\ta, v = self.bestChild(v,partitionWeights, debug=False)\n",
      " \t\t\tactions.append(a)\n",
      " \t\treturn actions\n",
      "@@ -267,15 +267,15 @@\n",
      " \t\tcntr=0\n",
      " \t\tv = self.root\n",
      " \t\tif output:\n",
      "-\t\t\tprint \"current state\"\n",
      "+\t\t\tprint(\"current state\")\n",
      " \t\t\t# rle.show()\n",
      "-\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "+\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      " \t\tactions, nodes = [], []\n",
      " \t\twhile v and not v.terminal and cntr<numActions:\n",
      " \t\t\t# print v.children.iteritems()\n",
      " \t\t\tif output:\n",
      "-\t\t\t\tprint \"options\"\n",
      "-\t\t\t\tprint [(ACTIONS[k],c.qVal) for k,c in v.children.iteritems()]\n",
      "+\t\t\t\tprint(\"options\")\n",
      "+\t\t\t\tprint([(ACTIONS[k],c.qVal) for k,c in v.children.items()])\n",
      " \t\t\t# a, v = self.bestChild(v,0)\n",
      " \t\t\ta, v = self.bestChild(v,(1,0,0))\n",
      " \n",
      "@@ -283,12 +283,12 @@\n",
      " \t\t\tnodes.append(v)\n",
      " \t\t\tif output:\n",
      " \t\t\t\tif v:\n",
      "-\t\t\t\t\tprint \"selected\"\n",
      "-\t\t\t\t\tprint ACTIONS[a]\n",
      "-\t\t\t\t\tprint \"resulted in\"\n",
      "+\t\t\t\t\tprint(\"selected\")\n",
      "+\t\t\t\t\tprint(ACTIONS[a])\n",
      "+\t\t\t\t\tprint(\"resulted in\")\n",
      " \t\t\t\t\t# Can't use rle.show() here, as it's doing a replay, rather than using the actual RLE.\n",
      "-\t\t\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "-\t\t\t\t\tprint \"\"\n",
      "+\t\t\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      "+\t\t\t\t\tprint(\"\")\n",
      " \t\t\tcntr+=1\n",
      " \t\t# if v.terminal:\n",
      " \t\t# \tdistance = 0\n",
      "@@ -351,7 +351,7 @@\n",
      " \t\t\t\t\treturn reward, v, iters\n",
      " \t\t\t\telse:\n",
      " \t\t\t\t\tif v.terminal!=terminal or not np.array_equal(v.state,rle._getSensors()):\n",
      "-\t\t\t\t\t\tprint \"in treePolicy\"\n",
      "+\t\t\t\t\t\tprint(\"in treePolicy\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\t\t# terminal = (not res['pcontinue']) or (rle._avatar is None)\n",
      "@@ -363,7 +363,7 @@\n",
      " \t\t\t\t\treturn reward, v, iters\n",
      " \n",
      " \t\treturn reward, v, iters\n",
      "-\t\tprint \"end of treepolicy\"\n",
      "+\t\tprint(\"end of treepolicy\")\n",
      " \t\tembed()\n",
      " \n",
      " \tdef expand(self, v, rle, domain_knowledge=False):\n",
      "@@ -380,7 +380,7 @@\n",
      " \t\t\taction_choices = self.actions\n",
      " \n",
      " \t\tfor a in action_choices:\n",
      "-\t\t\tif a not in v.children.keys():\n",
      "+\t\t\tif a not in list(v.children.keys()):\n",
      " \t\t\t\texpand_action = a\n",
      " \t\t\t\tres = rle.step(a)\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      "@@ -431,13 +431,13 @@\n",
      " \tdef maxChild(self, v):\n",
      " \t\ttmp = np.where(np.reshape(v.state, self.rle.outdim)==1)\n",
      " \t\tavatar_loc = tmp[0][0], tmp[1][0]\n",
      "-\t\tqVals = [v.children[a].qVal for a in v.children.keys()]\n",
      "-\t\tif len(qVals)>0 and avatar_loc in self.neighborDict.keys() and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      "+\t\tqVals = [v.children[a].qVal for a in list(v.children.keys())]\n",
      "+\t\tif len(qVals)>0 and avatar_loc in list(self.neighborDict.keys()) and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      " \t\t\t\tmaxVal = max(qVals)\n",
      "-\t\t\t\tchoices = [(a,c) for (a,c) in v.children.items() if c.qVal==maxVal]\n",
      "-\t\t\t\tfor (a,c) in v.children.items():\n",
      "-\t\t\t\t\tprint a, c.qVal\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tchoices = [(a,c) for (a,c) in list(v.children.items()) if c.qVal==maxVal]\n",
      "+\t\t\t\tfor (a,c) in list(v.children.items()):\n",
      "+\t\t\t\t\tprint(a, c.qVal)\n",
      "+\t\t\t\tprint(\"\")\n",
      " \t\t\t\t# for (a,c) in choices:\n",
      " \t\t\t\t# \tprint a, c.qVal\n",
      " \t\t\t\t# \tprint np.reshape(c.state, self.rle.outdim)\n",
      "@@ -475,7 +475,7 @@\n",
      " \t\t\tself.printexplorationweight = exploration_coefficient\n",
      " \t\t\t# print \"heuristic coefficient is now\", heuristic_coefficient\n",
      " \n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tcontinue\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -511,7 +511,7 @@\n",
      " \n",
      " \t\t# print \"\"\n",
      " \t\t# print np.reshape(v.state, self.outdim)\n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tfuncVal = -float('inf')\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -583,7 +583,7 @@\n",
      " \t\t\t\tbestAction = a\n",
      " \t\t\t\tbestChild = c\n",
      " \t\tif bestChild == None:\t## Tiebreaker\n",
      "-\t\t\tbestAction = random.choice(v.children.keys())\n",
      "+\t\t\tbestAction = random.choice(list(v.children.keys()))\n",
      " \t\t\tbestChild = v.children[bestAction]\n",
      " \n",
      " \t\treturn bestAction, bestChild\n",
      "@@ -606,7 +606,7 @@\n",
      " \n",
      " \t\t\titers += 1\n",
      " \t\t\t\n",
      "-\t\t\tif domain_knowledge and avatar_loc in self.actionDict.keys():\n",
      "+\t\t\tif domain_knowledge and avatar_loc in list(self.actionDict.keys()):\n",
      " \t\t\t\tsample = random.choice(self.actionDict[avatar_loc])\n",
      " \t\t\telse:\n",
      " \t\t\t\tsample = random.choice([(-1,0), (1,0), (0,-1), (0,1)])\n",
      "@@ -710,12 +710,12 @@\n",
      " \t\telif len(event)==2:\n",
      " \t\t\toutlist.append((event[0], getObjectColor(event[1])))\n",
      " \tif len(outlist)>0:\n",
      "-\t\tprint outlist\n",
      "+\t\tprint(outlist)\n",
      " \treturn outlist\n",
      " \n",
      " \n",
      " def observe(rle, obsSteps):\n",
      "-\tprint \"observing\"\n",
      "+\tprint(\"observing\")\n",
      " \tfor i in range(obsSteps):\n",
      " \t\tspriteInduction(rle._game, step=1)\n",
      " \t\tspriteInduction(rle._game, step=2)\n",
      "@@ -747,8 +747,8 @@\n",
      " \t## TODO: this will be problematic when new objects appear, if you don't update it.\n",
      " \t# all_objects = rle._game.getObjects()\n",
      " \n",
      "-\tprint \"\"\n",
      "-\tprint \"object goal is\", colorDict[str(subgoal.color)], rle._rect2pos(subgoal.rect)\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"object goal is\", colorDict[str(subgoal.color)], rle._rect2pos(subgoal.rect))\n",
      " \t# actions_executed = []\n",
      " \tstates_encountered = []\n",
      " \twhile not terminal and not goal_achieved:\n",
      "@@ -774,7 +774,7 @@\n",
      " \n",
      " \t\t\t\teffects = translateEvents(res['effectList'], all_objects) ##TODO: this gets object colors, not IDs.\n",
      " \t\t\t\t\n",
      "-\t\t\t\tprint ACTIONS[actions[i]]\n",
      "+\t\t\t\tprint(ACTIONS[actions[i]])\n",
      " \t\t\t\trle.show()\n",
      " \n",
      " \t\t\t\t# if symbolDict:\n",
      "@@ -802,7 +802,7 @@\n",
      " \t\t\t\t\t\t\trle._game.collision_objects.add(effect[2])\n",
      " \n",
      " \t\t\t\t\tif colorDict[str(subgoal.color)] in [item for sublist in effects for item in sublist]:\n",
      "-\t\t\t\t\t\tprint \"reached subgoal\"\n",
      "+\t\t\t\t\t\tprint(\"reached subgoal\")\n",
      " \t\t\t\t\t\tgoal_achieved = True\n",
      " \t\t\t\t\t\tif subgoal.name in rle._game.unknown_objects:\n",
      " \t\t\t\t\t\t\trle._game.unknown_objects.remove(subgoal.name)\n",
      "@@ -860,9 +860,9 @@\n",
      " \t\t\t\tspriteInduction(rle._game, step=3)\n",
      " \t\tif terminal:\n",
      " \t\t\tif rle._isDone()[1]:\n",
      "-\t\t\t\tprint \"game won\"\n",
      "+\t\t\t\tprint(\"game won\")\n",
      " \t\t\telse:\n",
      "-\t\t\t\tprint \"Agent died.\"\n",
      "+\t\t\t\tprint(\"Agent died.\")\n",
      " \treturn rle, hypotheses, finalEventList, candidate_new_colors, states_encountered\n",
      " \n",
      " \n",
      "@@ -871,7 +871,7 @@\n",
      " \trle = rleCreateFunc(OBSERVATION_GLOBAL)\n",
      " \tgame, level = defInputGame(filename)\n",
      " \toutdim = rle.outdim\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \t\n",
      " \tterminal = rle._isDone()[0]\n",
      " \t\n",
      "@@ -892,11 +892,11 @@\n",
      " \n",
      " \t\tfor j in range(min(len(actions), max_actions_per_plan)):\n",
      " \t\t\tif actions[j] is not None and not terminal:\n",
      "-\t\t\t\tprint ACTIONS[actions[j]]\n",
      "+\t\t\t\tprint(ACTIONS[actions[j]])\n",
      " \t\t\t\tres = rle.step(actions[j])\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      " \t\t\t\tterminal = not res['pcontinue']\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\t\tfinalStates.append(rle._game.getFullState())\n",
      " \n",
      " \t\ti+=1\n",
      "@@ -912,7 +912,7 @@\n",
      " \trle = rleCreateFunc(OBSERVATION_GLOBAL)\n",
      " \tgame, level = defInputGame(filename)\n",
      " \toutdim = rle.outdim\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \t\n",
      " \tterminal = rle._isDone()[0]\n",
      " \t\n",
      "@@ -922,7 +922,7 @@\n",
      " \n",
      " \tmcts = Basic_MCTS(existing_rle=rle, game=game, level=level, partitionWeights=[2,1,0], waypoint = waypoint)\n",
      " \tmcts.startTrainingPhase(10000, defaultPolicyMaxSteps, rle, mark_solution=True, solution_limit=500)\n",
      "-\tprint \"ended trainingphase\"\n",
      "+\tprint(\"ended trainingphase\")\n",
      " \treturn mcts\n",
      " \n",
      " \t# while not terminal:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored hyperparameter_optimization.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: hyperparameter_optimization.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- hyperparameter_optimization.py\t(original)\n",
      "+++ hyperparameter_optimization.py\t(refactored)\n",
      "@@ -1,6 +1,6 @@\n",
      " from hyperopt import fmin, tpe, hp\n",
      " from pathos.multiprocessing import ProcessingPool\n",
      "-from main_agent import Agent\n",
      "+from .main_agent import Agent\n",
      " import time\n",
      " import dill\n",
      " \n",
      "@@ -30,7 +30,7 @@\n",
      " \n",
      "     def gen_color():\n",
      "     \tfrom vgdl.colors import colorDict\n",
      "-    \tcolor_list = colorDict.values()\n",
      "+    \tcolor_list = list(colorDict.values())\n",
      "     \tcolor_list = [c for c in color_list if c not in ['UUWSWF']]\n",
      "     \tfor color in color_list:\n",
      "     \t\tyield color\n",
      "@@ -96,4 +96,4 @@\n",
      "         f.write(str(best))\n",
      " \n",
      " pool = ProcessingPool(nodes=10)\n",
      "-pool.map(optimize_game, range(10))\n",
      "+pool.map(optimize_game, list(range(10)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored agent_backup.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: agent_backup.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- agent_backup.py\t(original)\n",
      "+++ agent_backup.py\t(refactored)\n",
      "@@ -1,14 +1,14 @@\n",
      " import multiprocessing as mp\n",
      " from functools import partial\n",
      "-from util import *\n",
      "+from .util import *\n",
      " import sys, traceback\n",
      "-from core import colorDict, VGDLParser, keyPresses\n",
      "-from ontology import *\n",
      "-from theory_template import Precondition, InteractionRule, TerminationRule, TimeoutRule, \\\n",
      "+from .core import colorDict, VGDLParser, keyPresses\n",
      "+from .ontology import *\n",
      "+from .theory_template import Precondition, InteractionRule, TerminationRule, TimeoutRule, \\\n",
      " SpriteCounterRule, MultiSpriteCounterRule, Theory, Game, writeTheoryToTxt, generateSymbolDict, \\\n",
      " generateTheoryFromGame, expandLine, expandSprites, proposePredicates, getRuleSetsForClassPairPredicate,\\\n",
      " interateThresholds\n",
      "-from class_theory_template import Sprite\n",
      "+from .class_theory_template import Sprite\n",
      " import os, subprocess, shutil\n",
      " from collections import defaultdict\n",
      " import importlib\n",
      "@@ -18,19 +18,19 @@\n",
      " import copy\n",
      " from math import ceil, floor\n",
      " import warnings\n",
      "-from rlenvironmentnonstatic import createRLInputGame, createRLInputGameFromStrings, defInputGame, createMindEnv\n",
      "-from stateobsnonstatic import buildTracker, UNOBSERVABLE_PREDICATES\n",
      "+from .rlenvironmentnonstatic import createRLInputGame, createRLInputGameFromStrings, defInputGame, createMindEnv\n",
      "+from .stateobsnonstatic import buildTracker, UNOBSERVABLE_PREDICATES\n",
      " from termcolor import colored\n",
      " from line_profiler import LineProfiler\n",
      " from vgdl.util import manhattanDist, manhattanDist2, LinkedDict\n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      "-from colors import colorDict\n",
      "-import copy_reg\n",
      "+from .colors import colorDict\n",
      "+import copyreg\n",
      " import types\n",
      " import heapq\n",
      " from tqdm import tqdm, trange\n",
      " \n",
      "-import WBP\n",
      "+from . import WBP\n",
      " from termcolor import colored\n",
      " from pathos.helpers import mp\n",
      " \n",
      "@@ -58,16 +58,16 @@\n",
      " \t\tself.episodeStepGenerated = None\n",
      " \t\n",
      " \tdef display(self):\n",
      "-\t\tprint \"\"\n",
      "-\t\tprint \"diagnosis: {}\".format(self.diagnosis)\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(\"diagnosis: {}\".format(self.diagnosis))\n",
      " \t\tif self.targetTokens:\n",
      "-\t\t\tprint \"targetTokens: {}\".format(self.targetTokens)\n",
      "+\t\t\tprint(\"targetTokens: {}\".format(self.targetTokens))\n",
      " \t\telse:\n",
      "-\t\t\tprint \"targetToken: {}\".format(self.targetToken)\n",
      "-\t\tprint \"targetClass: {}\".format(self.targetClass)\n",
      "-\t\tprint \"targetColor: {}\".format(self.targetColor)\n",
      "-\t\tprint \"intPairs: {}\".format(self.intPairs)\n",
      "-\t\tprint \"components addressed: {}\".format(self.componentsAddressed)\n",
      "+\t\t\tprint(\"targetToken: {}\".format(self.targetToken))\n",
      "+\t\tprint(\"targetClass: {}\".format(self.targetClass))\n",
      "+\t\tprint(\"targetColor: {}\".format(self.targetColor))\n",
      "+\t\tprint(\"intPairs: {}\".format(self.intPairs))\n",
      "+\t\tprint(\"components addressed: {}\".format(self.componentsAddressed))\n",
      " \n",
      " \tdef copy(self):\n",
      " \t\te                   \t= errorMapEntry()\n",
      "@@ -165,7 +165,7 @@\n",
      " \t\tspriteInduction(self.rle._game, step=1, action=None)\n",
      " \n",
      " \t\tspriteList = []\n",
      "-\t\tcolors = self.rle._game.observation['trackedObjects'].keys()\n",
      "+\t\tcolors = list(self.rle._game.observation['trackedObjects'].keys())\n",
      " \t\tfor color in colors:\n",
      " \t\t\ts = Sprite(vgdlType=ResourcePack, colorName=color)\n",
      " \t\t\tspriteList.append(s)\n",
      "@@ -179,7 +179,7 @@\n",
      " \t\t## Instantiate a hypothesis that each singleton class might be the avatar\n",
      " \t\tself.hypotheses = []\n",
      " \t\t## Grab all singleton classes and instantiate hypotheses that they are the avatar.\n",
      "-\t\tfor color in self.symbolDict.keys():\n",
      "+\t\tfor color in list(self.symbolDict.keys()):\n",
      " \t\t\tif len(getSpritesByColor(self.rle._game, color)) == 1:\n",
      " \t\t\t\tnewTheory = copy.deepcopy(initialTheory)\n",
      " \t\t\t\toldClassName = newTheory.spriteObjects[color].className\n",
      "@@ -195,7 +195,7 @@\n",
      " \t\t\t\t\t\trule.slot2='avatar'\n",
      " \n",
      " \t\t\t\t## Rename classes to ensure canonical ordering: c2, c3, ...\n",
      "-\t\t\t\tif min([int(k[1:]) for k in newTheory.classes.keys() if 'c' in k])>2:\n",
      "+\t\t\t\tif min([int(k[1:]) for k in list(newTheory.classes.keys()) if 'c' in k])>2:\n",
      " \t\t\t\t\tfor s in newTheory.spriteSet:\n",
      " \t\t\t\t\t\tif s.className is not None and 'c' in s.className:\n",
      " \t\t\t\t\t\t\ttmpClassName = s.className\n",
      "@@ -230,7 +230,7 @@\n",
      " \t\tresults = []\n",
      " \t\tfor n_level, level_game in enumerate(level_game_pairs):\n",
      " \n",
      "-\t\t\tprint(\"Playing level {}\".format(n_level))\n",
      "+\t\t\tprint((\"Playing level {}\".format(n_level)))\n",
      " \t\t\t(self.gameString, self.levelString) = level_game\n",
      " \n",
      " \t\t\tfor epoch in range(1):\n",
      "@@ -269,7 +269,7 @@\n",
      " \t\t\t\tepisodes.append((n_level, steps, win, score))\n",
      " \t\t\t\tepisodes_played += 1\n",
      " \t\t\t\tif win:\n",
      "-\t\t\t\t\tprint 'won'\n",
      "+\t\t\t\t\tprint('won')\n",
      " \t\t\t\t\tbreak\n",
      " \t\t\t\ti += 1\n",
      " \t\t\tif i < num_episodes_per_level:\n",
      "@@ -280,7 +280,7 @@\n",
      " \tdef playEpisode(self, n_level, episode_num, flexible_goals=False, win=False, first_time_playing_level=False):\n",
      " \t\t\n",
      " \t\tself.initializeEnvironment()\n",
      "-\t\tprint \"initializing RLE\"\n",
      "+\t\tprint(\"initializing RLE\")\n",
      " \t\t# embed()\n",
      " \t\tsteps, self.quits, self.longHorizonObservations = 0,0,0\n",
      " \t\tself.all_objects[episode_num] = self.rle._game.getAllObjects()\n",
      "@@ -298,7 +298,7 @@\n",
      " \t\t\n",
      " \t\tif not self.hypotheses:\n",
      " \t\t\tif n_level!=0 and episode_num!=0:\n",
      "-\t\t\t\tprint \"Have no hypotheses but not playing the first episode / first level!\"\n",
      "+\t\t\t\tprint(\"Have no hypotheses but not playing the first episode / first level!\")\n",
      " \t\t\t\tembed()\n",
      " \t\t\tself.initializeHypotheses()\n",
      " \t\t\tupdateTerminations(self.rle, self.hypotheses, addNoveltyRules=False)\n",
      "@@ -306,7 +306,7 @@\n",
      " \t\tif first_time_playing_level:\n",
      " \t\t\t## Add defaults to theories for any new objects.\n",
      " \t\t\t## All hypotheses have the same number of classes / know about the same colors\n",
      "-\t\t\tnewColors = [k for k in envReal._game.observation['trackedObjects'].keys() if k not in self.hypotheses[0].spriteObjects]\n",
      "+\t\t\tnewColors = [k for k in list(envReal._game.observation['trackedObjects'].keys()) if k not in self.hypotheses[0].spriteObjects]\n",
      " \t\t\tif newColors:\n",
      " \t\t\t\tself.observe(self.rle, episode_num, OBSERVATION_PERIOD_LENGTH)\n",
      " \t\t\t\tfrom vgdl.ontology import Resource\n",
      "@@ -334,10 +334,10 @@\n",
      "  \t\t\t\ttry:\n",
      "  \t\t\t\t\ta = envReal._game.observation['trackedObjects'][avatarColor][0]\n",
      "  \t\t\t\texcept:\n",
      "- \t\t\t\t\tprint \"Didn't find avatar\"\n",
      "+ \t\t\t\t\tprint(\"Didn't find avatar\")\n",
      "  \t\t\t\t\tembed()\n",
      " \n",
      "- \t\t\t\tfor k,v in envReal._game.observation['trackedObjects'][avatarColor][0].inventory.items():\n",
      "+ \t\t\t\tfor k,v in list(envReal._game.observation['trackedObjects'][avatarColor][0].inventory.items()):\n",
      "  \t\t\t\t\tresourceClass = h.spriteObjects[k].className\n",
      "  \t\t\t\t\tresourceAmount, limit = v[0], v[1]\n",
      "  \t\t\t\t\tresourceClass = h.spriteObjects[k].className\n",
      "@@ -418,11 +418,11 @@\n",
      " \t\t\t\tsolution = []\n",
      " \n",
      " \t\t\tif solution and not p.quitting:\n",
      "-\t\t\t\tprint \"=============================================\"\n",
      "-\t\t\t\tprint \"got solution of length\", len(solution)\n",
      "+\t\t\t\tprint(\"=============================================\")\n",
      "+\t\t\t\tprint(\"got solution of length\", len(solution))\n",
      " \t\t\t\tfor g in p.gameString_array:\n",
      "-\t\t\t\t\tprint colored(g, 'green')\n",
      "-\t\t\t\tprint \"=============================================\"\n",
      "+\t\t\t\t\tprint(colored(g, 'green'))\n",
      "+\t\t\t\tprint(\"=============================================\")\n",
      " \n",
      " \t\t\tif self.shortHorizon:\n",
      " \t\t\t\tif not solution:\n",
      "@@ -432,7 +432,7 @@\n",
      " \t\t\telse:\n",
      " \t\t\t\tif (not solution) or p.quitting:\n",
      " \t\t\t\t\tif self.longHorizonObservations<self.longHorizonObservationLimit:\n",
      "-\t\t\t\t\t\tprint \"Didn't get solution or decided to quit. Observing, then replanning.\"\n",
      "+\t\t\t\t\t\tprint(\"Didn't get solution or decided to quit. Observing, then replanning.\")\n",
      " \t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\tself.observe(self.rle, episode_num, num_steps=5)\n",
      " \t\t\t\t\t\tsolution = [] ## You may have gotten p.quitting but also a solution; make sure you don't try to act on that if the planner decided it wasn't worth it.\n",
      "@@ -441,7 +441,7 @@\n",
      " \t\t\t\t\t\tquitting = True\n",
      " \n",
      " \t\t\tif emptyPlans > self.emptyPlansLimit:\n",
      "-\t\t\t\tprint \"observing\"\n",
      "+\t\t\t\tprint(\"observing\")\n",
      " \t\t\t\t# embed()\n",
      " \t\t\t\tself.observe(self.rle, episode_num, num_steps=5)\n",
      " \n",
      "@@ -455,7 +455,7 @@\n",
      " \t\t\t\t\t## you reground based on that.\n",
      " \t\t\t\t\tpredictionError, regroundForKillerTypes, regroundForStochasticTypes = self.regroundOrNot(action_num, predictedEnvs, self.hypotheses[0])\n",
      " \n",
      "-\t\t\t\t\tprint bestScoresAndHypotheses\n",
      "+\t\t\t\t\tprint(bestScoresAndHypotheses)\n",
      " \t\t\t\t\tself.hypotheses = [item[1] for item in bestScoresAndHypotheses]\n",
      " \t\t\t\t\t\n",
      " \t\t\t\t\tif selectedHypotheses[0]!=self.hypotheses[0]:\n",
      "@@ -467,7 +467,7 @@\n",
      " \n",
      " \t\t\t\t\tended, win = self.rle._isDone()\n",
      " \t\t\t\t\tif regroundForKillerTypes or regroundForStochasticTypes: \n",
      "-\t\t\t\t\t\tprint \"got reground for killer or stochastic type. Replanning\"\n",
      "+\t\t\t\t\t\tprint(\"got reground for killer or stochastic type. Replanning\")\n",
      " \t\t\t\t\t\tbreak\n",
      " \n",
      " \t\t\t\t\tif ended:\n",
      "@@ -479,7 +479,7 @@\n",
      " \t\t\t\t## You failed the game either because you made a mistake you couldn't recover from or because you timed out in your search.\n",
      " \t\t\t\t## Search more deeply next time.\n",
      " \t\t\t\tself.max_nodes *= self.max_nodes_annealing\n",
      "-\t\t\t\tprint \"You got quitting==True from planner. Embedding to debug.\"\n",
      "+\t\t\t\tprint(\"You got quitting==True from planner. Embedding to debug.\")\n",
      " \t\t\t\t# embed()\n",
      " \t\t\t\treturn False, self.rle._game.score, steps\n",
      " \t\t\n",
      "@@ -489,15 +489,15 @@\n",
      " \t\tscore = self.rle._game.score\n",
      " \t\toutput = \"ended episode. Win={}                   \t\t\t\t\t\t  \".format(win)\n",
      " \t\tif win:\n",
      "-\t\t\tprint colored('________________________________________________________________', 'white', 'on_green')\n",
      "-\t\t\tprint colored('________________________________________________________________', 'white', 'on_green')\n",
      "-\n",
      "-\t\t\tprint colored(output, 'white', 'on_green')\n",
      "-\t\t\tprint colored('________________________________________________________________', 'white', 'on_green')\n",
      "+\t\t\tprint(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "+\t\t\tprint(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "+\n",
      "+\t\t\tprint(colored(output, 'white', 'on_green'))\n",
      "+\t\t\tprint(colored('________________________________________________________________', 'white', 'on_green'))\n",
      " \t\telse:\n",
      "-\t\t\tprint colored('________________________________________________________________', 'white', 'on_red')\n",
      "-\t\t\tprint colored(output, 'white', 'on_red')\n",
      "-\t\t\tprint colored('________________________________________________________________', 'white', 'on_red')\n",
      "+\t\t\tprint(colored('________________________________________________________________', 'white', 'on_red'))\n",
      "+\t\t\tprint(colored(output, 'white', 'on_red'))\n",
      "+\t\t\tprint(colored('________________________________________________________________', 'white', 'on_red'))\n",
      " \n",
      " \t\treturn win, score, steps\n",
      " \t\n",
      "@@ -507,9 +507,9 @@\n",
      " \n",
      " \tdef testEpisodes(self, epoch=0, actionSequences=None):\n",
      " \t\tnum_cores = mp.cpu_count()\n",
      "-\t\tprint \"num cores: {}\".format(num_cores)\n",
      "+\t\tprint(\"num cores: {}\".format(num_cores))\n",
      " \t\tif num_cores<40:\n",
      "-\t\t\tprint \"WARNING: running on < 40 cores.\"\n",
      "+\t\t\tprint(\"WARNING: running on < 40 cores.\")\n",
      " \n",
      " \t\tif actionSequences == None:\n",
      " \t\t\tactionSequences = [\n",
      "@@ -532,7 +532,7 @@\n",
      " \t\ttotalTimeStart = time.time()\n",
      " \n",
      " \t\tfor episode_num, actions in enumerate(actionSequences):\n",
      "-\t\t\tprint \"initializing RLE. Epoch={}\".format(epoch)\n",
      "+\t\t\tprint(\"initializing RLE. Epoch={}\".format(epoch))\n",
      " \n",
      " \t\t\tself.initializeEnvironment()\n",
      " \t\t\tself.all_objects[episode_num] = self.rle._game.getAllObjects() ## we need to store all_objects across multiple episodes\n",
      "@@ -545,24 +545,24 @@\n",
      " \n",
      " \t\t\tfor num, action in enumerate(actions):\n",
      " \t\t\t\tif self.rle._isDone()[0]:\n",
      "-\t\t\t\t\tprint \"Game is over.\"\n",
      "+\t\t\t\t\tprint(\"Game is over.\")\n",
      " \t\t\t\t\tbreak\n",
      "-\t\t\t\tprint \">>> Step\", num+1, \"of\", len(actions), \"<<<\"\n",
      "+\t\t\t\tprint(\">>> Step\", num+1, \"of\", len(actions), \"<<<\")\n",
      " \t\t\t\t## initialize VRLEs\n",
      " \t\t\t\ttheoryRLEs = VrleInitPhase(self.hypotheses, self.rle)\n",
      " \t\t\t\tt2 = time.time()\n",
      " \t\t\t\tscoresAndHypotheses = self.executeStep(episode_num, self.rleHistory, self.actionHistory, action, self.hypotheses)\n",
      "-\t\t\t\tprint \"\"\n",
      "-\t\t\t\tprint \"executed step in {} seconds\".format(time.time()-t2)\n",
      "-\t\t\t\tprint \"\"\n",
      "-\t\t\t\tself.scores, self.hypotheses = zip(*scoresAndHypotheses)\n",
      "-\n",
      "-\t\t\tprint \">>> Embedded at the end of episode {}\".format(episode_num)\n",
      "+\t\t\t\tprint(\"\")\n",
      "+\t\t\t\tprint(\"executed step in {} seconds\".format(time.time()-t2))\n",
      "+\t\t\t\tprint(\"\")\n",
      "+\t\t\t\tself.scores, self.hypotheses = list(zip(*scoresAndHypotheses))\n",
      "+\n",
      "+\t\t\tprint(\">>> Embedded at the end of episode {}\".format(episode_num))\n",
      " \t\t\tembed()\n",
      " \n",
      " \t\ttotalTime = time.time() - totalTimeStart\n",
      " \n",
      "-\t\treturn (totalTime, zip(self.scores, self.hypotheses))\n",
      "+\t\treturn (totalTime, list(zip(self.scores, self.hypotheses)))\n",
      " \n",
      " \tdef manageNewObjects(self, episode_num, hypotheses, envRealPrev, action):\n",
      " \t\t# embed()\n",
      "@@ -596,7 +596,7 @@\n",
      " \t\treturn newRle\n",
      " \n",
      " \tdef scoreAndFilterTheories(self, newTheories, episode_num, displayTheories=False):\n",
      "-\t\tprint \"top of scoreAndFilterTheories\"\n",
      "+\t\tprint(\"top of scoreAndFilterTheories\")\n",
      " \n",
      " \t\tpenalties, imaginedEffectsPerTheory = MultiEpisodeExperienceReplay(newTheories, self.rleHistory[:episode_num+1], \\\n",
      " \t\t\t\tself.actionHistory[:episode_num+1], method=EXPERIENCE_REPLAY_METHOD, displayTheories=False, assumeZeroErrorTheoryExists=self.assumeZeroErrorTheoryExists)\n",
      "@@ -604,24 +604,24 @@\n",
      " \t\tfor n,imaginedEffects in enumerate(imaginedEffectsPerTheory):\n",
      " \t\t\tnewTheories[n].setOfImaginedEffects = newTheories[n].setOfImaginedEffects.union(imaginedEffects)\n",
      " \n",
      "-\t\tscoreAndTheoryTuples = zip(penalties, newTheories)\n",
      "+\t\tscoreAndTheoryTuples = list(zip(penalties, newTheories))\n",
      " \t\tscoreAndTheoryTuples = sorted(scoreAndTheoryTuples, key=lambda x: (x[0], x[1].prior()))\n",
      " \n",
      " \t\tfor num, sh in reversed(list(enumerate(scoreAndTheoryTuples))):\n",
      " \t\t\tif num > 5:\n",
      " \t\t\t\tcontinue\n",
      "-\t\t\tprint \"Theory: {} | Error: {}\".format(num, sh[0])\n",
      "+\t\t\tprint(\"Theory: {} | Error: {}\".format(num, sh[0]))\n",
      " \t\t\tsh[1].display()\n",
      " \t\tscoreAndTheoryTuples = [s for s in scoreAndTheoryTuples if not hasattr(s[1],'trueTheory')]      \n",
      " \n",
      " \t\tscoresAndHypotheses = [(h[0],h[1]) for h in filterTheories(scoreAndTheoryTuples, percentile=30, max_num=30,\n",
      " \t\t\tproportionOfSpriteTheories=None, errorCutoff=ERRORCUTOFF, usePrior=True)]\n",
      " \n",
      "-\t\tprint \"Experience replay complete.\"\n",
      "+\t\tprint(\"Experience replay complete.\")\n",
      " \t\tfor num, sh in enumerate(scoresAndHypotheses):\n",
      "-\t\t\tprint \"Theory: {} | Error: {}\".format(num, sh[0])\n",
      "-\t\tprint \"\"\n",
      "-\t\tprint \"{} survived\".format(len(scoresAndHypotheses))\n",
      "+\t\t\tprint(\"Theory: {} | Error: {}\".format(num, sh[0]))\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(\"{} survived\".format(len(scoresAndHypotheses)))\n",
      " \n",
      " \t\treturn scoresAndHypotheses, scoreAndTheoryTuples\n",
      " \n",
      "@@ -631,7 +631,7 @@\n",
      " \t\t## because we end up replanning no matter what.\n",
      " \n",
      " \t\tif not predictedEnvs:\n",
      "-\t\t\tprint \"warning! empty predictedEnvs in regroundOrNot!\"\n",
      "+\t\t\tprint(\"warning! empty predictedEnvs in regroundOrNot!\")\n",
      " \t\t\treturn False , False , False\n",
      " \n",
      " \t\tmatchedEnvs, la, lb = matchEnvs(self.rle, predictedEnvs[step_number+1])\n",
      "@@ -665,10 +665,10 @@\n",
      " \t\t## If we learn anything about orientation in this step for a sprite that was created in a previous step,\n",
      " \t\t## go back in time and assign that orientation to the previous steps that sprite was in. This is so that when you set the state to what you \n",
      " \t\t## remember from the past, you can incorporate this knowledge.\n",
      "-\t\torientedSprites = [s for s in [item for sublist in envReal._game.observation['trackedObjects'].values() for item in sublist] if s.firstorientation]\n",
      "+\t\torientedSprites = [s for s in [item for sublist in list(envReal._game.observation['trackedObjects'].values()) for item in sublist] if s.firstorientation]\n",
      " \t\tif orientedSprites:\n",
      "-\t\t\tfor i in list(reversed(range(len(self.rleHistory[episode_num])-1))):\n",
      "-\t\t\t\tallSprites = [s for s in [item for sublist in self.rleHistory[episode_num][i]._game.observation['trackedObjects'].values() \\\n",
      "+\t\t\tfor i in list(reversed(list(range(len(self.rleHistory[episode_num])-1)))):\n",
      "+\t\t\t\tallSprites = [s for s in [item for sublist in list(self.rleHistory[episode_num][i]._game.observation['trackedObjects'].values()) \\\n",
      " \t\t\t\t\t\tfor item in sublist]]\n",
      " \t\t\t\tmadeChange = False\n",
      " \t\t\t\tfor sprite in orientedSprites:\n",
      "@@ -701,11 +701,11 @@\n",
      " \t\t_, new_sprites, _ = matchEnvs(envReal, envRealPrev)\n",
      " \t\tself.rle._game.sprite_appearances = new_sprites\n",
      " \n",
      "-\t\tprint \"\"\n",
      "-\t\tprint keyPresses[action]\n",
      "-\t\tprint self.rle.show(color='blue')\n",
      "-\n",
      "-\t\tprint \"evaluating {} old theories and proposing new ones\".format(len(theoryRLEs))\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(keyPresses[action])\n",
      "+\t\tprint(self.rle.show(color='blue'))\n",
      "+\n",
      "+\t\tprint(\"evaluating {} old theories and proposing new ones\".format(len(theoryRLEs)))\n",
      " \t\tupdateTerminations(self.rle, hypotheses, addNoveltyRules=False)\n",
      " \t\t\n",
      " \t\t## TODO: If you ever want to not always run testAndExpand, you should implement\n",
      "@@ -719,8 +719,8 @@\n",
      " \t\tnewTheories = list(set(newTheories))\n",
      " \n",
      " \t\tself.allTheories.extend(newTheories)\n",
      "-\t\tprint \"\"\n",
      "-\t\tprint \"Tested and expanded {} theories to produce {} child theories\".format(len(theoryRLEs), len(newTheories))\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(\"Tested and expanded {} theories to produce {} child theories\".format(len(theoryRLEs), len(newTheories)))\n",
      " \n",
      " \t\tif newTheories:\n",
      " \t\t\t# embed()\n",
      "@@ -730,7 +730,7 @@\n",
      " \t\t\t# embed()\n",
      " \n",
      " \t\t\tif len(bestScoresAndHypotheses) == 0:\t\n",
      "-\t\t\t\tprint \"***** WARNING ***** 0 hypotheses survived filter ***** TRYING AGAIN *****\"\n",
      "+\t\t\t\tprint(\"***** WARNING ***** 0 hypotheses survived filter ***** TRYING AGAIN *****\")\n",
      " \t\t\t\t# print \"Addressing remaining error maps for {} theories\".format(len(newTheories))\n",
      " \t\t\t\t# embed()\n",
      " \n",
      "@@ -746,7 +746,7 @@\n",
      " \t\t\t\t\tnewerTheories = list(set(newerTheories))\n",
      " \n",
      " \t\t\t\tif len(newerTheories) == 0:\n",
      "-\t\t\t\t\tprint \"***** WARNING ***** still no hypotheses surviving filter when assumeZeroErrorTheoryExists is False, trying again\"\n",
      "+\t\t\t\t\tprint(\"***** WARNING ***** still no hypotheses surviving filter when assumeZeroErrorTheoryExists is False, trying again\")\n",
      " \t\t\t\t\tfor t in newTheories:\n",
      " \t\t\t\t\t\ttheories = addressRemainingErrorMaps(t, envRealPrev, self.rle, action, self.rleHistory, self.actionHistory)\n",
      " \t\t\t\t\t\tnewerTheories.extend(theories)\n",
      "@@ -758,12 +758,12 @@\n",
      " \t\t\t\tfor s , h in bestScoresAndHypotheses:\n",
      " \t\t\t\t\th.dryingPaint = set()\n",
      " \t\t\tif len(bestScoresAndHypotheses) == 0:\n",
      "-\t\t\t\tprint \"second attempt failed, 0 theories survived filter\"\n",
      "+\t\t\t\tprint(\"second attempt failed, 0 theories survived filter\")\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \t\telse:\n",
      "-\t\t\tprint \"WARNING: You are returning 'bestScoresAndHypotheses' but these scores are fake and are\\\n",
      "-\t\t\t\t\treally the result of not scoring theories for the beginning observation period.\"\n",
      "+\t\t\tprint(\"WARNING: You are returning 'bestScoresAndHypotheses' but these scores are fake and are\\\n",
      "+\t\t\t\t\treally the result of not scoring theories for the beginning observation period.\")\n",
      " \t\t\tbestScoresAndHypotheses = [(0.0, h) for h in hypotheses]\n",
      " \n",
      " \t\tself.statesEncountered.append(self.rle._game.getFullState())\n",
      "@@ -784,7 +784,7 @@\n",
      " def setSpriteState(sprite, matchingSprite, hypothesis):\n",
      " \n",
      " \tif not matchingSprite:\n",
      "-\t\tprint \"WARNING: didn't find matching sprite in setSpriteState; this shouldn't happen\"\n",
      "+\t\tprint(\"WARNING: didn't find matching sprite in setSpriteState; this shouldn't happen\")\n",
      " \t\tembed()\n",
      " \n",
      " \tsprite.rect \t\t= pygame.Rect(matchingSprite.rect.left, matchingSprite.rect.top, matchingSprite.rect.width, matchingSprite.rect.height)\n",
      "@@ -792,7 +792,7 @@\n",
      " \tsprite.lastmove \t= matchingSprite.lastmove\n",
      " \tsprite.ID \t\t\t= matchingSprite.ID\n",
      " \tsprite.resources \t= defaultdict(int)\n",
      "-\tfor rcolor in matchingSprite.inventory.keys():\n",
      "+\tfor rcolor in list(matchingSprite.inventory.keys()):\n",
      " \t\tif rcolor not in hypothesis.spriteObjects:\n",
      " \t\t\tcontinue\n",
      " \t\t\t# print 'in setSpriteState: next line is going to crash'\n",
      "@@ -850,7 +850,7 @@\n",
      " \t## this means the RotatingAvatar's orientation is actually RIGHT now, and can set it to that.\n",
      " \t## IMPORTANT: This means that each sprite's orientation will only be correct if this function is called between each time-step.\n",
      " \tif debug:\n",
      "-\t\tprint \"in setVrleState\"\n",
      "+\t\tprint(\"in setVrleState\")\n",
      " \t\tembed()\n",
      " \tif not makeInitialVrle:\n",
      " \t\ttmp_kill_list = []\n",
      "@@ -872,7 +872,7 @@\n",
      " \t\t\t\t\t## Make as many new sprites as you need and put them at (0,0); we'll set their position and state below.\n",
      " \t\t\t\t\t[Vrle._game._createSprite([classKey], (0,0)) for n in range(rleSpriteCount-vrleSpriteCount)]\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"problem in setVrleState\"\n",
      "+\t\t\t\t\tprint(\"problem in setVrleState\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t# Now copy over sprite state (if we have any left of that type)\n",
      " \t\t\tif not Vrle._game.sprite_groups[classKey]:\n",
      "@@ -903,17 +903,17 @@\n",
      " \t\t# Vrle = theoryRLE if theoryRLE else createMindEnv(gameString, levelString, output=False)\n",
      " \t\tVrle = createMindEnv(gameString, levelString, output=False)\n",
      " \texcept:\n",
      "-\t\tprint \"in initializeVrle\"\n",
      "+\t\tprint(\"in initializeVrle\")\n",
      " \t\tembed()\n",
      "-\tVrle._game.colorToClassDict = {k:v.className for k,v in hypothesis.spriteObjects.items()}\n",
      "+\tVrle._game.colorToClassDict = {k:v.className for k,v in list(hypothesis.spriteObjects.items())}\n",
      " \tif makeInitialVrle:\n",
      "-\t\tfor k,v in Vrle._game.sprite_groups.iteritems():\n",
      "+\t\tfor k,v in Vrle._game.sprite_groups.items():\n",
      " \t\t\tif v:\n",
      " \t\t\t\tVrle._game.extra_sprites[k] = copy.deepcopy(v[0])\n",
      " \n",
      " \t## Don't do any of the rest if we have an ungrammatical hypothesis caused by num(avatars)>1.\n",
      " \tif len(stateToSet._game.observation['trackedObjects'][hypothesis.classes['avatar'][0].colorName])>1:\n",
      "-\t\tprint \"Warning. In initializeVrle. Got more than one avatar. Returning None as Vrle.\"\n",
      "+\t\tprint(\"Warning. In initializeVrle. Got more than one avatar. Returning None as Vrle.\")\n",
      " \t\t# embed()\n",
      " \t\tVrle = None\n",
      " \t\treturn Vrle\n",
      "@@ -1016,7 +1016,7 @@\n",
      " \n",
      " \t## Check for an ungrammatical theory.\n",
      " \tif envA is None:\n",
      "-\t\tprint \"Warning: got ungrammatical theory\"\n",
      "+\t\tprint(\"Warning: got ungrammatical theory\")\n",
      " \t\te = errorMapEntry()\n",
      " \t\te.diagnosis.append('ungrammatical theory')\n",
      " \t\te.targetToken = None\n",
      "@@ -1036,7 +1036,7 @@\n",
      " \t\t\tlonely_sprites_envB = [s for s in lonely_sprites_envB if s.colorName == targetColor]\n",
      " \n",
      " \t\texcept:\n",
      "-\t\t\tprint \"targetClass filter in errorSignal failed\"\n",
      "+\t\t\tprint(\"targetClass filter in errorSignal failed\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \t## Penalize distance and additional/missing sprites\n",
      "@@ -1056,10 +1056,10 @@\n",
      " \t\t\n",
      " \t\t## Calculate inventory penalty for the sprite\n",
      " \t\tinventory_penalty = 0\n",
      "-\t\tkeys = list(set(t[0].inventory.keys()+t[1].inventory.keys()))\n",
      "+\t\tkeys = list(set(list(t[0].inventory.keys())+list(t[1].inventory.keys())))\n",
      " \t\tfor k in keys:\n",
      "-\t\t\tt0_k = t[0].inventory[k] if k in t[0].inventory.keys() else (0,0)\n",
      "-\t\t\tt1_k = t[1].inventory[k] if k in t[1].inventory.keys() else (0,0)\n",
      "+\t\t\tt0_k = t[0].inventory[k] if k in list(t[0].inventory.keys()) else (0,0)\n",
      "+\t\t\tt1_k = t[1].inventory[k] if k in list(t[1].inventory.keys()) else (0,0)\n",
      " \t\t\tinventory_penalty += abs(t0_k[0]-t1_k[0])\n",
      " \t\ttotal_penalty += np.log((e_inventory)**inventory_penalty) #likelihood\n",
      " \n",
      "@@ -1096,9 +1096,9 @@\n",
      " \t\t\t\n",
      " \t\t\t# If RandomNPC: compare sB position to where it could have been given the hypothetical speed and random direction\n",
      " \t\t\tif 'Random' in str(sA_type):\n",
      "-\t\t\t\tif 'speed' in theory.spriteObjects[sA.colorName].args.keys():\n",
      "+\t\t\t\tif 'speed' in list(theory.spriteObjects[sA.colorName].args.keys()):\n",
      " \t\t\t\t\tsA_speed = theory.spriteObjects[sA.colorName].args['speed']\n",
      "-\t\t\t\telif 'speed' in theory.spriteObjects[sA.colorName].__dict__.keys():\n",
      "+\t\t\t\telif 'speed' in list(theory.spriteObjects[sA.colorName].__dict__.keys()):\n",
      " \t\t\t\t\tsA_speed = theory.spriteObjects[sA.colorName].speed\n",
      " \t\t\t\telse:\n",
      " \t\t\t\t\t## this only happens when you initialize the real theory for testing but haven't explicitly set the speed\n",
      "@@ -1170,7 +1170,7 @@\n",
      " \t\t\t\terrs = diagnosePosMismatch(sA, sB, sPrev, envA, envB, envPrev, dist_ts, theory)\n",
      " \t\t\t\terrorMap.extend(errs)\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"reportError problem\"\n",
      "+\t\t\tprint(\"reportError problem\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \t\tif penalty_only and earlyStopping and 1.-np.exp(total_penalty) > 0.0001:\n",
      "@@ -1208,7 +1208,7 @@\n",
      " \n",
      " \t\tif candidates_in_killList == []:\n",
      " \t\t\tif not sPrev: #there is no envA sprite where sPrev should have been\n",
      "-\t\t\t\tprint \"empty killList in A, meaning the matching is wrong\"\n",
      "+\t\t\t\tprint(\"empty killList in A, meaning the matching is wrong\")\n",
      " \t\t\t\t## You need to figure out what to pass to diagnosePosMismatch for sA, since it\n",
      " \t\t\t\t## doesn't exist.\n",
      " \t\t\t\tembed()\n",
      "@@ -1220,7 +1220,7 @@\n",
      " \t\t\t\t## if there was a kill event and an appearance event somewhere far, we should really see this as\n",
      " \t\t\t\t## an appearance\n",
      " \t\t\t\t## Really, you should look at sprite matching better.\n",
      "-\t\t\t\tprint \"manhattanDist2 > 1\"\n",
      "+\t\t\t\tprint(\"manhattanDist2 > 1\")\n",
      " \t\t\t\tembed()\n",
      " \t\t\t\tappeared_sprites_envB.append(sB)\n",
      " \t\t\t\tcontinue\n",
      "@@ -1270,7 +1270,7 @@\n",
      " \t\tsB = findNearestSprite(sA, candidates_in_killList)\n",
      " \n",
      " \t\tif not sB:\n",
      "-\t\t\tprint \"WARNING: No target and interaction pair found in object destruction. You have not implemented anything resulting from that diagnosis.\"\n",
      "+\t\t\tprint(\"WARNING: No target and interaction pair found in object destruction. You have not implemented anything resulting from that diagnosis.\")\n",
      " \t\t\te.diagnosis.append('objectDidNotAppear')\n",
      " \t\t\te.targetToken = None\n",
      " \t\t\te.intPairs = []\n",
      "@@ -1292,7 +1292,7 @@\n",
      " \t\terrorMap.append(e)\n",
      " \t# 2.3) Appearance\n",
      " \tfor sB in appeared_sprites_envB:\n",
      "-\t\tprint \"WARNING: Found unexpected appearance\"\n",
      "+\t\tprint(\"WARNING: Found unexpected appearance\")\n",
      " \t\te = errorMapEntry()\n",
      " \t\te.diagnosis.append('newObjectAppeared')\n",
      " \t\te.targetToken = sB\n",
      "@@ -1310,23 +1310,23 @@\n",
      " \t\t# Simultaneously find culprit classes - an overlapping sprite could have launched the sprite due to its class\n",
      " \t\t\n",
      " \t\tneighbors_curr_and_prev = neighboringSpritesColors(envB, sB) + neighboringSpritesColors(envPrev, sB)\n",
      "-\t\tnearestSprites = findNearestSprite(sB, [item for sublist in envA._game.observation['trackedObjects'].values() for item in sublist])\n",
      "+\t\tnearestSprites = findNearestSprite(sB, [item for sublist in list(envA._game.observation['trackedObjects'].values()) for item in sublist])\n",
      " \n",
      " \t\tfor nearestSprite in nearestSprites:\n",
      " \t\t\tif nearestSprite.colorName in neighbors_curr_and_prev:\n",
      " \t\t\t\te.intPairs.append((e.targetClass, nearestSprite.colorName))\n",
      " \n",
      " \t\tif not e.intPairs:\n",
      "-\t\t\tprint \"Couldn't make intPairs in unexpected appearance case.\"\n",
      "+\t\t\tprint(\"Couldn't make intPairs in unexpected appearance case.\")\n",
      " \t\terrorMap.append(e)\n",
      " \n",
      " \t# 3) Inventory change\n",
      " \tfor t in matched_sprites:\n",
      " \t\tinventory_penalty = 0\n",
      "-\t\tkeys = list(set(t[0].inventory.keys()+t[1].inventory.keys()))\n",
      "+\t\tkeys = list(set(list(t[0].inventory.keys())+list(t[1].inventory.keys())))\n",
      " \t\tfor k in keys:\n",
      "-\t\t\tt0_k = t[0].inventory[k] if k in t[0].inventory.keys() else (0,0)\n",
      "-\t\t\tt1_k = t[1].inventory[k] if k in t[1].inventory.keys() else (0,0)\n",
      "+\t\t\tt0_k = t[0].inventory[k] if k in list(t[0].inventory.keys()) else (0,0)\n",
      "+\t\t\tt1_k = t[1].inventory[k] if k in list(t[1].inventory.keys()) else (0,0)\n",
      " \t\t\tinventory_penalty += abs(t0_k[0]-t1_k[0])\n",
      " \n",
      " \t\tif inventory_penalty > 0:\n",
      "@@ -1346,10 +1346,10 @@\n",
      " \t\tsPrev, dist_ts = find_sPrev(sB, envB, envPrev)\n",
      " \t\tif not sPrev:\n",
      " \t\t\tcontinue\n",
      "-\t\tkeys = list(set(sB.inventory.keys()+sPrev.inventory.keys()))\n",
      "+\t\tkeys = list(set(list(sB.inventory.keys())+list(sPrev.inventory.keys())))\n",
      " \t\tfor k in keys:\n",
      "-\t\t\tsB_k = sB.inventory[k] if k in sB.inventory.keys() else (0,0)\n",
      "-\t\t\tsPrev_k = sPrev.inventory[k] if k in sPrev.inventory.keys() else (0,0)\n",
      "+\t\t\tsB_k = sB.inventory[k] if k in list(sB.inventory.keys()) else (0,0)\n",
      "+\t\t\tsPrev_k = sPrev.inventory[k] if k in list(sPrev.inventory.keys()) else (0,0)\n",
      " \t\t\tinventory_penalty += abs(sB_k[0]-sPrev_k[0])\n",
      " \n",
      " \t\tif inventory_penalty > 0:\n",
      "@@ -1394,7 +1394,7 @@\n",
      " \t\t\t\t\t\te.intPairs = int_pairs\n",
      " \t\t\t\t\t\te.targetTokens = targetTokens\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"problem in errorMap consolidation in errorSignal\"\n",
      "+\t\t\t\t\tprint(\"problem in errorMap consolidation in errorSignal\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\tlst = [errorMap[0]]\n",
      " \t\tfor e in errorMap[1:]:\n",
      "@@ -1428,7 +1428,7 @@\n",
      " \tFunction to find neighbors of sprite in the given environment (should be where the sprite came from)\n",
      " \t\"\"\"\n",
      " \t# Find potential interaction partners: neighboring sprites in previous step\n",
      "-\tall_sprites = [item for sublist in env._game.observation['trackedObjects'].values() for item in sublist]\n",
      "+\tall_sprites = [item for sublist in list(env._game.observation['trackedObjects'].values()) for item in sublist]\n",
      " \n",
      " \t# Neighbors of problematic sprite in real world in previous time step\n",
      " \tneighbors = [s for s in all_sprites if manhattanDist2(s, sprite)<=np.sqrt(distanceThreshold) and s!=sprite]\n",
      "@@ -1471,7 +1471,7 @@\n",
      " \tfor className in neighbors_prev:\n",
      " \t\te.intPairs.append( (sA.colorName,className) )\n",
      " \t# Determine mininum distance to neighbors in current real env -> to distinguish unexpectedPosition and unexpectedOverlap\n",
      "-\tall_sprites_envB = [item for sublist in envB._game.observation['trackedObjects'].values() for item in sublist]\n",
      "+\tall_sprites_envB = [item for sublist in list(envB._game.observation['trackedObjects'].values()) for item in sublist]\n",
      " \tnearest_sprite = findNearestSprite(sB, [s for s in all_sprites_envB if (s!=sB)])[0]\n",
      " \n",
      " \tnearest_dist = manhattanDist2(sB, nearest_sprite)\n",
      "@@ -1503,12 +1503,12 @@\n",
      " \t\t# find sprite in envA that corresponds to covered sprite in envB\n",
      " \t\tcolor = nearest_sprite.colorName\n",
      " \t\tclassName_envA = ''\n",
      "-\t\tfor k in [key for key in envA._game.observation['trackedObjects'].keys() if envA._game.observation['trackedObjects'][key]]:\n",
      "+\t\tfor k in [key for key in list(envA._game.observation['trackedObjects'].keys()) if envA._game.observation['trackedObjects'][key]]:\n",
      " \t\t\tif color == envA._game.observation['trackedObjects'][k][0].colorName:\n",
      " \t\t\t\tclassName_envA = k\n",
      "-\t\tcovered_sprite_envB = findNearestSprite(sB, [item for sublist in envB._game.observation['trackedObjects'].values() for item in sublist if sB!=item])[0]\n",
      "+\t\tcovered_sprite_envB = findNearestSprite(sB, [item for sublist in list(envB._game.observation['trackedObjects'].values()) for item in sublist if sB!=item])[0]\n",
      " \t\tif covered_sprite_envB.colorName not in theory.spriteObjects:\n",
      "-\t\t\tprint \"diagnosePosMismatch found a new color\"\n",
      "+\t\t\tprint(\"diagnosePosMismatch found a new color\")\n",
      " \t\t\te1 = errorMapEntry()\n",
      " \t\t\te1.diagnosis.append('newClass')\n",
      " \t\t\te1.targetToken = covered_sprite_envB\n",
      "@@ -1535,8 +1535,8 @@\n",
      " \treturn errorMaps\n",
      " \n",
      " def matchEnvs(envA, envB):\n",
      "-\tall_sprites_envA = [item for sublist in envA._game.observation['trackedObjects'].values() for item in sublist]\n",
      "-\tall_sprites_envB = [item for sublist in envB._game.observation['trackedObjects'].values() for item in sublist]\n",
      "+\tall_sprites_envA = [item for sublist in list(envA._game.observation['trackedObjects'].values()) for item in sublist]\n",
      "+\tall_sprites_envB = [item for sublist in list(envB._game.observation['trackedObjects'].values()) for item in sublist]\n",
      " \n",
      " \tID_dict = {}\n",
      " \n",
      "@@ -1549,7 +1549,7 @@\n",
      " \t\t\tID_dict[sprite.ID].append(sprite)\n",
      " \t\telse:\n",
      " \t\t\tlonely_sprites_envB.append(sprite)\n",
      "-\tfor v in ID_dict.values():\n",
      "+\tfor v in list(ID_dict.values()):\n",
      " \t\tif len(v)==2:\n",
      " \t\t\tmatched_sprites.append((v[0], v[1], manhattanDist2(v[0], v[1])))\n",
      " \t\telif len(v)==1:\n",
      "@@ -1709,7 +1709,7 @@\n",
      " \n",
      " \tlonely_sprites_envA = list(unmatchedA)\n",
      " \tlonely_sprites_envB = list(unmatchedB)\n",
      "-\tmatched_sprites = [(s1, s2, manhattanDist2(s1, s2)) for matched_color in matched_colors.values() for s1, s2 in matched_color.iteritems() ]\n",
      "+\tmatched_sprites = [(s1, s2, manhattanDist2(s1, s2)) for matched_color in list(matched_colors.values()) for s1, s2 in matched_color.items() ]\n",
      " \n",
      " \treturn matched_sprites, lonely_sprites_envA, lonely_sprites_envB\n",
      " \n",
      "@@ -1768,7 +1768,7 @@\n",
      " \tcumulative_penalties = []\n",
      " \n",
      " \tif len(hypotheses)>1:\n",
      "-\t\tprint \"got more than 1 hypothesis in singleTheoryExperienceReplay\"\n",
      "+\t\tprint(\"got more than 1 hypothesis in singleTheoryExperienceReplay\")\n",
      " \t\tembed()\n",
      " \t\n",
      " \n",
      "@@ -1781,17 +1781,17 @@\n",
      " \ttry:\n",
      " \t\tkey = (method, targetColor, rleHistory[0].ID, len(rleHistory))\n",
      " \texcept:\n",
      "-\t\tprint \"key for singleTheoryExperienceReplay failed\"\n",
      "+\t\tprint(\"key for singleTheoryExperienceReplay failed\")\n",
      " \t\tembed()\n",
      " \tif not displayStates and key in hypotheses[0].experienceReplayRecord:\n",
      " \t\treturn hypotheses[0].experienceReplayRecord[key], hypotheses[0].setOfImaginedEffects\n",
      " \n",
      " \tif method == 'all':\n",
      " \t\tif displayStates:\n",
      "-\t\t\tprint \"Playing replay FORWARD. Default is backwards.\"\n",
      "-\t\t\tindices = range(len(rleHistory))\n",
      "+\t\t\tprint(\"Playing replay FORWARD. Default is backwards.\")\n",
      "+\t\t\tindices = list(range(len(rleHistory)))\n",
      " \t\telse:\n",
      "-\t\t\tindices = list(reversed(range(len(rleHistory)-1)))\n",
      "+\t\t\tindices = list(reversed(list(range(len(rleHistory)-1))))\n",
      " \t\t\t# indices = indices[1:min(5, len(indices))]\n",
      " \t\t\tkeyForPreviousSequence = (method, targetColor, rleHistory[0].ID, len(rleHistory)-1)\n",
      " \t\t\tif keyForPreviousSequence in hypotheses[0].experienceReplayRecord:\n",
      "@@ -1809,7 +1809,7 @@\n",
      " \t\tif len(rleHistory)<2:\n",
      " \t\t\tactionsPerIndex = 0\n",
      " \t\t\tindices = [0]\n",
      "-\t\t\tprint \"got screenLastStep on short sequence\"\n",
      "+\t\t\tprint(\"got screenLastStep on short sequence\")\n",
      " \t\telse:\n",
      " \t\t\tindices = [-2]\n",
      " \t\t\tactionsPerIndex = 1\n",
      "@@ -1842,17 +1842,17 @@\n",
      " \t\tend = min(idx+actionsPerIndex, len(actionHistory))\n",
      " \n",
      " \t\tif displayStates:\n",
      "-\t\t\tprint \"setting state to index {}. Grounding state looks like this:\".format(idx)\n",
      "-\t\t\tprint rleHistory[idx].show()\n",
      "-\t\t\tprint \"hypothetical states are in blue below; should match the black state above.\"\n",
      "+\t\t\tprint(\"setting state to index {}. Grounding state looks like this:\".format(idx))\n",
      "+\t\t\tprint(rleHistory[idx].show())\n",
      "+\t\t\tprint(\"hypothetical states are in blue below; should match the black state above.\")\n",
      " \t\t\tfor env in theoryRLEs:\n",
      "-\t\t\t\tprint env.show(color='blue')\n",
      "+\t\t\t\tprint(env.show(color='blue'))\n",
      " \n",
      " \t\tfor n, action in enumerate(actionHistory[idx:end]):\n",
      " \t\t\tpenalties = []\n",
      " \t\t\tif displayStates:\n",
      "-\t\t\t\tprint \"after taking action {}, real state looked like this:\".format(keyPresses[action])\n",
      "-\t\t\t\tprint rleHistory[idx+n+1].show()\n",
      "+\t\t\t\tprint(\"after taking action {}, real state looked like this:\".format(keyPresses[action]))\n",
      "+\t\t\t\tprint(rleHistory[idx+n+1].show())\n",
      " \t\t\tfor num, env in enumerate(theoryRLEs):                      \n",
      " \n",
      " \t\t\t\tif env is not None:\n",
      "@@ -1862,12 +1862,12 @@\n",
      " \t\t\t\t\t## 1 or lim of each resource\n",
      " \t\t\t\t\tavatarColor = hypotheses[num].classes['avatar'][0].colorName\n",
      " \t\t\t\t\tresourceDict = {}\n",
      "-\t\t\t\t\tfor k in env._game.sprite_groups['avatar'][0].resources.keys():\n",
      "+\t\t\t\t\tfor k in list(env._game.sprite_groups['avatar'][0].resources.keys()):\n",
      " \t\t\t\t\t\tresourceColor = hypotheses[num].classes[k][0].colorName\t\n",
      " \t\t\t\t\t\ttry:\t\t\t\n",
      " \t\t\t\t\t\t\tresourceDict[k] = env._game.observation['trackedObjects'][avatarColor][0].inventory[resourceColor]\n",
      " \t\t\t\t\t\texcept:\n",
      "-\t\t\t\t\t\t\tprint 'resourceDict problem'\n",
      "+\t\t\t\t\t\t\tprint('resourceDict problem')\n",
      " \t\t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\t\timaginedEffects = env.step(action)['effectList']\n",
      "@@ -1877,7 +1877,7 @@\n",
      " \t\t\t\t\t\tsetOfImaginedEffects.add((effect[0], eff1Class, eff2Class))\n",
      " \t\t\t\t\t\tsetOfImaginedEffects.add((effect[0], eff2Class, eff1Class))\n",
      " \n",
      "-\t\t\t\t\t\tfor k,v in resourceDict.items():\n",
      "+\t\t\t\t\t\tfor k,v in list(resourceDict.items()):\n",
      " \t\t\t\t\t\t\tif v[0]>0:\n",
      " \t\t\t\t\t\t\t\tsetOfImaginedEffects.add((effect[0], eff1Class, eff2Class, k, True, False))\n",
      " \t\t\t\t\t\t\t\tsetOfImaginedEffects.add((effect[0], eff2Class, eff1Class, k, True, False))\n",
      "@@ -1889,16 +1889,16 @@\n",
      " \t\t\t\t\t\trleHistory[idx+n], targetColor=targetColor, penalty_only=True, earlyStopping=assumeZeroErrorTheoryExists)\n",
      " \t\t\t\t\tpenalties.append(penalty)\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"exception in experienceReplay\"\n",
      "+\t\t\t\t\tprint(\"exception in experienceReplay\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\tif displayStates:\n",
      "-\t\t\t\t\tprint \"resulting state incurred a penalty of {} and looks like this:\".format(penalty)\n",
      "-\t\t\t\t\tprint env.show(color='green')\n",
      "+\t\t\t\t\tprint(\"resulting state incurred a penalty of {} and looks like this:\".format(penalty))\n",
      "+\t\t\t\t\tprint(env.show(color='green'))\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\tcumulative_penalties.append(penalties)\n",
      " \t\n",
      " \tif not cumulative_penalties:\n",
      "-\t\tprint \"Warning: did not run experience replay.\"\n",
      "+\t\tprint(\"Warning: did not run experience replay.\")\n",
      " \t\tcumulative_penalties = [[0]*len(hypotheses)]\n",
      " \n",
      " \tcumulative_penalties = np.array(cumulative_penalties)\n",
      "@@ -1915,11 +1915,11 @@\n",
      " \t# if len(hypotheses)>100:\n",
      " \t\t# print \">100 hypotheses\"\n",
      " \t\t# embed()\n",
      "-\titr = trange(len(hypotheses)) if len(hypotheses) > 20 else range(len(hypotheses))\n",
      "+\titr = trange(len(hypotheses)) if len(hypotheses) > 20 else list(range(len(hypotheses)))\n",
      " \tfor num in itr:\n",
      " \t\th = hypotheses[num]\n",
      " \t\tif displayTheories:\n",
      "-\t\t\tprint \"running experienceReplay on {}:\".format(num)\n",
      "+\t\t\tprint(\"running experienceReplay on {}:\".format(num))\n",
      " \t\t\th.display()\n",
      " \t\tmean_penalties, setOfImaginedEffects = \\\n",
      " \t\t\t\tsingleTheoryExperienceReplay(rleHistory, actionHistory, method, targetColor, displayStates, [h],  assumeZeroErrorTheoryExists=assumeZeroErrorTheoryExists, errorCutoff=errorCutoff)\n",
      "@@ -1949,7 +1949,7 @@\n",
      " \n",
      " \tif sum([len(r) for r in rleHistories]) > 10 or len(hypotheses)>10:\n",
      " \t\tt1 = time.time()\n",
      "-\t\tprint \"Running MultiEpisodeExperienceReplay on {} hypotheses, {} episodes and {} time-steps total\".format(len(hypotheses), len(rleHistories), sum([len(r) for r in rleHistories]))\n",
      "+\t\tprint(\"Running MultiEpisodeExperienceReplay on {} hypotheses, {} episodes and {} time-steps total\".format(len(hypotheses), len(rleHistories), sum([len(r) for r in rleHistories])))\n",
      " \n",
      " \tmulti_episode_mean_penalties = []\n",
      " \timaginedEffectsPerTheory = [set() for i in range(len(hypotheses))]\n",
      "@@ -1972,7 +1972,7 @@\n",
      " \n",
      " \tmulti_episode_mean_penalties = np.mean(multi_episode_mean_penalties, axis=0)\n",
      " \tif sum([len(r) for r in rleHistories]) > 10 or len(hypotheses)>10:\n",
      "-\t\tprint \"MultiEpisodeExperienceReplay on {} theories, {} episodes and {} time-steps took {} seconds\".format(len(hypotheses), len(rleHistories), sum([len(r) for r in rleHistories]), time.time()-t1)\n",
      "+\t\tprint(\"MultiEpisodeExperienceReplay on {} theories, {} episodes and {} time-steps took {} seconds\".format(len(hypotheses), len(rleHistories), sum([len(r) for r in rleHistories]), time.time()-t1))\n",
      " \n",
      " \treturn multi_episode_mean_penalties, imaginedEffectsPerTheory\n",
      " \n",
      "@@ -2079,7 +2079,7 @@\n",
      " \t\t# print \"In base case. Correcting error for {} for {} theories\".format(errorMap.targetClass, len(theories))\n",
      " \t\tt1 = time.time()\n",
      " \t\tnewTheories = []\n",
      "-\t\tprint \"expanding theories for one errorMap\"\n",
      "+\t\tprint(\"expanding theories for one errorMap\")\n",
      " \t\tfor theory in theories:\n",
      " \t\t\tnewTheories.extend(expandTheoryForOneErrorMap(errorMap, envRealPrev, envRealCurrent, prevAction, rleHistories, actionHistories, \n",
      " \t\t\t\t\ttheory, classPairPlusPredicateToRuleSets))\n",
      "@@ -2092,14 +2092,14 @@\n",
      " \t\tif sum([len(episode) for episode in rleHistories]) == OBSERVATION_PERIOD_LENGTH:\n",
      " \t\t\tpenalties, _ = MultiEpisodeExperienceReplay(newTheories, rleHistories, \\\n",
      " \t\t\t\t\tactionHistories, method=EXPERIENCE_REPLAY_METHOD, targetColor=errorMap.targetColor, errorCutoff=.5)\n",
      "-\t\t\tscoreAndTheoryTuples = zip(penalties, newTheories)\n",
      "-\t\t\tprint \"{} theories before filtering\".format(len(scoreAndTheoryTuples))\n",
      "+\t\t\tscoreAndTheoryTuples = list(zip(penalties, newTheories))\n",
      "+\t\t\tprint(\"{} theories before filtering\".format(len(scoreAndTheoryTuples)))\n",
      " \n",
      " \t\t\tscoreAndTheoryTupleCandidates = [tup for tup in scoreAndTheoryTuples if tup[0] < perColorErrorBaselines[errorMap.targetColor]]\n",
      " \t\t\tif scoreAndTheoryTupleCandidates:\n",
      " \t\t\t\tscoreAndTheoryTuples = scoreAndTheoryTupleCandidates\n",
      " \n",
      "-\t\t\tprint \"{} theories after first filter\".format(len(scoreAndTheoryTuples))\n",
      "+\t\t\tprint(\"{} theories after first filter\".format(len(scoreAndTheoryTuples)))\n",
      " \t\t\tscoreAndTheoryTuples = sorted(scoreAndTheoryTuples, key=lambda x: (x[0], x[1].prior()))\n",
      " \n",
      " \t\t\t# hyperparameters here:\n",
      "@@ -2107,23 +2107,23 @@\n",
      " \t\t\tmedianDivisor = 3\n",
      " \n",
      " \t\t\tscoreAndTheoryTuples = filterByPrior(scoreAndTheoryTuples, numPerLevel=theoriesPerErrorLevel)\n",
      "-\t\t\tprint \"{} theories after filtering by prior\".format(len(scoreAndTheoryTuples))\n",
      "+\t\t\tprint(\"{} theories after filtering by prior\".format(len(scoreAndTheoryTuples)))\n",
      " \n",
      " \t\t\t# update error threshold for this color\n",
      " \t\t\tmed = scoreAndTheoryTuples[len(scoreAndTheoryTuples)//medianDivisor][0] + .000001 # to allow all infinitestimals\n",
      "-\t\t\tprint 'new baseline:' , med\n",
      "+\t\t\tprint('new baseline:' , med)\n",
      " \t\t\tperColorErrorBaselines[errorMap.targetColor] = med\n",
      " \t\t\tscoreAndTheoryTuples = [tup for tup in scoreAndTheoryTuples if tup[0] < perColorErrorBaselines[errorMap.targetColor]]\n",
      "-\t\t\tprint \"{} theories after filtering with new baseline\".format(len(scoreAndTheoryTuples))\n",
      "-\n",
      "-\t\t\tprint 'scores:' , [t[0] for t in scoreAndTheoryTuples]\n",
      "+\t\t\tprint(\"{} theories after filtering with new baseline\".format(len(scoreAndTheoryTuples)))\n",
      "+\n",
      "+\t\t\tprint('scores:' , [t[0] for t in scoreAndTheoryTuples])\n",
      " \t\t\t# embed()\n",
      " \n",
      " \t\telse:\n",
      " \t\t\trleHistory, actionHistory = rleHistories[episode_num], actionHistories[episode_num]\n",
      " \t\t\tpenalties, _ = MultiEpisodeExperienceReplay(newTheories, [rleHistory[-2:]], \\\n",
      " \t\t\t\t\t[actionHistory[-1:]], method=EXPERIENCE_REPLAY_METHOD, targetColor = errorMap.targetColor)\n",
      "-\t\t\tscoreAndTheoryTuples = zip(penalties, newTheories)\n",
      "+\t\t\tscoreAndTheoryTuples = list(zip(penalties, newTheories))\n",
      " \t\t\tscoreAndTheoryTuples = sorted(scoreAndTheoryTuples, key=lambda x: (x[0], x[1].prior()))\n",
      " \t\t\n",
      " \t\tnewTheories = [s[1] for s in scoreAndTheoryTuples]\n",
      "@@ -2147,22 +2147,22 @@\n",
      " \tn = 1 # n is the number of allowed rules for a particular classpair-ordering, probably (TODO)\n",
      " \n",
      " \terrorMap = errorMap.copy()\n",
      "-\terrorMap.targetClass = theory.spriteObjects[errorMap.targetClass].className if errorMap.targetClass in theory.spriteObjects.keys() else 'unknown'\n",
      "+\terrorMap.targetClass = theory.spriteObjects[errorMap.targetClass].className if errorMap.targetClass in list(theory.spriteObjects.keys()) else 'unknown'\n",
      " \n",
      " \tif errorMap.intPairs:\n",
      " \t\tnewIntPairs = []\n",
      " \t\tfor pair in errorMap.intPairs:\n",
      " \t\t\t\n",
      "-\t\t\tif pair[0] in theory.spriteObjects.keys():\n",
      "+\t\t\tif pair[0] in list(theory.spriteObjects.keys()):\n",
      " \t\t\t\tp0 = theory.spriteObjects[pair[0]].className\n",
      "-\t\t\telif pair[0] in theory.classes.keys():\n",
      "+\t\t\telif pair[0] in list(theory.classes.keys()):\n",
      " \t\t\t\tp0 = pair[0]\n",
      " \t\t\telse:\n",
      " \t\t\t\tp0 = 'unknown'\n",
      " \n",
      "-\t\t\tif pair[1] in theory.spriteObjects.keys():\n",
      "+\t\t\tif pair[1] in list(theory.spriteObjects.keys()):\n",
      " \t\t\t\tp1 = theory.spriteObjects[pair[1]].className\n",
      "-\t\t\telif pair[1] in theory.classes.keys():\n",
      "+\t\t\telif pair[1] in list(theory.classes.keys()):\n",
      " \t\t\t\tp1 = pair[1]\n",
      " \t\t\telse:\n",
      " \t\t\t\tp1 = 'unknown'\n",
      "@@ -2173,7 +2173,7 @@\n",
      " \t## If we were about to make modifications we've made already, don't waste the time.\n",
      " \tif any([errorMap == e for e in theory.errorMapHistory]):\n",
      " \t\terrorMap.display()\n",
      "-\t\tprint \"we've addressed this error before. Skipping it\"\n",
      "+\t\tprint(\"we've addressed this error before. Skipping it\")\n",
      " \t\tnewTheories = [theory]\n",
      " \t\treturn newTheories\n",
      " \n",
      "@@ -2191,7 +2191,7 @@\n",
      " \t\t# print \"got inventoryChange\"\n",
      " \t\t# embed()\n",
      " \t\tfor k in errorMap.targetToken.inventory:\n",
      "-\t\t\tif k not in theory.spriteObjects.keys():\n",
      "+\t\t\tif k not in list(theory.spriteObjects.keys()):\n",
      " \t\t\t\tcolor = k\n",
      " \t\t\t\texisting_classes = [key for key in theory.classes if key[0] == 'c']\n",
      " \t\t\t\tmax_num = max([int(c[1:]) for c in existing_classes])\n",
      "@@ -2208,10 +2208,10 @@\n",
      " \t# print \"about to check for new sprites\"\n",
      " \t# embed()\n",
      " \t## If there are new objects on screen, add them to the thery or reason about related objects (e.g., spawnPoints)\n",
      "-\tif errorMap.targetClass not in theory.classes.keys() or errorMap.targetToken in envRealCurrent._game.observation['new_sprites']:\n",
      "+\tif errorMap.targetClass not in list(theory.classes.keys()) or errorMap.targetToken in envRealCurrent._game.observation['new_sprites']:\n",
      " \n",
      " \t\t## If there are unknown colors on screen, add them to the theory here.\t\t\n",
      "-\t\tif errorMap.targetClass not in theory.classes.keys():\n",
      "+\t\tif errorMap.targetClass not in list(theory.classes.keys()):\n",
      " \t\t\tif errorMap.targetColor in theory.spriteObjects:\n",
      " \t\t\t\terrorMap.targetClass = theory.spriteObjects[errorMap.targetColor].className\n",
      " \t\t\telse:\n",
      "@@ -2222,7 +2222,7 @@\n",
      " \t\t\t\tnewClassName = 'c'+str(class_num)\n",
      " \t\t\t\terrorMap.targetClass = newClassName\n",
      " \t\t\t\ttheory.addSpriteToTheory(newClassName, errorMap.targetColor, vgdlType=Resource)\n",
      "-\t\t\t\tprint \"Got unknown targetclass for {}. Added generic sprite to spriteSet and interactionSet\".format(errorMap.targetToken.colorName)\n",
      "+\t\t\t\tprint(\"Got unknown targetclass for {}. Added generic sprite to spriteSet and interactionSet\".format(errorMap.targetToken.colorName))\n",
      " \n",
      " \t\t\tif 'newClass' in errorMap.diagnosis:\n",
      " \t\t\t\tnewTheories = [theory]\n",
      "@@ -2253,7 +2253,7 @@\n",
      " \n",
      " \t\tif 'newObjectAppeared' in eM.diagnosis:\n",
      " \t\t\tif eM.targetClass in theoryCopy.expandedSprites:\n",
      "-\t\t\t\tprint \"removing {} from theory.expandedSprites\".format(eM.targetClass)\n",
      "+\t\t\t\tprint(\"removing {} from theory.expandedSprites\".format(eM.targetClass))\n",
      " \t\t\t\ttheoryCopy.expandedSprites.remove(eM.targetClass)\n",
      " \t\t\n",
      " \t\t## SpriteSet induction step\n",
      "@@ -2386,7 +2386,7 @@\n",
      " \n",
      " \tdef gen_color():\n",
      " \t\tfrom vgdl.colors import colorDict\n",
      "-\t\tcolor_list = colorDict.values()\n",
      "+\t\tcolor_list = list(colorDict.values())\n",
      " \t\tcolor_list = [c for c in color_list if c not in ['UUWSWF']]\n",
      " \t\tfor color in color_list:\n",
      " \t\t\tyield color\n",
      "@@ -2426,7 +2426,7 @@\n",
      " \tresults = []\n",
      " \tif multiTesting:\n",
      " \t\t# have to make a new agent each time since they're really different games all in one\n",
      "-\t\tfor num in xrange(len(level_game_pairs)):\n",
      "+\t\tfor num in range(len(level_game_pairs)):\n",
      " \t\t\tagent = Agent('full', gameName)\n",
      " \n",
      " \t\t\ttry:\n",
      "@@ -2449,8 +2449,8 @@\n",
      " \t\tresults = agent.testCurriculum(level_game_pairs, actionSequences)\n",
      " \n",
      " \tfor num,res in enumerate(results):\n",
      "-\t\tprint '\\n--------test {} produced the following:--------'.format(num+1)\n",
      "-\t\tprint res\n",
      "-\t\tprint ''\n",
      "+\t\tprint('\\n--------test {} produced the following:--------'.format(num+1))\n",
      "+\t\tprint(res)\n",
      "+\t\tprint('')\n",
      " \n",
      " \tembed()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored subjective.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: subjective.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- subjective.py\t(original)\n",
      "+++ subjective.py\t(refactored)\n",
      "@@ -10,9 +10,9 @@\n",
      " \n",
      " import pygame\n",
      " \n",
      "-from ontology import DARKGRAY, BASEDIRS, LIGHTGRAY, RED, LIGHTBLUE\n",
      "-from tools import squarePoints\n",
      "-from interfaces import GameEnvironment\n",
      "+from .ontology import DARKGRAY, BASEDIRS, LIGHTGRAY, RED, LIGHTBLUE\n",
      "+from .tools import squarePoints\n",
      "+from .interfaces import GameEnvironment\n",
      " \n",
      " backscale = 0.4\n",
      " midscale = 0.65\n",
      "@@ -147,9 +147,9 @@\n",
      "         \n",
      "     def reset(self):\n",
      "         self.screen.blit(self.background, (0,0))        \n",
      "-        for ps in wallLocations.values():\n",
      "+        for ps in list(wallLocations.values()):\n",
      "             self._drawPolygon(ps, DARKGRAY) \n",
      "-        for ps in floorLocations.values():\n",
      "+        for ps in list(floorLocations.values()):\n",
      "             self._drawPolygon(ps, LIGHTGRAY) \n",
      "         pygame.display.flip()\n",
      "         \n",
      "@@ -206,7 +206,7 @@\n",
      "     def _drawState(self):\n",
      "         self.screen.reset()\n",
      "         for iswall, fid, pos in self._nearTileIncrements():\n",
      "-            for oname, ps in self._obstypes.items():\n",
      "+            for oname, ps in list(self._obstypes.items()):\n",
      "                 b = (oname in blocky)\n",
      "                 col = self._obscols[oname]\n",
      "                 if pos in ps:\n",
      "@@ -223,7 +223,7 @@\n",
      "         \n",
      " \n",
      " def test1():\n",
      "-    from ontology import GREEN, ORANGE, WHITE\n",
      "+    from .ontology import GREEN, ORANGE, WHITE\n",
      "     s = SubjectiveSceen()\n",
      "     s._initScreen()\n",
      "     s._colorWall(3, RED)\n",
      "@@ -256,7 +256,7 @@\n",
      "     \n",
      " def test2():\n",
      "     from examples.gridphysics.mazes import polarmaze_game, maze_level_1\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     game_str, map_str = polarmaze_game, maze_level_1\n",
      "     g = VGDLParser().parseGame(game_str)\n",
      "     g.buildLevel(map_str)    \n",
      "@@ -270,10 +270,10 @@\n",
      " def test3():\n",
      "     from examples.gridphysics.mazes import polarmaze_game\n",
      "     from examples.gridphysics.mazes.simple import maze_level_1b\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-    from interfaces import GameTask\n",
      "-    from agents import InteractiveAgent, UserTiredException    \n",
      "+    from .interfaces import GameTask\n",
      "+    from .agents import InteractiveAgent, UserTiredException    \n",
      "     game_str, map_str = polarmaze_game, maze_level_1b\n",
      "     g = VGDLParser().parseGame(game_str)\n",
      "     g.buildLevel(map_str)    \n",
      "@@ -286,7 +286,7 @@\n",
      "         exper.doEpisodes(1)\n",
      "     except UserTiredException:\n",
      "         pass\n",
      "-    print senv._allEvents\n",
      "+    print(senv._allEvents)\n",
      "     \n",
      "     \n",
      " if  __name__ == \"__main__\":   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored agent_saved.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: agent_saved.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- agent_saved.py\t(original)\n",
      "+++ agent_saved.py\t(refactored)\n",
      "@@ -1,9 +1,9 @@\n",
      " from basic_mcts_domain import *\n",
      "-from util import *\n",
      "-from core import colorDict\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectSubgoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, ruleCluster, Theory, Game\n",
      "+from .util import *\n",
      "+from .core import colorDict\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectSubgoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, ruleCluster, Theory, Game\n",
      " \n",
      " '''\n",
      " ## helpful functions or access methods:\n",
      "@@ -25,7 +25,7 @@\n",
      " \t## Initialize rle the agent behaves in.\n",
      " \trleCreateFunc = createRLSimpleGame4\n",
      " \trle = rleCreateFunc(OBSERVATION_GLOBAL)\n",
      "-\trle._game.unknown_objects = rle._game.sprite_groups.keys()\n",
      "+\trle._game.unknown_objects = list(rle._game.sprite_groups.keys())\n",
      " \trle._game.unknown_objects.remove('avatar') \t\t## For now we're asumming agent knows self.\n",
      " \trle.agentStatePrev = {}\n",
      " \tall_objects = rle._game.getObjects()\n",
      "@@ -35,9 +35,9 @@\n",
      " \n",
      " \t## When you restart episodes, reset the rle.agentStatePrev. Maybe some other things, too.\n",
      " \t\n",
      "-\tprint \"\"\n",
      "-\tprint \"\"\n",
      "-\tprint np.reshape(rle._getSensors(), rle.outdim)\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"\")\n",
      "+\tprint(np.reshape(rle._getSensors(), rle.outdim))\n",
      " \tsample = sampleFromDistribution(rle._game.spriteDistribution, all_objects)\n",
      " \tg = Game(spriteInductionResult=sample)\n",
      " \t## Temporary hack -- change as soon as we can write theory files.\n",
      "@@ -56,9 +56,9 @@\n",
      " \n",
      " \t\t## Plan to achieve that goal\n",
      " \t\trle, hypotheses = getToSubgoal(rle, Vrle, subgoal, all_objects, finalEventList, sample)\n",
      "-\t\tprint \"in agent loop\"\n",
      "-\t\tfrom rlenvironmentnonstatic import *\n",
      "-\t\tfrom theory_template import writeTheoryToTxt\n",
      "+\t\tprint(\"in agent loop\")\n",
      "+\t\tfrom .rlenvironmentnonstatic import *\n",
      "+\t\tfrom .theory_template import writeTheoryToTxt\n",
      " \t\t# rle2= createRLtextTheory(obsType=OBSERVATION_GLOBAL)\n",
      " \t\t\n",
      " \n",
      "@@ -73,7 +73,7 @@\n",
      " \t\tembed()\n",
      " \t\t## Select hypothesis according to whichever method, make new VRLE\n",
      " \t\t## theory_to_world(hypotheses[0]) ## should write new .py file\n",
      "-\t\tprint \"\"\n",
      "+\t\tprint(\"\")\n",
      " \n",
      " \t# embed()\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Can't parse VGDL_ontology.dot: ParseError: bad input: type=1, value='digraph', context=(' ', (1, 7))\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: There was 1 error:\n",
      "RefactoringTool: Can't parse VGDL_ontology.dot: ParseError: bad input: type=1, value='digraph', context=(' ', (1, 7))\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to flexible_goals.py\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: flexible_goals.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored core.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: core.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- core.py\t(original)\n",
      "+++ core.py\t(refactored)\n",
      "@@ -5,10 +5,10 @@\n",
      " '''\n",
      " import pygame\n",
      " from random import choice\n",
      "-from tools import Node, indentTreeParser\n",
      "+from .tools import Node, indentTreeParser\n",
      " from collections import defaultdict\n",
      "-from tools import roundedPoints\n",
      "-from colors import *\n",
      "+from .tools import roundedPoints\n",
      "+from .colors import *\n",
      " import os, shutil\n",
      " import datetime\n",
      " import uuid\n",
      "@@ -24,7 +24,7 @@\n",
      " import time\n",
      " import os\n",
      " import uuid\n",
      "-from util import getObjectColor\n",
      "+from .util import getObjectColor\n",
      " \n",
      " # ---------------------------------------------------------------------\n",
      " #     Constants\n",
      "@@ -68,9 +68,9 @@\n",
      "     @staticmethod\n",
      "     def playSubjectiveGame(game_str, map_str):\n",
      "         from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-        from interfaces import GameTask\n",
      "-        from subjective import SubjectiveGame\n",
      "-        from agents import InteractiveAgent, UserTiredException\n",
      "+        from .interfaces import GameTask\n",
      "+        from .subjective import SubjectiveGame\n",
      "+        from .agents import InteractiveAgent, UserTiredException\n",
      "         g = VGDLParser().parseGame(game_str)\n",
      "         g.buildLevel(map_str)\n",
      "         senv = SubjectiveGame(g, actionDelay=100, recordingEnabled=True)\n",
      "@@ -105,7 +105,7 @@\n",
      "         \"\"\" Whatever is visible in the global namespace (after importing the ontologies)\n",
      "         can be used in the VGDL, and is evaluated.\n",
      "         \"\"\"\n",
      "-        from ontology import * # @UnusedWildImport\n",
      "+        from .ontology import * # @UnusedWildImport\n",
      "         return eval(estr)\n",
      " \n",
      "     def parseInteractions(self, inodes):\n",
      "@@ -116,7 +116,7 @@\n",
      "                 class1, class2 = [x.strip() for x in pair.split(\" \") if len(x)>0]\n",
      "                 self.game.collision_eff.append(tuple([class1, class2, eclass, args]))\n",
      "                 if self.verbose:\n",
      "-                    print \"Collision\", pair, \"has effect:\", edef\n",
      "+                    print(\"Collision\", pair, \"has effect:\", edef)\n",
      "         \n",
      "         if 'cloneSprite' in [e[2].__name__ for e in self.game.collision_eff]:\n",
      "             self.has_clonesprite = True\n",
      "@@ -153,7 +153,7 @@\n",
      "         for tn in tnodes:\n",
      "             sclass, args = self._parseArgs(tn.content)\n",
      "             if self.verbose:\n",
      "-                print \"Adding:\", sclass, args\n",
      "+                print(\"Adding:\", sclass, args)\n",
      "             self.game.terminations.append(sclass(**args))\n",
      " \n",
      "     def parseConditions(self, cnodes):\n",
      "@@ -182,7 +182,7 @@\n",
      "             # import pdb; pdb.set_trace()\n",
      "             if len(sn.children) == 0:\n",
      "                 if self.verbose:\n",
      "-                    print \"Defining:\", key, sclass, arguments, stypes\n",
      "+                    print(\"Defining:\", key, sclass, arguments, stypes)\n",
      "                 # import pdb; pdb.set_trace()\n",
      "                 self.game.sprite_constr[key] = (sclass, arguments, stypes)\n",
      "                 self.game.alt_sprite_constr[key] = (sclass, arguments, stypes)\n",
      "@@ -255,8 +255,8 @@\n",
      "     load_save_enabled = True\n",
      " \n",
      "     def __init__(self, **kwargs):\n",
      "-        from ontology import Immovable, DARKGRAY, BLACK, MovingAvatar, GOLD\n",
      "-        for name, value in kwargs.iteritems():\n",
      "+        from .ontology import Immovable, DARKGRAY, BLACK, MovingAvatar, GOLD\n",
      "+        for name, value in kwargs.items():\n",
      "             if hasattr(self, name):\n",
      "                 self.__dict__[name] = value\n",
      "             # else:\n",
      "@@ -335,9 +335,9 @@\n",
      "         self.all_killed=[] # All items that have been killed\n",
      " \n",
      "     def buildLevel(self, lstr):\n",
      "-        from ontology import stochastic_effects\n",
      "+        from .ontology import stochastic_effects\n",
      "         lines = [l for l in lstr.split(\"\\n\") if len(l)>0]\n",
      "-        lengths = map(len, lines)\n",
      "+        lengths = list(map(len, lines))\n",
      "         assert min(lengths)==max(lengths), \"Inconsistent line lengths.\"\n",
      "         self.width = lengths[0]\n",
      "         self.height = len(lines)\n",
      "@@ -350,7 +350,7 @@\n",
      "         self.screensize = (self.width*self.block_size, self.height*self.block_size)\n",
      " \n",
      "         # set up resources\n",
      "-        for res_type, (sclass, args, _) in self.sprite_constr.iteritems():\n",
      "+        for res_type, (sclass, args, _) in self.sprite_constr.items():\n",
      "             if issubclass(sclass, Resource):\n",
      "                 if 'res_type' in args:\n",
      "                     res_type = args['res_type']\n",
      "@@ -384,7 +384,7 @@\n",
      "         self.sprite_order.append('avatar')\n",
      " \n",
      "     def buildLevelFromPos(self, positions):\n",
      "-        from ontology import stochastic_effects\n",
      "+        from .ontology import stochastic_effects\n",
      "         dims = positions[0]\n",
      "         self.width = dims[0]\n",
      "         self.height = dims[1]\n",
      "@@ -394,7 +394,7 @@\n",
      "         self.block_size = max(2,int(800./max(self.width, self.height)))\n",
      "         self.screensize = (self.width*self.block_size, self.height*self.block_size)\n",
      " \n",
      "-        for res_type, (sclass, args, _) in self.sprite_constr.iteritems():\n",
      "+        for res_type, (sclass, args, _) in self.sprite_constr.items():\n",
      "             if issubclass(sclass, Resource):\n",
      "                 if 'res_type' in args:\n",
      "                     res_type = args['res_type']\n",
      "@@ -407,7 +407,7 @@\n",
      " \n",
      "         for key in pos:\n",
      "             for loc in pos[key]:\n",
      "-                print loc\n",
      "+                print(loc)\n",
      "                 self._createSprite([key],(loc[0]*self.block_size,loc[1]*self.block_size))\n",
      " \n",
      "         self.kill_list=[]\n",
      "@@ -445,7 +445,7 @@\n",
      " \n",
      "         for key in keys:\n",
      "             if self.num_sprites > self.MAX_SPRITES:\n",
      "-                print \"Sprite limit reached.\"\n",
      "+                print(\"Sprite limit reached.\")\n",
      "                 return res\n",
      " \n",
      "             sclass, args, stypes = self.sprite_constr[key]\n",
      "@@ -494,7 +494,7 @@\n",
      "             self.screen = pygame.display.set_mode((1,1))\n",
      "             self.background = pygame.Surface(size)\n",
      "         else:\n",
      "-            from ontology import LIGHTGRAY\n",
      "+            from .ontology import LIGHTGRAY\n",
      "             pygame.init()\n",
      "             self.screen = pygame.display.set_mode(size)\n",
      "             self.background = pygame.Surface(size)\n",
      "@@ -531,7 +531,7 @@\n",
      "     def getAvatars(self):\n",
      "         \"\"\" The currently alive avatar(s) \"\"\"\n",
      "         res = []\n",
      "-        for ss in self.sprite_groups.values():\n",
      "+        for ss in list(self.sprite_groups.values()):\n",
      "             if ss and isinstance(ss[0], Avatar):\n",
      "                 res.extend([s for s in ss if s not in self.kill_list])\n",
      "         return res\n",
      "@@ -606,7 +606,7 @@\n",
      "                 else:\n",
      "                     ss[pos] = attrs\n",
      " \n",
      "-                for a, val in s.__dict__.iteritems():\n",
      "+                for a, val in s.__dict__.items():\n",
      "                     if a not in ias:\n",
      "                         attrs[a] = val\n",
      "                 if s.resources:\n",
      "@@ -623,17 +623,17 @@\n",
      "         self.reset()\n",
      "         self.score = fs['score']\n",
      "         self.ended = fs['ended']\n",
      "-        for key, ss in fs['objects'].iteritems():\n",
      "+        for key, ss in fs['objects'].items():\n",
      "             self.sprite_groups[key] = [] ## Added 4/31/17\n",
      "-            for ID, attrs in ss.iteritems():\n",
      "+            for ID, attrs in ss.items():\n",
      "                 try:\n",
      "                     p = attrs['x'], attrs['y']\n",
      "                 except:\n",
      "                     p = attrs[x], attrs[y]\n",
      "                 s = self._createSprite_cheap(key, p)\n",
      "-                for a, val in attrs.iteritems():\n",
      "+                for a, val in attrs.items():\n",
      "                     if a == 'resources':\n",
      "-                        for r, v in val.iteritems():\n",
      "+                        for r, v in val.items():\n",
      "                             s.resources[r] = v\n",
      "                     else:\n",
      "                         s.__setattr__(a, val)\n",
      "@@ -790,7 +790,7 @@\n",
      " \n",
      "                         if dim:\n",
      "                             sprites = self.getSprites(classprite1)\n",
      "-                            spritesFiltered = filter(lambda sprite: sprite.__dict__[dim] == sprite2.__dict__[dim], sprites)\n",
      "+                            spritesFiltered = [sprite for sprite in sprites if sprite.__dict__[dim] == sprite2.__dict__[dim]]\n",
      "                             for sC in spritesFiltered:\n",
      "                                 new_effects.append((effect, sprite1, sC, self, kwargs))\n",
      "                                 spritesActedOn.add(sprite1)\n",
      "@@ -860,7 +860,7 @@\n",
      "                 if len(element)==3:\n",
      "                     colorTuple = (element[0], color1, color2)\n",
      "                 elif len(element)>3:\n",
      "-                    for k in element[3].keys():\n",
      "+                    for k in list(element[3].keys()):\n",
      "                         if k=='stype':\n",
      "                             element[3][k] = getObjectColor(element[3][k], all_objects, self, colorDict)\n",
      "                     colorTuple = (element[0], color1, color2, element[3])\n",
      "@@ -873,8 +873,8 @@\n",
      "         ## for a particular classPair. Make sure we're reporting the other one, too.\n",
      "         unaccountedForOrderedPairs = []\n",
      "         effectsToAdd = []\n",
      "-        for k, v in classPairEffects.items():\n",
      "-            if (k[1], k[0]) not in classPairEffects.keys():\n",
      "+        for k, v in list(classPairEffects.items()):\n",
      "+            if (k[1], k[0]) not in list(classPairEffects.keys()):\n",
      "                 missingOrderedPair = (k[1], k[0])\n",
      "                 unaccountedForOrderedPairs.append(missingOrderedPair)\n",
      " \n",
      "@@ -919,10 +919,10 @@\n",
      "                         spriteClass = s.name\n",
      "                         spriteColor = s.colorName\n",
      "         except:\n",
      "-            print \"getSpriteClass problem\"\n",
      "+            print(\"getSpriteClass problem\")\n",
      "             embed()\n",
      "         if None in [spriteClass, spriteColor]:\n",
      "-            print \"failed to find sprite class or spriteColor\"\n",
      "+            print(\"failed to find sprite class or spriteColor\")\n",
      "             embed()\n",
      " \n",
      "         return spriteClass, spriteColor\n",
      "@@ -980,7 +980,7 @@\n",
      "         sprite_output = \"output/{}_{}_sprites.txt\".format(name,timestamp)\n",
      " \n",
      "         # --------- Game-play ------------\n",
      "-        from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+        from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      " \n",
      "         finalEventList = []\n",
      "         agentStatePrev = {}\n",
      "@@ -1012,7 +1012,7 @@\n",
      "                 self.setFullState(self.playback_states[self.playback_index])\n",
      "                 current_state = self.playback_states[self.playback_index]\n",
      "             except:\n",
      "-                print \"playback is failing\"\n",
      "+                print(\"playback is failing\")\n",
      "                 embed()\n",
      " \n",
      "             # Save the event and agent state\n",
      "@@ -1065,11 +1065,11 @@\n",
      "         if win:\n",
      "             # self.score += 1\n",
      "             self.win = True\n",
      "-            print \"Game won, with score %s\" % self.score\n",
      "+            print(\"Game won, with score %s\" % self.score)\n",
      "         else:\n",
      "             # self.score -= 1\n",
      "             self.win = False\n",
      "-            print \"Playback is incomplete, or game is lost. Score=%s\" % self.score\n",
      "+            print(\"Playback is incomplete, or game is lost. Score=%s\" % self.score)\n",
      "         \n",
      "         # if make_movie:\n",
      "             # self.makeMovie(parameter_string, gameName)\n",
      "@@ -1130,7 +1130,7 @@\n",
      "         sprite_output = \"output/{}_{}_sprites.txt\".format(name,timestamp)\n",
      " \n",
      "         # --------- Game-play ------------\n",
      "-        from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+        from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      " \n",
      "         finalEventList = []\n",
      "         agentStatePrev = {}\n",
      "@@ -1144,7 +1144,7 @@\n",
      "         sprite_types = [Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile]\n",
      "         self.all_objects = self.getAllObjects() #self.getObjects() # Save all objects, some which may be killed in game\n",
      "         ##figure out keypress type:\n",
      "-        disableContinuousKeyPress = all([item.physicstype.__name__=='GridPhysics' for sublist in self.sprite_groups.values() for item in sublist])\n",
      "+        disableContinuousKeyPress = all([item.physicstype.__name__=='GridPhysics' for sublist in list(self.sprite_groups.values()) for item in sublist])\n",
      " \n",
      "         self.spriteDistribution = {}\n",
      "         self.movement_options = {}\n",
      "@@ -1192,7 +1192,7 @@\n",
      "                         #     self.keystate = tuple(self.keystate)\n",
      "                         #     self.playback_index += 1\n",
      " \n",
      "-                        if lastKeyPress.index(1) in keyPresses.keys():\n",
      "+                        if lastKeyPress.index(1) in list(keyPresses.keys()):\n",
      "                             keyPressType = keyPresses[lastKeyPress.index(1)]\n",
      "                             # print keyPressType\n",
      " \n",
      "@@ -1251,20 +1251,20 @@\n",
      "                         #     self.score = 1\n",
      " \n",
      "                         self.win = True\n",
      "-                        print time.time()-t1, len(self.actions), win, self.score\n",
      "-                        print \"Termination\", t.__dict__\n",
      "-                        print \"Game won, with score %s\" % self.score\n",
      "+                        print(time.time()-t1, len(self.actions), win, self.score)\n",
      "+                        print(\"Termination\", t.__dict__)\n",
      "+                        print(\"Game won, with score %s\" % self.score)\n",
      "                     else:\n",
      "                         self.win = False\n",
      "                         # self.score -=1 ## Added 3/16/17\n",
      "-                        print time.time()-t1, len(self.actions), win, self.score\n",
      "-                        print \"Game lost. Score=%s\" % self.score\n",
      "+                        print(time.time()-t1, len(self.actions), win, self.score)\n",
      "+                        print(\"Game lost. Score=%s\" % self.score)\n",
      "                     # np.save(\"temp_data.npy\", [time.time()-t1, len(self.actions), self.win, self.score])\n",
      "                     allStates.append(self.getFullState())\n",
      " \n",
      "                     pygame.time.wait(10)\n",
      "-                    print len(self.actions), win, self.score\n",
      "-                    print \"ended in {} steps\".format(self.time)\n",
      "+                    print(len(self.actions), win, self.score)\n",
      "+                    print(\"ended in {} steps\".format(self.time))\n",
      "                     return win, self.score\n",
      "                     # pygame.quit()\n",
      "                     # sys.exit()\n",
      "@@ -1314,7 +1314,7 @@\n",
      "             # allStates.append(self.getFullState())\n",
      " \n",
      "         if(persist_movie):\n",
      "-            print \"Creating Movie\"\n",
      "+            print(\"Creating Movie\")\n",
      "             self.video_file = \"./videos/\" +  str(self.uiud) + \".mp4\"\n",
      "             subprocess.call([\"ffmpeg\",\"-y\",  \"-r\", \"30\", \"-b\", \"800\", \"-i\", tmpl, self.video_file ])\n",
      "             [os.remove(f) for f in glob.glob(tmp_dir + \"*\" + str(self.uiud) + \"*\")]\n",
      "@@ -1338,13 +1338,13 @@\n",
      "                 # self.score = 1\n",
      "             # self.score +=1 # Added 3/16/17\n",
      "             self.win = True\n",
      "-            print \"Game won, with score %s\" % self.score\n",
      "+            print(\"Game won, with score %s\" % self.score)\n",
      "             # np.save(\"temp_data.npy\", [time.time()-t1, len(self.actions), self.win, self.score])\n",
      " \n",
      "         else:\n",
      "             self.win = False\n",
      "             # self.score -=1 # Added 3/16/17\n",
      "-            print \"Game lost. Score=%s\" % self.score\n",
      "+            print(\"Game lost. Score=%s\" % self.score)\n",
      "             # np.save(\"temp_data.npy\", [time.time()-t1, len(self.actions), self.win, self.score])\n",
      " \n",
      " \n",
      "@@ -1358,7 +1358,7 @@\n",
      "         return self.getAvatars()[0].declare_possible_actions()\n",
      " \n",
      "     def startGameExternalPlayer(self, headless, persist_movie, movie_dir):\n",
      "-        print \"in startgameexternalplayer\"\n",
      "+        print(\"in startgameexternalplayer\")\n",
      "         embed()\n",
      "         self._initScreen(self.screensize, headless)\n",
      "         pygame.display.flip()\n",
      "@@ -1452,7 +1452,7 @@\n",
      "         return hash(self.ID)\n",
      " \n",
      "     def __init__(self, pos, size=(10,10), color=None, speed=None, cooldown=None, physicstype=None, **kwargs):\n",
      "-        from ontology import GridPhysics\n",
      "+        from .ontology import GridPhysics\n",
      "         self.rect = pygame.Rect(pos, size)\n",
      "         self.x = pos[0]\n",
      "         self.y = pos[1]\n",
      "@@ -1470,17 +1470,17 @@\n",
      "         self.color = color or self.color or PURPLE#(140, 20, 140)\n",
      "         if self.color == ENDOFSCREEN:\n",
      "             self.ID = 'ENDOFSCREEN'\n",
      "-        if str(self.color) in colorDict.keys():\n",
      "+        if str(self.color) in list(colorDict.keys()):\n",
      "             self.colorName = colorDict[str(self.color)]\n",
      "         else:\n",
      "             self.colorName = str(self.color)\n",
      " \n",
      "         #self.color = color or self.color or (choice(self.COLOR_DISC), choice(self.COLOR_DISC), choice(self.COLOR_DISC))\n",
      "-        for name, value in kwargs.iteritems():\n",
      "+        for name, value in kwargs.items():\n",
      "             try:\n",
      "                 self.__dict__[name] = value\n",
      "             except:\n",
      "-                print \"WARNING: undefined parameter '%s' for sprite '%s'! \"%(name, self.__class__.__name__)\n",
      "+                print(\"WARNING: undefined parameter '%s' for sprite '%s'! \"%(name, self.__class__.__name__))\n",
      "         # how many timesteps ago was the last move?\n",
      "         self.lastmove = 0\n",
      "         # how many timesteps ago was the last displacement? We'll use this to track more generic hypotheses,\n",
      "@@ -1523,7 +1523,7 @@\n",
      "         return (self.rect[0]-self.lastrect[0], self.rect[1]-self.lastrect[1])\n",
      " \n",
      "     def _draw(self, game):\n",
      "-        from ontology import LIGHTGREEN\n",
      "+        from .ontology import LIGHTGREEN\n",
      "         screen = game.screen\n",
      " \n",
      "         if self.shrinkfactor != 0:\n",
      "@@ -1554,7 +1554,7 @@\n",
      " \n",
      "     def _drawResources(self, game, screen, rect):\n",
      "         \"\"\" Draw progress bars on the bottom third of the sprite \"\"\"\n",
      "-        from ontology import BLACK\n",
      "+        from .ontology import BLACK\n",
      "         tot = len(self.resources)\n",
      "         barheight = rect.height/3.5/tot\n",
      "         offset = rect.top+2*rect.height/3.\n",
      "@@ -1628,7 +1628,7 @@\n",
      " \n",
      "     def get_args(self):\n",
      "         args = {}\n",
      "-        for key, value in self.__dict__.iteritems():\n",
      "+        for key, value in self.__dict__.items():\n",
      "             if key != 'name':\n",
      "                 args[key] = value\n",
      "         if 'win' not in args:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored mcts2.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: mcts2.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mcts2.py\t(original)\n",
      "+++ mcts2.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,14 +14,14 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from qlearner import *\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .qlearner import *\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -45,7 +45,7 @@\n",
      " \tdef __init__(self, existing_rle=False, game = None, level = None, partitionWeights=[1,0,1],\\\n",
      " \t\t         rleCreateFunc=False, obsType = OBSERVATION_GLOBAL, decay_factor=.8, num_workers=1):\n",
      " \t\tif not existing_rle and not rleCreateFunc:\n",
      "-\t\t\tprint \"You must pass either an existing rle or an rleCreateFunc\"\n",
      "+\t\t\tprint(\"You must pass either an existing rle or an rleCreateFunc\")\n",
      " \t\t\treturn\n",
      " \t\t# assumption: not starting on terminal state\n",
      " \t\t\"\"\"\n",
      "@@ -102,7 +102,7 @@\n",
      " \t\tgoal_loc = np.where(np.reshape(self.rle._getSensors(), self.outdim)==goal_code)\n",
      " \t\tgoal_loc = goal_loc[0][0], goal_loc[1][0] #(y,x)\n",
      " \n",
      "-\t\tif 'avatar' in self._obstypes.keys():\n",
      "+\t\tif 'avatar' in list(self._obstypes.keys()):\n",
      " \t\t\tinverted_avatar_loc=self._obstypes['avatar'][0]\n",
      " \t\t\tavatar_loc = (inverted_avatar_loc[1], inverted_avatar_loc[0])\n",
      " \t\t\tself.avatar_code = np.reshape(self.rle._getSensors(), self.outdim)[avatar_loc[0]][avatar_loc[1]]\n",
      "@@ -144,7 +144,7 @@\n",
      " \t\tkillerObjectCodes = []\n",
      " \t\tif hasattr(self.rle, 'killerObjects'):\n",
      " \t\t\tfor o in self.rle.killerObjects:\n",
      "-\t\t\t\tif o in self.rle._obstypes.keys():\n",
      "+\t\t\t\tif o in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\t\tkillerObjectCodes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(o)))\n",
      " \t\tboard = np.reshape(self.rle._getSensors(), self.rle.outdim)\n",
      " \t\tgoal_loc = np.where(board==goal_code)\n",
      "@@ -179,7 +179,7 @@\n",
      " \t\t\t\t\t\t\t\t# print \"found altered path\", path[subgoal_index+i]\n",
      " \t\t\t\t\t\t\t\tbreak\n",
      " \t\t\t\t\t\texcept:\n",
      "-\t\t\t\t\t\t\tprint \"indices didn't work out in looking for different path\"\n",
      "+\t\t\t\t\t\t\tprint(\"indices didn't work out in looking for different path\")\n",
      " \n",
      " \t\t\t\t# self.subgoals.append(path[subgoal_index])\n",
      " \t\treturn self.subgoals\n",
      "@@ -195,10 +195,10 @@\n",
      " \t\t\t# print \"immovables\", immovables\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall', 'poison']\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self._obstypes.keys():\n",
      "+\t\t\tif i in list(self._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -231,7 +231,7 @@\n",
      " \t\twhile len(self.rewardQueue)>0:\n",
      " \t\t\tloc = self.rewardQueue.popleft()\n",
      " \t\t\tif loc not in self.processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tself.processed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -275,8 +275,8 @@\n",
      " \t\t\tVrle = copy.deepcopy(VRLE)\n",
      " \n",
      " \t\t\tif i%100==0 and len(rewards)>0:\n",
      "-\t\t\t\tprint \"Training cycle: %i\"%i\n",
      "-\t\t\t\tprint \"avg. rewards for last group of 100\", np.mean(rewards[-100:])\n",
      "+\t\t\t\tprint(\"Training cycle: %i\"%i)\n",
      "+\t\t\t\tprint(\"avg. rewards for last group of 100\", np.mean(rewards[-100:]))\n",
      " \n",
      " \t\t\tif defaultPolicySolveStep:\n",
      " \t\t\t\treward, v, iters = self.treePolicy(self.root, Vrle, step_horizon, \\\n",
      "@@ -312,7 +312,7 @@\n",
      " \tdef getBestActionsForPlayout(self, partitionWeights, debug=False):\n",
      " \t\tv = self.root\n",
      " \t\tactions = []\n",
      "-\t\twhile v and not v.terminal and len(v.children.keys())>0:\n",
      "+\t\twhile v and not v.terminal and len(list(v.children.keys()))>0:\n",
      " \t\t\ta, v = self.bestChild(v,partitionWeights, debug=debug)\n",
      " \t\t\tactions.append(a)\n",
      " \t\treturn actions\n",
      "@@ -332,24 +332,24 @@\n",
      " \t\tcntr=0\n",
      " \t\tv = self.root\n",
      " \t\tif output:\n",
      "-\t\t\tprint \"current state\"\n",
      "-\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "+\t\t\tprint(\"current state\")\n",
      "+\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      " \t\tactions, nodes = [], []\n",
      " \t\twhile v and not v.terminal and cntr<numActions:\n",
      " \t\t\tif output:\n",
      "-\t\t\t\tprint \"options\"\n",
      "-\t\t\t\tprint [(ACTIONS[k],c.qVal) for k,c in v.children.iteritems()]\n",
      "+\t\t\t\tprint(\"options\")\n",
      "+\t\t\t\tprint([(ACTIONS[k],c.qVal) for k,c in v.children.items()])\n",
      " \t\t\ta, v = self.bestChild(v,(1,0,0))\n",
      " \n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tnodes.append(v)\n",
      " \t\t\tif output:\n",
      " \t\t\t\tif v:\n",
      "-\t\t\t\t\tprint \"selected\"\n",
      "-\t\t\t\t\tprint ACTIONS[a]\n",
      "-\t\t\t\t\tprint \"resulted in\"\n",
      "-\t\t\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "-\t\t\t\t\tprint \"\"\n",
      "+\t\t\t\t\tprint(\"selected\")\n",
      "+\t\t\t\t\tprint(ACTIONS[a])\n",
      "+\t\t\t\t\tprint(\"resulted in\")\n",
      "+\t\t\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      "+\t\t\t\t\tprint(\"\")\n",
      " \t\t\tcntr+=1\n",
      " \t\t# if v.terminal:\n",
      " \t\t# \tdistance = 0\n",
      "@@ -387,11 +387,11 @@\n",
      " \n",
      " \t\t\t\tres = rle.step(a) ## TODO: you're getting the bestChild and taking bestAction, but in a stochastic game you will end up in\n",
      " \t\t\t\t\t\t\t\t\t## a different state despite having taken the same action. Is this what you want?\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\t\tterminal = rle._isDone()[0]\n",
      " \n",
      " \t\t\t\tif terminal != v.terminal or not np.array_equal(v.state, rle._getSensors()):\n",
      "-\t\t\t\t\tprint \"inconsistency in node and rle\"\n",
      "+\t\t\t\t\tprint(\"inconsistency in node and rle\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\t# terminal = (not res['pcontinue']) or (rle._avatar is None)\n",
      " \t\t\t\tif terminal:\n",
      "@@ -417,7 +417,7 @@\n",
      " \t\t\taction_choices = self.actions\n",
      " \n",
      " \t\tfor a in action_choices:\n",
      "-\t\t\tif a not in v.children.keys():\n",
      "+\t\t\tif a not in list(v.children.keys()):\n",
      " \t\t\t\texpand_action = a\n",
      " \t\t\t\tres = rle.step(a)\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      "@@ -442,13 +442,13 @@\n",
      " \tdef maxChild(self, v):\n",
      " \t\ttmp = np.where(np.reshape(v.state, self.outdim)==1)\n",
      " \t\tavatar_loc = tmp[0][0], tmp[1][0]\n",
      "-\t\tqVals = [v.children[a].qVal for a in v.children.keys()]\n",
      "-\t\tif len(qVals)>0 and avatar_loc in self.neighborDict.keys() and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      "+\t\tqVals = [v.children[a].qVal for a in list(v.children.keys())]\n",
      "+\t\tif len(qVals)>0 and avatar_loc in list(self.neighborDict.keys()) and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      " \t\t\t\tmaxVal = max(qVals)\n",
      "-\t\t\t\tchoices = [(a,c) for (a,c) in v.children.items() if c.qVal==maxVal]\n",
      "-\t\t\t\tfor (a,c) in v.children.items():\n",
      "-\t\t\t\t\tprint a, c.qVal\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tchoices = [(a,c) for (a,c) in list(v.children.items()) if c.qVal==maxVal]\n",
      "+\t\t\t\tfor (a,c) in list(v.children.items()):\n",
      "+\t\t\t\t\tprint(a, c.qVal)\n",
      "+\t\t\t\tprint(\"\")\n",
      " \t\t\t\treturn random.choice(choices)\n",
      " \t\telse:\n",
      " \t\t\treturn (None, None)\n",
      "@@ -489,7 +489,7 @@\n",
      " \t\t\tself.printexplorationweight = exploration_coefficient\n",
      " \t\t\t# print \"heuristic coefficient is now\", heuristic_coefficient\n",
      " \n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tcontinue\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -498,7 +498,7 @@\n",
      " \t\t\t\tif self.avatar_code not in [l%2 for l in c.state]: ##we're in a terminal losing state\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t## Don't give pseudoreward\n",
      " \t\t\t\t\tif not c.terminal:\n",
      "-\t\t\t\t\t\tprint \"terminal state but avatar not in c.state\"\n",
      "+\t\t\t\t\t\tprint(\"terminal state but avatar not in c.state\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\t\t# vLoc = np.where(np.reshape(v.state, self.outdim)%2==self.avatar_code)\n",
      "@@ -528,15 +528,15 @@\n",
      " \t\t\t\t\t\tsumVisitCount += abs(math.sqrt(2*math.log(v.visitCount)/c.visitCount))\n",
      " \t\t\t\t\t\tsumPseudoReward += abs(transform(loc))/c.visitCount\n",
      " \t\t\t\t\texcept TypeError:\n",
      "-\t\t\t\t\t\tprint \"loc is weird type\"\n",
      "+\t\t\t\t\t\tprint(\"loc is weird type\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \n",
      " \n",
      " \t\t# print \"\"\n",
      " \t\tif debug:\n",
      "-\t\t\tprint np.reshape(v.state, self.outdim)\n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\t\tprint(np.reshape(v.state, self.outdim))\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tfuncVal = -float('inf')\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -591,18 +591,18 @@\n",
      " \t\t\t\t\t        + exploration_coefficient*math.sqrt(2*math.log(v.visitCount)/c.visitCount)/sumVisitCount \\\n",
      " \t\t\t\t\t        + heuristic_coefficient*(transform(loc)/c.visitCount)/sumPseudoReward\t\n",
      " \t\t\t\tif debug:\n",
      "-\t\t\t\t\tprint a, funcVal\n",
      "+\t\t\t\t\tprint(a, funcVal)\n",
      " \n",
      " \t\t\tif funcVal > maxFuncVal:\n",
      " \t\t\t\tmaxFuncVal = funcVal\n",
      " \t\t\t\tbestAction = a\n",
      " \t\t\t\tbestChild = c\n",
      " \t\tif bestChild == None:\t## Tiebreaker\n",
      "-\t\t\tbestAction = random.choice(v.children.keys())\n",
      "+\t\t\tbestAction = random.choice(list(v.children.keys()))\n",
      " \t\t\tbestChild = v.children[bestAction]\n",
      " \n",
      " \t\tif debug:\n",
      "-\t\t\tprint \"\"\n",
      "+\t\t\tprint(\"\")\n",
      " \t\treturn bestAction, bestChild\n",
      " \n",
      " \tdef defaultPolicy(self, v, rle, step_horizon, domain_knowledge=False):\n",
      "@@ -623,7 +623,7 @@\n",
      " \n",
      " \t\t\titers += 1\n",
      " \t\t\t\n",
      "-\t\t\tif domain_knowledge and avatar_loc in self.actionDict.keys():\n",
      "+\t\t\tif domain_knowledge and avatar_loc in list(self.actionDict.keys()):\n",
      " \t\t\t\tsample = random.choice(self.actionDict[avatar_loc])\n",
      " \t\t\telse:\n",
      " \t\t\t\tsample = random.choice([(-1,0), (1,0), (0,-1), (0,1)])\n",
      "@@ -631,7 +631,7 @@\n",
      " \t\t\ta = sample\n",
      " \n",
      " \t\t\tres = rle.step(a)\n",
      "-\t\t\tprint rle.show()\n",
      "+\t\t\tprint(rle.show())\n",
      " \t\t\tnew_state = res[\"observation\"]\n",
      " \t\t\tstate = new_state\n",
      " \t\t\tterminal = rle._isDone()[0]\n",
      "@@ -639,7 +639,7 @@\n",
      " \t\t\t\treward += g*self.rewardScaling\n",
      " \t\t\telse:\n",
      " \t\t\t\treward += g*res['reward']\n",
      "-\t\t\tprint reward\t\t\t\n",
      "+\t\t\tprint(reward)\t\t\t\n",
      " \t\t\tg *= self.decay_factor\n",
      " \n",
      " \t\treturn reward, iters\n",
      "@@ -698,10 +698,10 @@\n",
      " \tgameString, levelString = defInputGame(gameFilename, randomize=True)\n",
      " \trleCreateFunc = lambda: createRLInputGame(gameFilename)\n",
      " \trle = rleCreateFunc()\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \t# rle.immovables = ['wall', 'poison1', 'poison2']\n",
      "-\tprint \"\"\n",
      "-\tprint \"Initializing learner. Playing\", gameFilename\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"Initializing learner. Playing\", gameFilename)\n",
      " \tmcts=Basic_MCTS(existing_rle=rle, game=gameString, level=levelString, partitionWeights=[5,3,3])\t# ql.getBestActionsForPlayout(False, True)\n",
      " \t# for x in range(10):\n",
      " \t# \tprint '{0}\\r'.format(x),\n",
      "@@ -714,7 +714,7 @@\n",
      " \tactions = mcts.getBestActionsForPlayout()\n",
      " \t# ql.runEpisode()\n",
      " \tt2 = time.time() - t1\n",
      "-\tprint \"done in {} seconds\".format(t2)\n",
      "+\tprint(\"done in {} seconds\".format(t2))\n",
      " \t# ql.learn(100, satisfice=False)\n",
      " \n",
      " \tembed()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to keypress.py\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: keypress.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored test.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: test.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- test.py\t(original)\n",
      "+++ test.py\t(refactored)\n",
      "@@ -1,8 +1,8 @@\n",
      " from theory_template_080116 import Game, TimeStep\n",
      "-from sampleVGDLString import *\n",
      "-from taxonomy import *\n",
      "+from .sampleVGDLString import *\n",
      "+from .taxonomy import *\n",
      " from class_theory_template_071916 import *\n",
      "-from induction import runInduction_DFS\n",
      "+from .induction import runInduction_DFS\n",
      " from IPython import embed\n",
      " import time, ast\n",
      " \n",
      "@@ -30,16 +30,16 @@\n",
      " \tend = time.time()\n",
      " \n",
      " \t# Useful for quick view of test results\n",
      "-\tprint \"########################\"\n",
      "-\tprint \"Checking {}...\".format(name)\n",
      "-\tprint \"TIME TO RUN: {}\".format(end-start)\n",
      "-\tprint \"Expected number of hypotheses {} = actual number of hypotheses {}? {}\".format(expectedHypotheses, len(hypotheses), expectedHypotheses==len(hypotheses))\n",
      "+\tprint(\"########################\")\n",
      "+\tprint(\"Checking {}...\".format(name))\n",
      "+\tprint(\"TIME TO RUN: {}\".format(end-start))\n",
      "+\tprint(\"Expected number of hypotheses {} = actual number of hypotheses {}? {}\".format(expectedHypotheses, len(hypotheses), expectedHypotheses==len(hypotheses)))\n",
      " \t\n",
      " \tif expectedHypotheses==len(hypotheses): \t# TODO: Are there other parameters which we want to check?\n",
      "-\t\tprint \">>>> PASS!\"\n",
      "+\t\tprint(\">>>> PASS!\")\n",
      " \telse:\n",
      "-\t\tprint \">>>> FAIL :(\"\n",
      "-\tprint \"\\n########################\\n\\n\\n\\n\\n\\n\\n\"\n",
      "+\t\tprint(\">>>> FAIL :(\")\n",
      "+\tprint(\"\\n########################\\n\\n\\n\\n\\n\\n\\n\")\n",
      " \n",
      " \treturn hypotheses\n",
      " \n",
      "@@ -61,16 +61,16 @@\n",
      " \n",
      " \thypotheses=list(g.inductionOverMultipleTraces(traces, verbose))\n",
      " \tend = time.time()\n",
      "-\tprint \"########################\"\n",
      "-\tprint \"Checking {}...\".format(names)\n",
      "-\tprint \"TIME TO RUN: {}\".format(end-start)\n",
      "-\tprint \"Expected number of hypotheses {} = actual number of hypotheses {}? {}\".format(expectedHypotheses, len(hypotheses), expectedHypotheses==len(hypotheses))\n",
      "+\tprint(\"########################\")\n",
      "+\tprint(\"Checking {}...\".format(names))\n",
      "+\tprint(\"TIME TO RUN: {}\".format(end-start))\n",
      "+\tprint(\"Expected number of hypotheses {} = actual number of hypotheses {}? {}\".format(expectedHypotheses, len(hypotheses), expectedHypotheses==len(hypotheses)))\n",
      " \tif expectedHypotheses==len(hypotheses):\n",
      "-\t\tprint \">>>> PASS!\"\n",
      "+\t\tprint(\">>>> PASS!\")\n",
      " \telse:\n",
      "-\t\tprint \">>>> FAIL :(\"\n",
      "+\t\tprint(\">>>> FAIL :(\")\n",
      " \t# TODO: Are there other parameters which we want to check?\n",
      "-\tprint \"\\n########################\\n\\n\\n\\n\\n\\n\\n\"\n",
      "+\tprint(\"\\n########################\\n\\n\\n\\n\\n\\n\\n\")\n",
      " \t# embed()\n",
      " \treturn hypotheses\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP.py\t(original)\n",
      "+++ WBP.py\t(refactored)\n",
      "@@ -3,10 +3,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame\n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict, sys\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic\n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict, sys\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic\n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " import math\n",
      "@@ -16,20 +16,20 @@\n",
      " # import ipdb\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "-from util import *\n",
      "+from queue import Queue\n",
      "+from .util import *\n",
      " # import multiprocessing\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " NoveltyRule, generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from ontology import MovingAvatar, HorizontalAvatar, VerticalAvatar, FlakAvatar, AimedFlakAvatar, OrientedAvatar, \\\n",
      "+from .ontology import MovingAvatar, HorizontalAvatar, VerticalAvatar, FlakAvatar, AimedFlakAvatar, OrientedAvatar, \\\n",
      " \tRotatingAvatar, RotatingFlippingAvatar, NoisyRotatingFlippingAvatar, ShootAvatar, AimedAvatar, \\\n",
      " \t\tAimedFlakAvatar\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " # from line_profiler import LineProfiler\n",
      "-import cPickle\n",
      "+import pickle\n",
      " \n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      " NONE = 0\n",
      "@@ -48,13 +48,13 @@\n",
      " \t\tself.rle = rle\n",
      " \t\tself.gameFilename = gameFilename\n",
      " \t\tself.hyperparameter_index = hyperparameters['idx']\n",
      "-\t\tself.hyperparameters = dict((k, hyperparameters[k]) for k in hyperparameters.keys() if k not in ['idx'])\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.hyperparameters = dict((k, hyperparameters[k]) for k in list(hyperparameters.keys()) if k not in ['idx'])\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = defaultdict(lambda:0) #set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.seen_limits = seen_limits\n",
      " \t\tself.IW_k = IW_k\n",
      " \t\tself.objIDs = {}\n",
      "@@ -79,7 +79,7 @@\n",
      " \t\tself.exhausted_novelty = True\n",
      " \t\tself.extra_atom = extra_atom\n",
      " \t\tself.gameString_array = []\n",
      "-\t\tself.rolloutHyperparameters = dict([(k,v) if 'second' not in k else (k,0) for k,v in self.hyperparameters.items()])\n",
      "+\t\tself.rolloutHyperparameters = dict([(k,v) if 'second' not in k else (k,0) for k,v in list(self.hyperparameters.items())])\n",
      " \t\tself.hypotenuse_squared = self.rle.outdim[0]**2 + self.rle.outdim[1]**2\n",
      " \n",
      " \t\tif theory == None:\n",
      "@@ -89,7 +89,7 @@\n",
      " \t\t\tself.theory.interactionSet.extend(fakeInteractionRules)\n",
      " \t\t\tself.theory.updateTerminations()\n",
      " \t\n",
      "-\t\tif any([t in str(s.vgdlType) for s in self.theory.spriteObjects.values() for t in ['Missile', 'Random', 'Chaser']]):\n",
      "+\t\tif any([t in str(s.vgdlType) for s in list(self.theory.spriteObjects.values()) for t in ['Missile', 'Random', 'Chaser']]):\n",
      " \t\t\tmovingTypesInGame = True\n",
      " \t\telse:\n",
      " \t\t\tmovingTypesInGame = False\n",
      "@@ -102,7 +102,7 @@\n",
      " \t\telif self.hyperparameter_index == 3:\n",
      " \t\t\tself.position_score_multiplier = -10\n",
      " \t\telse:\n",
      "-\t\t\tprint \"Warning: haven't thought about position_score_multiplier for idx {}\".format(self.hyperparameter_index)\n",
      "+\t\t\tprint(\"Warning: haven't thought about position_score_multiplier for idx {}\".format(self.hyperparameter_index))\n",
      " \t\t\tself.position_score_multiplier = -10\n",
      " \n",
      " \t\t#################\n",
      "@@ -110,7 +110,7 @@\n",
      " \t\tself.display = False\n",
      " \n",
      " \t\tif self.display:\n",
      "-\t\t\tprint \"In planner; MovingTypesInGame: {}. Planning with idx {} and position_multiplier {}\".format(movingTypesInGame, self.hyperparameter_index, self.position_score_multiplier)\n",
      "+\t\t\tprint(\"In planner; MovingTypesInGame: {}. Planning with idx {} and position_multiplier {}\".format(movingTypesInGame, self.hyperparameter_index, self.position_score_multiplier))\n",
      " \n",
      " \n",
      " \t\tif self.theory.classes['avatar'][0].args and 'stype' in self.theory.classes['avatar'][0].args:\n",
      "@@ -124,11 +124,11 @@\n",
      " \t\t\t\tself.boxes.append(rule.slot1)\n",
      " \n",
      " \t\tif self.display:\n",
      "-\t\t\tprint 'max nodes', self.max_nodes\n",
      "-\t\t\tprint \"exta atom is {}\".format(self.extra_atom)\n",
      "+\t\t\tprint('max nodes', self.max_nodes)\n",
      "+\t\t\tprint(\"exta atom is {}\".format(self.extra_atom))\n",
      " \n",
      " \t\ti=1\n",
      "-\t\tfor k in rle._game.all_objects.keys():\n",
      "+\t\tfor k in list(rle._game.all_objects.keys()):\n",
      " \t\t\tself.objIDs[k] = i * 100 * (rle.outdim[0]*rle.outdim[1]+self.padding)\n",
      " \t\t\ti+=1\n",
      " \n",
      "@@ -138,7 +138,7 @@\n",
      " \n",
      " \t\tself.killer_types = [inter.slot2 for inter in self.theory.interactionSet if inter.slot1=='avatar' and inter.interaction in ['killSprite']]\n",
      " \t\tif self.display and self.killer_types:\n",
      "-\t\t\tprint 'killer types', self.killer_types\n",
      "+\t\t\tprint('killer types', self.killer_types)\n",
      " \n",
      " \t\tself.short_horizon = shortHorizon\n",
      " \t\tself.conservative = conservative\n",
      "@@ -147,19 +147,19 @@\n",
      " \n",
      " \t\tself.getAvailableActions()\n",
      " \t\tif self.display:\n",
      "-\t\t\tprint \"available actions:\", self.actions\n",
      "+\t\t\tprint(\"available actions:\", self.actions)\n",
      " \n",
      " \t\tif self.conservative:\n",
      " \t\t\tself.hyperparameters['sprite_negative_mult'] = 100\n",
      " \t\t\tif self.display:\n",
      "-\t\t\t\tprint \"Planning conservatively. Switched sprite_negative_mult to {}\".format(self.hyperparameters['sprite_negative_mult'])\n",
      "+\t\t\t\tprint(\"Planning conservatively. Switched sprite_negative_mult to {}\".format(self.hyperparameters['sprite_negative_mult']))\n",
      " \t\telse:\n",
      " \t\t\tif self.display:\n",
      "-\t\t\t\tprint \"Planning normally.\"\n",
      "+\t\t\t\tprint(\"Planning normally.\")\n",
      " \t\t## Ignore objects we don't want to track (i.e., non-moving immovables.)\n",
      " \t\tself.objectsToTrack = []\n",
      "-\t\tfor k in rle._game.sprite_groups.keys():\n",
      "-\t\t\tif ((k in self.theory.classes.keys() and ('Resource' or 'Immovable') in str(self.theory.classes[k][0].vgdlType) and not \\\n",
      "+\t\tfor k in list(rle._game.sprite_groups.keys()):\n",
      "+\t\t\tif ((k in list(self.theory.classes.keys()) and ('Resource' or 'Immovable') in str(self.theory.classes[k][0].vgdlType) and not \\\n",
      " \t\t\t(('bounceForward' or 'pullWithIt') in [rule.interaction for rule in self.theory.interactionSet if k in [rule.slot1, rule.slot2]])) or\n",
      " \t\t\tlen(rle._game.sprite_groups[k])>self.objectNumberTrackingLimit):\n",
      " \t\t\t\tpass# self.objectsToNotTrackInAtomList.append(k)\n",
      "@@ -174,8 +174,8 @@\n",
      " \t\t\t\tself.classesWhoseLocationsWeIgnore.append(k)\n",
      " \n",
      " \t\tif self.display:\n",
      "-\t\t\tprint \"ignoring presences for\", self.classesWhosePresenceWeIgnore\n",
      "-\t\t\tprint \"ignoring locations for\", self.classesWhoseLocationsWeIgnore\n",
      "+\t\t\tprint(\"ignoring presences for\", self.classesWhosePresenceWeIgnore)\n",
      "+\t\t\tprint(\"ignoring locations for\", self.classesWhoseLocationsWeIgnore)\n",
      " \t\t# Compute starting number of each SpriteCounter stype\n",
      " \t\tself.firstOrderHorizon = firstOrderHorizon\n",
      " \t\tself.starting_stype_n = {}\n",
      "@@ -221,7 +221,7 @@\n",
      " \t\t# \tself.actions = [NONE, K_UP, K_DOWN, K_LEFT, K_RIGHT]\n",
      " \t\t\n",
      " \t\t## get actions from avatar-type definition\n",
      "-\t\tself.actions = self.rle._game.getAvatars()[0].declare_possible_actions().values()\n",
      "+\t\tself.actions = list(self.rle._game.getAvatars()[0].declare_possible_actions().values())\n",
      " \t\tif self.conservative or self.addWaitAction:\n",
      " \t\t\tself.actions.append(NONE)\n",
      " \t\tself.actions = sorted(self.actions)\t\t\n",
      "@@ -239,7 +239,7 @@\n",
      " \t\t\t## a function of the Flicker's presence is being taken care of by that. Otherwise the agent can keep exploring states that have no actual effect\n",
      " \t\t\t## on the game state.\n",
      " \t\t\tif ((len(rle._game.sprite_groups[k])>0 and\n",
      "-\t\t\t\t\trle._game.sprite_groups[k][0].colorName in self.theory.spriteObjects.keys() and\n",
      "+\t\t\t\t\trle._game.sprite_groups[k][0].colorName in list(self.theory.spriteObjects.keys()) and\n",
      " \t\t\t\t\tany([obj in str(self.theory.spriteObjects[rle._game.sprite_groups[k][0].colorName].vgdlType) for obj in self.objectsWhoseLocationsWeIgnore])) or\n",
      " \t\t\t\tk in self.classesWhoseLocationsWeIgnore) or k==self.thingWeShoot:\n",
      " \n",
      "@@ -282,7 +282,7 @@\n",
      " \t\t\t## a function of the Flicker's presence is being taken care of by that. Otherwise the agent can keep exploring states that have no actual effect\n",
      " \t\t\t## on the game state.\n",
      " \t\t\tif (len(rle._game.sprite_groups[k])>0 and\n",
      "-\t\t\t\t\trle._game.sprite_groups[k][0].colorName in self.theory.spriteObjects.keys() and\n",
      "+\t\t\t\t\trle._game.sprite_groups[k][0].colorName in list(self.theory.spriteObjects.keys()) and\n",
      " \t\t\t\t\tany([obj in str(self.theory.spriteObjects[rle._game.sprite_groups[k][0].colorName].vgdlType) for obj in self.objectsWhosePresenceWeIgnore]) or\n",
      " \t\t\t\t\tk in self.classesWhosePresenceWeIgnore) or k==self.thingWeShoot:\n",
      " \t\t\t\tpass\n",
      "@@ -311,7 +311,7 @@\n",
      " \n",
      " \tdef compareDicts(self, d1,d2):\n",
      " \t\t## only tells us what is in d2 that isn't in d1, as well as differences in values between shared keys\n",
      "-\t\treturn [k for k in d2.keys() if (k not in d1.keys() or d1[k]!=d2[k])]\n",
      "+\t\treturn [k for k in list(d2.keys()) if (k not in list(d1.keys()) or d1[k]!=d2[k])]\n",
      " \n",
      " \tdef delta(self, node1, node2):\n",
      " \t\tif node1 is None:\n",
      "@@ -337,7 +337,7 @@\n",
      " \t\t\n",
      " \t\t## normal mode\n",
      " \t\t# if not self.conservative:\n",
      "-\t\tacceptableNodes = filter(lambda n: n.novelty<self.IW_k+1, QReward)\n",
      "+\t\tacceptableNodes = [n for n in QReward if n.novelty<self.IW_k+1]\n",
      " \t\t## sort max to min for pop()\n",
      " \t\tbestNodes = sorted(acceptableNodes, key=lambda n: (-n.intrinsic_reward, n.novelty))\n",
      " \t\t# else:\n",
      "@@ -355,13 +355,13 @@\n",
      " \t\t\t# print current.intrinsic_reward\n",
      " \t\t\t# current.ended, current.win = current.rle._isDone()\n",
      " \t\t\tif (current.terminal, current.win) == (True, False):\n",
      "-\t\t\t\tprint \"rewardSelection picked a loss node!!\"\n",
      "+\t\t\t\tprint(\"rewardSelection picked a loss node!!\")\n",
      " \t\t\t\tembed()\n",
      " \t\t\tif current.terminal and not current.win:\n",
      "-\t\t\t\tprint \"rewardSelection picked a loss node!!\"\n",
      "+\t\t\t\tprint(\"rewardSelection picked a loss node!!\")\n",
      " \t\t\t\tembed()\n",
      " \t\t\tif current.badOutcomes:\n",
      "-\t\t\t\tprint \"picked a node with >0 badoutcomes\"\n",
      "+\t\t\t\tprint(\"picked a node with >0 badoutcomes\")\n",
      " \t\t\t\tembed()\n",
      " \t\texcept:\n",
      " \t\t\t# if self.display:\n",
      "@@ -407,7 +407,7 @@\n",
      " \n",
      " \t\twhile (len(QNovelty)>0 or len(QReward)>0) and i<self.max_nodes:\n",
      " \t\t\tif self.display and i%100==0:\n",
      "-\t\t\t\tprint \"Searching node {}\".format(i)\n",
      "+\t\t\t\tprint(\"Searching node {}\".format(i))\n",
      " \n",
      " \t\t\tcurrent = self.rewardSelection(QReward, QNovelty)\n",
      " \t\t\t\n",
      "@@ -418,7 +418,7 @@\n",
      " \t\t\t\t\t\t# embed()\n",
      " \t\t\t\telse:\n",
      " \t\t\t\t\tif self.display:\n",
      "-\t\t\t\t\t\tprint \"Failed to find a novel node. Quitting\"\n",
      "+\t\t\t\t\t\tprint(\"Failed to find a novel node. Quitting\")\n",
      " \t\t\t\t\tnode = start\n",
      " \n",
      " \t\t\t\t# if self.short_horizon:\n",
      "@@ -455,7 +455,7 @@\n",
      " \t\t\t\t\t\t## QReward only has nodes that didn't result in loss states. Return *some* plan here to make sure things don't break\n",
      " \t\t\t\t\t\t## This is a plan of taking a single 'wait' action.\n",
      " \t\t\t\t\t\tif self.display:\n",
      "-\t\t\t\t\t\t\tprint \"QReward was empty -- returning a futile plan of a single 'none' action\"\n",
      "+\t\t\t\t\t\t\tprint(\"QReward was empty -- returning a futile plan of a single 'none' action\")\n",
      " \t\t\t\t\t\tstart = Node(self.rle, self, [], None)\n",
      " \t\t\t\t\t\tstart.rle = self.rle\n",
      " \t\t\t\t\t\tchild = Node(self.rle, self, start.actionSeq+[0], start)\n",
      "@@ -477,7 +477,7 @@\n",
      " \t\t\t\tself.quitting = True\n",
      " \t\t\t\tself.exhausted_novelty = True\n",
      " \t\t\t\tif self.display:\n",
      "-\t\t\t\t\tprint \"was in None or PickMaxNode\"\n",
      "+\t\t\t\t\tprint(\"was in None or PickMaxNode\")\n",
      " \t\t\t\t# embed()\n",
      " \t\t\t\treturn node, gameString_array, object_positions_array\n",
      " \n",
      "@@ -522,23 +522,23 @@\n",
      " \t\t\t\t\t\t\tcurrent_actions = self.actions\n",
      " \t\t\t\t\t\t\t# current_actions = [0, K_LEFT, K_RIGHT, K_UP, K_DOWN]\n",
      " \t\t\t\t\t\t\tif self.display:\n",
      "-\t\t\t\t\t\t\t\tprint \"didn't change current_actions; will plan normally\"\n",
      "-\t\t\t\t\t\t\t\tprint \"nearest dangerous sprite:\", manhattanDist(current.rle._rect2pos(avatar.rect), current.rle._rect2pos(nearest.rect))\n",
      "+\t\t\t\t\t\t\t\tprint(\"didn't change current_actions; will plan normally\")\n",
      "+\t\t\t\t\t\t\t\tprint(\"nearest dangerous sprite:\", manhattanDist(current.rle._rect2pos(avatar.rect), current.rle._rect2pos(nearest.rect)))\n",
      " \n",
      " \t\t\texcept (IndexError, AttributeError, TypeError) as e:\n",
      "-\t\t\t\tprint \"got triple except.\"\n",
      "+\t\t\t\tprint(\"got triple except.\")\n",
      " \t\t\t\tembed()\n",
      " \t\t\t\tpass\n",
      " \n",
      " \t\t\tif self.display:\n",
      "-\t\t\t\tprint \"________________\"\n",
      "+\t\t\t\tprint(\"________________\")\n",
      " \t\t\t\t# try:\n",
      " \t\t\t\t\t# print current.rle._game.getAvatars()[0].resources\n",
      " \t\t\t\t# except:\n",
      " \t\t\t\t\t# print \"\"\n",
      " \t\t\t\tif current.actionSeq:\n",
      "-\t\t\t\t\tprint actionDict[current.actionSeq[-1]]\n",
      "-\t\t\t\tprint current.rle.show()\n",
      "+\t\t\t\t\tprint(actionDict[current.actionSeq[-1]])\n",
      "+\t\t\t\tprint(current.rle.show())\n",
      " \t\t\t# if self.killer_types:\n",
      " \t\t\t\t# embed()\n",
      " \t\t\tfor a in current_actions:\n",
      "@@ -562,26 +562,26 @@\n",
      " \t\t\t\t\t\t\tif isinstance(term, SpriteCounterRule) and term.termination.win==True:\n",
      " \t\t\t\t\t\t\t\tstype = term.termination.stype\n",
      " \t\t\t\t\t\t\t\tn_stypes = len([0 for sprite in self.findObjectsInRLE(child.rle, stype)])\n",
      "-\t\t\t\t\t\t\t\tif stype in self.starting_stype_n.keys() and self.starting_stype_n[stype] > n_stypes:\n",
      "+\t\t\t\t\t\t\t\tif stype in list(self.starting_stype_n.keys()) and self.starting_stype_n[stype] > n_stypes:\n",
      " \t\t\t\t\t\t\t\t\tif ended and not win:\n",
      " \t\t\t\t\t\t\t\t\t\tchild.win, foundWin = False, False\n",
      " \t\t\t\t\t\t\t\t\telse:\n",
      " \t\t\t\t\t\t\t\t\t\tchild.terminal = True\n",
      " \t\t\t\t\t\t\t\t\t\tchild.win, foundWin = True, True\n",
      " \t\t\t\t\t\t\t\t\t\tif self.display:\n",
      "-\t\t\t\t\t\t\t\t\t\t\tprint \"exiting early because progress was made toward\", stype\n",
      "+\t\t\t\t\t\t\t\t\t\t\tprint(\"exiting early because progress was made toward\", stype)\n",
      " \t\t\t\t\t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\t\telif isinstance(term, MultiSpriteCounterRule) and term.termination.win==True:\n",
      " \t\t\t\t\t\t\t\tstypes = term.termination.stypes\n",
      " \t\t\t\t\t\t\t\tn_stypes = sum([len(self.findObjectsInRLE(child.rle, stype)) for stype in stypes if self.findObjectsInRLE(child.rle, stype)])\n",
      "-\t\t\t\t\t\t\t\tif tuple(stypes) in self.starting_stype_n.keys() and self.starting_stype_n[tuple(stypes)] > n_stypes:\n",
      "+\t\t\t\t\t\t\t\tif tuple(stypes) in list(self.starting_stype_n.keys()) and self.starting_stype_n[tuple(stypes)] > n_stypes:\n",
      " \t\t\t\t\t\t\t\t\tif ended and not win:\n",
      " \t\t\t\t\t\t\t\t\t\tchild.win, foundWin = False, False\n",
      " \t\t\t\t\t\t\t\t\telse:\n",
      " \t\t\t\t\t\t\t\t\t\tchild.terminal = True\n",
      " \t\t\t\t\t\t\t\t\t\tchild.win, foundWin = True, True\n",
      " \t\t\t\t\t\t\t\t\t\tif self.display:\n",
      "-\t\t\t\t\t\t\t\t\t\t\tprint \"exiting early because progress was made toward\", stypes\n",
      "+\t\t\t\t\t\t\t\t\t\t\tprint(\"exiting early because progress was made toward\", stypes)\n",
      " \t\t\t\t\t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\t\tif foundWin:\n",
      " \t\t\t\t\t\t\t\tbreak\n",
      "@@ -604,7 +604,7 @@\n",
      " \t\t\t\t\t\tif self.display:\n",
      " \t\t\t\t\t\t\t# print child.rle.show()\n",
      " \t\t\t\t\t\t\tif t:\n",
      "-\t\t\t\t\t\t\t\tprint t.__dict__\n",
      "+\t\t\t\t\t\t\t\tprint(t.__dict__)\n",
      " \t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\t# if a == K_LEFT:\n",
      "@@ -638,7 +638,7 @@\n",
      " \t\t\t\tbestNodes = sorted(self.winning_states, key=lambda n: (-n.intrinsic_reward))\n",
      " \t\t\t\tbestNode = bestNodes[0]\n",
      " \t\t\t\tif self.display:\n",
      "-\t\t\t\t\tprint \"found winning states\"\n",
      "+\t\t\t\t\tprint(\"found winning states\")\n",
      " \t\t\t\treturn bestNode, gameString_array, object_positions_array\n",
      " \n",
      " \t\t\t# print i\n",
      "@@ -649,13 +649,13 @@\n",
      " \t\tif self.conservative:\n",
      " \t\t\tif QReward:\n",
      " \t\t\t\tif self.display:\n",
      "-\t\t\t\t\tprint \"In short-horizon mode; selecting highest-reward longest sequence\"\n",
      "+\t\t\t\t\tprint(\"In short-horizon mode; selecting highest-reward longest sequence\")\n",
      " \t\t\t\tnode = max(QReward, key=lambda n:(n.intrinsic_reward, len(n.actionSeq)))\n",
      " \t\t\telse:\n",
      " \t\t\t\t## QReward only has nodes that didn't result in loss states. Return *some* plan here to make sure things don't break\n",
      " \t\t\t\t## This is a plan of taking a single 'wait' action.\n",
      " \t\t\t\tif self.display:\n",
      "-\t\t\t\t\tprint \"QReward was empty -- returning a futile plan of a single 'none' action\"\n",
      "+\t\t\t\t\tprint(\"QReward was empty -- returning a futile plan of a single 'none' action\")\n",
      " \t\t\t\tstart = Node(self.rle, self, [], None)\n",
      " \t\t\t\tstart.rle = self.rle\n",
      " \t\t\t\tchild = Node(self.rle, self, start.actionSeq+[0], start)\n",
      "@@ -678,9 +678,9 @@\n",
      " \t\t\t# print \"in WBP, within short horizon\"\n",
      " \t\t\t# embed()\n",
      " \t\t\tif not self.conservative and self.display:\n",
      "-\t\t\t\tprint \"End of shorthorizon plan\"\n",
      "+\t\t\t\tprint(\"End of shorthorizon plan\")\n",
      " \t\t\telif self.conservative and self.display:\n",
      "-\t\t\t\tprint \"End of conservative plan\"\n",
      "+\t\t\t\tprint(\"End of conservative plan\")\n",
      " \t\t\t# print \"ended conservative plan\"\n",
      " \t\t\t# embed()\n",
      " \t\t\treturn node, gameString_array, object_positions_array\n",
      "@@ -718,7 +718,7 @@\n",
      " \n",
      " \tdef fastcopy(self, rle):\n",
      " \t\tnewRle = self.empty_copy(rle)\n",
      "-\t\tfor k,v in rle.__dict__.iteritems():\n",
      "+\t\tfor k,v in rle.__dict__.items():\n",
      " \t\t\tctype = str(type(getattr(rle,k)))\n",
      " \t\t\tif 'defaultdict' in ctype or 'dict' in ctype:\n",
      " \t\t\t\tnewRle.__dict__[k] = v.copy()\n",
      "@@ -746,7 +746,7 @@\n",
      " \t\t\t\t\t\t'lastrect','lastmove','stypes', 'lastdisplacement',\n",
      " \t\t\t\t\t\t'speed','cooldown','direction','color','colorName']\n",
      " \n",
      "-\t\tfor k,v in rle._game.__dict__.iteritems():\n",
      "+\t\tfor k,v in rle._game.__dict__.items():\n",
      " \t\t\tif k in ignoreKeys: continue\n",
      " \n",
      " \t\t\tctype = str(type(getattr(rle._game,k)))\n",
      "@@ -762,7 +762,7 @@\n",
      " \t\t\t\telse:\n",
      " \t\t\t\t\t#print \"embed\"\n",
      " \t\t\t\t\tnew_sprite_groups = defaultdict(list)\n",
      "-\t\t\t\t\tfor group_name, group in rle._game.sprite_groups.iteritems():\n",
      "+\t\t\t\t\tfor group_name, group in rle._game.sprite_groups.items():\n",
      " \t\t\t\t\t\tfor sprite in group:\n",
      " \t\t\t\t\t\t\t#embed()\n",
      " \t\t\t\t\t\t\tif sprite.colorName == 'DARKGRAY':\n",
      "@@ -770,7 +770,7 @@\n",
      " \t\t\t\t\t\t\telse:\n",
      " \t\t\t\t\t\t\t\tnew_sprite = self.empty_copy(sprite)\n",
      " \t\t\t\t\t\t\t\ttry:\n",
      "-\t\t\t\t\t\t\t\t\tfor attr in sprite.__dict__.keys():\n",
      "+\t\t\t\t\t\t\t\t\tfor attr in list(sprite.__dict__.keys()):\n",
      " \t\t\t\t\t\t\t\t\t\tif hasattr(sprite, attr):\n",
      " \t\t\t\t\t\t\t\t\t\t\tsetattr(new_sprite, attr, getattr(sprite, attr))\n",
      " \t\t\t\t\t\t\t\t\t\t#else:\n",
      "@@ -851,7 +851,7 @@\n",
      " \t\t\t\tres = vrle.step(a, getTermination=True, getEffectList=True)\n",
      " \t\t\t\t# ended, win, t = res['ended'], res['win'], res['termination']\n",
      " \t\t\t\tif self.WBP.display:\n",
      "-\t\t\t\t\tprint vrle.show(indent=True, color='cyan')\n",
      "+\t\t\t\t\tprint(vrle.show(indent=True, color='cyan'))\n",
      " \t\t\t\tcurrHeuristicVal = self.heuristics(vrle, **self.WBP.rolloutHyperparameters)\n",
      " \t\t\t\theuristicVal = currHeuristicVal-prevHeuristicVal\n",
      " \t\t\t\trolloutArray.append(heuristicVal)\n",
      "@@ -866,20 +866,20 @@\n",
      " \t\t\t\t\t\tif isinstance(term, SpriteCounterRule) and term.termination.win==True:\n",
      " \t\t\t\t\t\t\tstype = term.termination.stype\n",
      " \t\t\t\t\t\t\tn_stypes = len([0 for sprite in self.WBP.findObjectsInRLE(vrle, stype)])\n",
      "-\t\t\t\t\t\t\tif stype in self.WBP.starting_stype_n.keys() and self.WBP.starting_stype_n[stype] > n_stypes:\n",
      "+\t\t\t\t\t\t\tif stype in list(self.WBP.starting_stype_n.keys()) and self.WBP.starting_stype_n[stype] > n_stypes:\n",
      " \t\t\t\t\t\t\t\tif not (terminal and not win):\n",
      " \t\t\t\t\t\t\t\t\tterminal, win = True, True\n",
      " \t\t\t\t\t\t\t\t\tif self.WBP.display:\n",
      "-\t\t\t\t\t\t\t\t\t\tprint \"exiting rollout early because progress was made toward\", stype\n",
      "+\t\t\t\t\t\t\t\t\t\tprint(\"exiting rollout early because progress was made toward\", stype)\n",
      " \t\t\t\t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\telif isinstance(term, MultiSpriteCounterRule) and term.termination.win==True:\n",
      " \t\t\t\t\t\t\tstypes = term.termination.stypes\n",
      " \t\t\t\t\t\t\tn_stypes = sum([len(self.WBP.findObjectsInRLE(vrle, stype)) for stype in stypes if self.WBP.findObjectsInRLE(vrle, stype)])\n",
      "-\t\t\t\t\t\t\tif tuple(stypes) in self.WBP.starting_stype_n.keys() and self.WBP.starting_stype_n[tuple(stypes)] > n_stypes:\n",
      "+\t\t\t\t\t\t\tif tuple(stypes) in list(self.WBP.starting_stype_n.keys()) and self.WBP.starting_stype_n[tuple(stypes)] > n_stypes:\n",
      " \t\t\t\t\t\t\t\tif not(terminal and not win):\n",
      " \t\t\t\t\t\t\t\t\tterminal, win = True, True\n",
      " \t\t\t\t\t\t\t\t\tif self.WBP.display:\n",
      "-\t\t\t\t\t\t\t\t\t\tprint \"exiting rollout early because progress was made toward\", stypes\n",
      "+\t\t\t\t\t\t\t\t\t\tprint(\"exiting rollout early because progress was made toward\", stypes)\n",
      " \t\t\t\t\t\t\t\t\t\t# embed()\n",
      " \t\t\t\t\t\tif win:\n",
      " \t\t\t\t\t\t\tbreak\n",
      "@@ -899,7 +899,7 @@\n",
      " \t\t\t\t\t\t\t\tterminal, win = False, False\n",
      " \t\t\t\t\t\tif terminal:\n",
      " \t\t\t\t\t\t\tif self.WBP.display:\n",
      "-\t\t\t\t\t\t\t\tprint t.name, t.s1, t.s2\n",
      "+\t\t\t\t\t\t\t\tprint(t.name, t.s1, t.s2)\n",
      " \t\t\t\t\texcept (IndexError, AttributeError) as e:\n",
      " \t\t\t\t\t\t# Avatar is dead or doesn't have projectile\n",
      " \t\t\t\t\t\tpass\n",
      "@@ -917,7 +917,7 @@\n",
      " \t\t# embed()\n",
      " \t\tif win:\n",
      " \t\t\tif self.WBP.display:\n",
      "-\t\t\t\tprint \"rolloutwin\"\n",
      "+\t\t\t\tprint(\"rolloutwin\")\n",
      " \t\t\t# embed()\n",
      " \t\t\tself.terminal = terminal\n",
      " \t\t\tself.win = win\n",
      "@@ -1240,7 +1240,7 @@\n",
      " \t\t\t\ttry:\n",
      " \t\t\t\t\tresource_positions = [np.concatenate([self.WBP.findObjectsInRLE(rle, yielder) for yielder in yielders]) for yielders in resource_yielder_names]\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"problem with resource positions\"\n",
      "+\t\t\t\t\tprint(\"problem with resource positions\")\n",
      " \t\t\t\t\tresource_positions = []\n",
      " \t\t\t\t\t# embed()\n",
      " \n",
      "@@ -1604,7 +1604,7 @@\n",
      " \t\t\t\t\t\tbadOutcomeLimit = 0\n",
      " \t\t\t\t\t\tokOutcomes, badOutcomes = [], []\n",
      " \t\t\t\t\t\tfor i in range(10):\n",
      "-\t\t\t\t\t\t\tprint \"multiple samples\"\n",
      "+\t\t\t\t\t\t\tprint(\"multiple samples\")\n",
      " \t\t\t\t\t\t\tvrle = self.fastcopy(self.parent.rle)\n",
      " \t\t\t\t\t\t\tres = vrle.step(a, return_obs=True)\n",
      " \t\t\t\t\t\t\tterminal, win = res['ended'], res['win']\n",
      "@@ -1640,7 +1640,7 @@\n",
      " \t\t\t\t\t\tself.metabolic_cost = 0\n",
      " \t\t\t\t\t\t# self.terminal, self.win = vrle._isDone()\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t\t# print \"in a reconstructed node\"\n",
      "@@ -1722,9 +1722,9 @@\n",
      " \t\t# print(\"intrinsic_reward {}\".format(self.intrinsic_reward))\n",
      " \t\ttry:\n",
      " \t\t\t## Planner should return a plan when the agent has reached the limit of any particular resource (because we now should be curious about new objects, which we're taking care of in main_agent)\n",
      "-\t\t\tif any([self.rle._game.getAvatars()[0].resources[k]==self.WBP.theory.resource_limits[k] for k in self.rle._game.getAvatars()[0].resources.keys() if k not in self.WBP.seen_limits]):\n",
      "+\t\t\tif any([self.rle._game.getAvatars()[0].resources[k]==self.WBP.theory.resource_limits[k] for k in list(self.rle._game.getAvatars()[0].resources.keys()) if k not in self.WBP.seen_limits]):\n",
      " \t\t\t\t# if self.WBP.display:\n",
      "-\t\t\t\tprint \"resource limit win\"\n",
      "+\t\t\t\tprint(\"resource limit win\")\n",
      " \t\t\t\tself.win=True\n",
      " \t\texcept IndexError:\n",
      " \t\t\tpass\n",
      "@@ -1755,12 +1755,12 @@\n",
      " \t\ti = 0\n",
      " \t\tfor objType in vrle._game.sprite_groups:\n",
      " \t\t\tfor s in vrle._game.sprite_groups[objType]:\n",
      "-\t\t\t\tif s.ID not in self.WBP.objIDs.keys():\n",
      "+\t\t\t\tif s.ID not in list(self.WBP.objIDs.keys()):\n",
      " \t\t\t\t\tif s.name=='bullet':\n",
      " \t\t\t\t\t\ts.ID = len([o for o in vrle._game.sprite_groups[objType] if o not in vrle._game.kill_list])\n",
      " \t\t\t\t\telse:\n",
      " \t\t\t\t\t\ts.ID = len(vrle._game.sprite_groups[objType])\n",
      "-\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(self.WBP.objIDs.keys())+1) * 100 * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      "+\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(list(self.WBP.objIDs.keys()))+1) * 100 * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      " \t\t\t\t\ti+=1\n",
      " \t\treturn\n",
      " \n",
      "@@ -1771,20 +1771,20 @@\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      " \t\tif not make_movie:\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\twhile not terminal and i<len(self.actionSeq):\n",
      " \t\t\ta = self.actionSeq[i]\n",
      " \t\t\tvrle.step(a)\n",
      " \t\t\tif not make_movie:\n",
      "-\t\t\t\tprint actionDict[a]\n",
      "-\t\t\t\tprint vrle.show()\n",
      "+\t\t\t\tprint(actionDict[a])\n",
      "+\t\t\t\tprint(vrle.show())\n",
      " \t\t\telse:\n",
      " \t\t\t\tself.finalStatesEncountered.append(vrle._game.getFullState())\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\ti+=1\n",
      " def gen_color():\n",
      " \tfrom vgdl.colors import colorDict\n",
      "-\tcolor_list = colorDict.values()\n",
      "+\tcolor_list = list(colorDict.values())\n",
      " \tcolor_list = [c for c in color_list if c not in ['UUWSWF']]\n",
      " \tfor color in color_list:\n",
      " \t\tyield color\n",
      "@@ -1889,14 +1889,14 @@\n",
      " \tgvgname = \"./{}/{}\".format(gameFileString,game_name)\n",
      " \tgameString = read_gvgai_game('{}.txt'.format(gvgname))\n",
      " \tgame_levels = [l for l in os.listdir(gameFileString) if l[0:len(game_name+'_lvl')] == game_name+'_lvl']\n",
      "-\tprint game_levels\n",
      "+\tprint(game_levels)\n",
      " \tlevel_game_pairs = []\n",
      " \tfor level_number in range(len(game_levels)):\n",
      " \t\twith open('{}_lvl{}.txt'.format(gvgname, level_number), 'r') as level:\n",
      " \t\t\tlevel_game_pairs.append([gameString, level.read()])\n",
      " \n",
      " \thyperparameters = hyperparameter_sets[hyperparameter_index]\n",
      "-\tplanner_hyperparameters = dict((k, hyperparameters[k]) for k in hyperparameters.keys() if k not in ['short_horizon', 'first_order_horizon'])\n",
      "+\tplanner_hyperparameters = dict((k, hyperparameters[k]) for k in list(hyperparameters.keys()) if k not in ['short_horizon', 'first_order_horizon'])\n",
      " \n",
      " \n",
      " \tgameString, levelString = level_game_pairs[level_num]\n",
      "@@ -1927,15 +1927,15 @@\n",
      " \t\tgameString_array = p.gameString_array\n",
      " \t\tobjectPositionsArray = objectPositionsArray[::-1]\n",
      " \tif solution and not p.quitting:\n",
      "-\t\tprint \"=============================================\"\n",
      "-\t\tprint \"got solution of length\", len(solution)\n",
      "-\t\tprint colored(p.gameString_array[0], 'green')\n",
      "+\t\tprint(\"=============================================\")\n",
      "+\t\tprint(\"got solution of length\", len(solution))\n",
      "+\t\tprint(colored(p.gameString_array[0], 'green'))\n",
      " \t\tfor i,g in enumerate(p.gameString_array[1:]):\n",
      "-\t\t\tprint actionDict[solution[i]]\n",
      "-\t\t\tprint colored(g, 'green')\n",
      "-\t\tprint \"=============================================\"\n",
      "-\n",
      "-\tprint time.time()-t1\n",
      "+\t\t\tprint(actionDict[solution[i]])\n",
      "+\t\t\tprint(colored(g, 'green'))\n",
      "+\t\tprint(\"=============================================\")\n",
      "+\n",
      "+\tprint(time.time()-t1)\n",
      " \t\n",
      " \t# from core import VGDLParser\n",
      " \t# embed()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored agents.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: agents.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- agents.py\t(original)\n",
      "+++ agents.py\t(refactored)\n",
      "@@ -13,7 +13,7 @@\n",
      " from pybrain.rl.learners.modelbased import policyIteration\n",
      " from pybrain.utilities import drawIndex\n",
      " \n",
      "-from ontology import BASEDIRS\n",
      "+from .ontology import BASEDIRS\n",
      " \n",
      " class UserTiredException(Exception):\n",
      "     \"\"\" Raised when the player is fed up of the game. \"\"\"\n",
      "@@ -25,7 +25,7 @@\n",
      "     def getAction(self):\n",
      "         from pygame.locals import K_LEFT, K_RIGHT, K_UP, K_DOWN\n",
      "         from pygame.locals import K_ESCAPE, QUIT        \n",
      "-        from ontology import RIGHT, LEFT, UP, DOWN\n",
      "+        from .ontology import RIGHT, LEFT, UP, DOWN\n",
      "         pygame.event.pump()\n",
      "         keystate = pygame.key.get_pressed()    \n",
      "         res = None\n",
      "@@ -53,7 +53,7 @@\n",
      "     def buildOptimal(game_env, discountFactor=0.99):\n",
      "         \"\"\" Given a game, find the optimal (state-based) policy and \n",
      "         return an agent that is playing accordingly. \"\"\"\n",
      "-        from mdpmap import MDPconverter\n",
      "+        from .mdpmap import MDPconverter\n",
      "         C = MDPconverter(env=game_env)\n",
      "         Ts, R, _ = C.convert()\n",
      "         policy, _ = policyIteration(Ts, R, discountFactor=discountFactor)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored bigloop2.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: bigloop2.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- bigloop2.py\t(original)\n",
      "+++ bigloop2.py\t(refactored)\n",
      "@@ -1,11 +1,11 @@\n",
      "-from mcts import *\n",
      "-from util import *\n",
      "-from core import colorDict\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectSubgoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt\n",
      "+from .mcts import *\n",
      "+from .util import *\n",
      "+from .core import colorDict\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectSubgoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt\n",
      " import importlib\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " '''\n",
      " ## helpful functions or access methods:\n",
      "@@ -21,7 +21,7 @@\n",
      " \n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t## Initialize rle the agent behaves in.\n",
      " \trle = rleCreateFunc()\n",
      "-\trle._game.unknown_objects = rle._game.sprite_groups.keys()\n",
      "+\trle._game.unknown_objects = list(rle._game.sprite_groups.keys())\n",
      " \trle._game.unknown_objects.remove('avatar') \t\t\t\t\t\t\t\t\t\t\t## For now we're asumming agent knows self.\n",
      " \trle.agentStatePrev = {}\n",
      " \tall_objects = rle._game.getObjects()\n",
      "@@ -40,12 +40,12 @@\n",
      " \n",
      " \t## Fix this mess. Store the unknown categories. Select among those for a goal, and then provide that to selectToken.\n",
      " \tif unknown_categories==False:\n",
      "-\t\tprint \"initializing unknown objects:\"\n",
      "-\t\tunknown_categories = [k for k in rle._game.sprite_groups.keys() if k!='avatar']\n",
      "-\t\tprint [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in unknown_categories]\n",
      "+\t\tprint(\"initializing unknown objects:\")\n",
      "+\t\tunknown_categories = [k for k in list(rle._game.sprite_groups.keys()) if k!='avatar']\n",
      "+\t\tprint([colorDict[str(rle._game.sprite_groups[k][0].color)] for k in unknown_categories])\n",
      " \telse:\n",
      "-\t\tprint \"already know some objects. Unknown:\"\n",
      "-\t\tprint [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in unknown_categories]\n",
      "+\t\tprint(\"already know some objects. Unknown:\")\n",
      "+\t\tprint([colorDict[str(rle._game.sprite_groups[k][0].color)] for k in unknown_categories])\n",
      " \n",
      " \n",
      " \t# if unknown_objects==False: ##uninitialized\n",
      "@@ -99,18 +99,18 @@\n",
      " \t\t\tVrle.immovables = immovables\n",
      " \n",
      " \t\tif goalColor:\n",
      "-\t\t\tkey = [k for k in rle._game.sprite_groups.keys() if \\\n",
      "+\t\t\tkey = [k for k in list(rle._game.sprite_groups.keys()) if \\\n",
      " \t\t\tcolorDict[str(rle._game.sprite_groups[k][0].color)]==goalColor][0]\n",
      " \t\t\tactual_goal = rle._game.sprite_groups[key][0]\n",
      " \t\t\tobject_goal = actual_goal\n",
      "-\t\t\tprint \"goal is known:\", goalColor\n",
      "+\t\t\tprint(\"goal is known:\", goalColor)\n",
      " \t\telse:\n",
      " \t\t\ttry:\n",
      " \t\t\t\tobject_goal = random.choice(unknown_objects)\n",
      " \t\t\t\tembed()\n",
      " \t\t\t\tsubgoalLocation = selectSubgoalToken(Vrle, 'wall', unknown_objects)\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"no unknown objects and no goal? Embedding so you can debug.\"\n",
      "+\t\t\t\tprint(\"no unknown objects and no goal? Embedding so you can debug.\")\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \n",
      "@@ -148,7 +148,7 @@\n",
      " \t\t\thypotheses[0].goalColor=goalColor\n",
      " \n",
      " \tif playback:\t\t\t## TODO: Aritro cleans this up.\n",
      "-\t\tprint \"in playback\"\n",
      "+\t\tprint(\"in playback\")\n",
      " \t\tfrom vgdl.core import VGDLParser\n",
      " \t\tfrom examples.gridphysics.simpleGame4 import level, game\n",
      " \t\tplaybackGame = push_game\n",
      "@@ -180,6 +180,6 @@\n",
      " \t\t\tunknown_objects=unknown_objects, goalColor=goalColor, finalEventList=finalEventList, \\\n",
      " \t\t\tplayback=True)\n",
      " \t\ttally.append(won)\n",
      "-\t\tprint \"episode ended. Win:\", won\n",
      "-\t\tprint \"__________________________________________________\"\n",
      "-\tprint \"Won\", sum(tally), \"out of \", len(tally), \"episodes.\"\n",
      "+\t\tprint(\"episode ended. Win:\", won)\n",
      "+\t\tprint(\"__________________________________________________\")\n",
      "+\tprint(\"Won\", sum(tally), \"out of \", len(tally), \"episodes.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored value_iteration.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: value_iteration.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- value_iteration.py\t(original)\n",
      "+++ value_iteration.py\t(refactored)\n",
      "@@ -20,7 +20,7 @@\n",
      " \t\t\tfor a in actions:\n",
      " \t\t\t\tq_val = 0\n",
      " \t\t\t\tnewStateDist = transition[(prevState,a)]\n",
      "-\t\t\t\tfor newState, newStateProb in newStateDist.items():\n",
      "+\t\t\t\tfor newState, newStateProb in list(newStateDist.items()):\n",
      " \t\t\t\t\tq_val += rewardFunc(prevState) + gamma * newStateProb * oldValues[newState]\n",
      " \n",
      " \t\t\t\tnewValues[prevState] = max(newValues[prevState], q_val)\n",
      "@@ -28,7 +28,7 @@\n",
      " \t\treturn newValues\n",
      " \n",
      " \tvalues = {s: rewardFunc(s) for s in states}\n",
      "-\tterminationStates = {s for s,v in values.items() if v != 0}\n",
      "+\tterminationStates = {s for s,v in list(values.items()) if v != 0}\n",
      " \tfor i in range(iterations):\n",
      " \t\tvalues = single_value_iteration(values,terminationStates)\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored main_agent.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: main_agent.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- main_agent.py\t(original)\n",
      "+++ main_agent.py\t(refactored)\n",
      "@@ -1,22 +1,22 @@\n",
      " # from IPython import embed\n",
      "-from util import *\n",
      "-from core import colorDict, VGDLParser, sys, keyPresses\n",
      "-from ontology import *\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, \\\n",
      "+from .util import *\n",
      "+from .core import colorDict, VGDLParser, sys, keyPresses\n",
      "+from .ontology import *\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, \\\n",
      " SpriteCounterRule, MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt, generateSymbolDict, \\\n",
      " generateTheoryFromGame\n",
      " import os, subprocess, shutil\n",
      " from collections import defaultdict\n",
      "-import WBP\n",
      "+from . import WBP\n",
      " import importlib\n",
      " import numpy as np\n",
      " import random\n",
      "-import cPickle\n",
      "+import pickle\n",
      " import time\n",
      " from datetime import datetime\n",
      " import copy\n",
      "-from metaplanner import translateEvents, observe\n",
      "-from rlenvironmentnonstatic import createRLInputGame, createRLInputGameFromStrings, defInputGame, createMindEnv\n",
      "+from .metaplanner import translateEvents, observe\n",
      "+from .rlenvironmentnonstatic import createRLInputGame, createRLInputGameFromStrings, defInputGame, createMindEnv\n",
      " from termcolor import colored\n",
      " from pygame import K_LEFT, K_UP, K_RIGHT, K_DOWN, K_SPACE\n",
      " \n",
      "@@ -171,8 +171,8 @@\n",
      "             self.stored_max_nodes = self.max_nodes\n",
      " \n",
      "             if self.display_text:\n",
      "-                print \"Switching hyperparameters to {}\".format(new_index)\n",
      "-        planner_hyperparameters = dict((k, self.hyperparameters[k]) for k in self.hyperparameters.keys() if k not in ['short_horizon', 'first_order_horizon'])\n",
      "+                print(\"Switching hyperparameters to {}\".format(new_index))\n",
      "+        planner_hyperparameters = dict((k, self.hyperparameters[k]) for k in list(self.hyperparameters.keys()) if k not in ['short_horizon', 'first_order_horizon'])\n",
      "         return planner_hyperparameters\n",
      " \n",
      "     def initializeEnvironment(self):\n",
      "@@ -210,7 +210,7 @@\n",
      " \n",
      "     def getSpritesByColor(self, rle, color):\n",
      "         outList = []\n",
      "-        for k in rle._game.sprite_groups.keys():\n",
      "+        for k in list(rle._game.sprite_groups.keys()):\n",
      "             if rle._game.sprite_groups[k] and rle._game.sprite_groups[k][0].colorName==color:\n",
      "                 outList.extend(rle._game.sprite_groups[k])\n",
      "         if outList:\n",
      "@@ -226,7 +226,7 @@\n",
      "         ## Sets positions of objects in Vrle to what they were in the rle. Bypasses clunky VGDL level description.\n",
      " \n",
      "         old_sprite_groups = Vrle._game.sprite_groups\n",
      "-        for k in old_sprite_groups.keys():\n",
      "+        for k in list(old_sprite_groups.keys()):\n",
      "             if old_sprite_groups[k]:\n",
      "                 color = Vrle._game.sprite_groups[k][0].colorName\n",
      "                 matchingSpritesInRLE = self.getSpritesByColor(rle, color)\n",
      "@@ -255,11 +255,11 @@\n",
      "                             # print \"Failed to get params for Missile in main_agent\"\n",
      "                             # embed()\n",
      "                             pass\n",
      "-        for k,v in Vrle._game.sprite_groups.items():\n",
      "+        for k,v in list(Vrle._game.sprite_groups.items()):\n",
      "             for sprite in v:\n",
      "                 if sprite not in Vrle._game.kill_list:\n",
      "                     loc = (sprite.rect.left, sprite.rect.top)\n",
      "-                    if loc in Vrle._game.positionDict.keys():\n",
      "+                    if loc in list(Vrle._game.positionDict.keys()):\n",
      "                         Vrle._game.positionDict[loc].append(sprite)\n",
      "                     else:\n",
      "                         Vrle._game.positionDict[loc] = [sprite]\n",
      "@@ -319,15 +319,15 @@\n",
      "         return gameObject\n",
      " \n",
      "     def completeHypotheses(self, allObjects, statesEncountered, compactStates, first_time_playing_level):\n",
      "-        previous_colors = [o['type']['color'] for o in self.previous_objects.values()]\n",
      "-        current_colors = [o['type']['color'] for o in allObjects.values()]\n",
      "+        previous_colors = [o['type']['color'] for o in list(self.previous_objects.values())]\n",
      "+        current_colors = [o['type']['color'] for o in list(allObjects.values())]\n",
      "         if all([c in previous_colors for c in current_colors]):\n",
      "             self.observe(self.rle, 0, self.bestSpriteTypeDict, statesEncountered, compactStates, display=self.display_states) ## observe a couple steps so that you're not completely clueless about object movements when you're restarting a level.\n",
      "         else:\n",
      "             self.observe(self.rle, 5, self.bestSpriteTypeDict, statesEncountered, compactStates, display=self.display_states) ## observe many steps so that you're not completely clueless about object movements for the new level\n",
      " \n",
      "         ## Make sure any objects that appeared while we were observing are reflected in allObjects\n",
      "-        for k,v in self.rle._game.getObjects().items():\n",
      "+        for k,v in list(self.rle._game.getObjects().items()):\n",
      "             if k not in allObjects:\n",
      "                 allObjects[k] = v\n",
      " \n",
      "@@ -372,7 +372,7 @@\n",
      "         fullStateEpisodes, episodeCompactStates = {}, {}\n",
      "         for n_level, level_game in enumerate(level_game_pairs):\n",
      " \n",
      "-            print(\"Playing level {}\".format(n_level))\n",
      "+            print((\"Playing level {}\".format(n_level)))\n",
      "             (self.gameString, self.levelString) = level_game\n",
      "             self.max_nodes = self.starting_max_nodes\n",
      "             self.stored_max_nodes = self.max_nodes\n",
      "@@ -418,7 +418,7 @@\n",
      " \n",
      "                 first_time_playing_level = False\n",
      "                 i += 1\n",
      "-                print \"Finished in \", time.time() - t1\n",
      "+                print(\"Finished in \", time.time() - t1)\n",
      " \n",
      "                 episodeCompactStates[n_level] = allCompactStates\n",
      "                 fullStateEpisodes[n_level] = allStatesEncountered\n",
      "@@ -427,11 +427,11 @@\n",
      "                 if self.record_states:\n",
      "                     gameInfo = {'gameString':self.gameString, 'levelString':self.levelString, 'gameName':self.gameFilename}\n",
      "                     episodeList = [v for k,v in sorted(episodeCompactStates.items())]\n",
      "-                    print \"n_level\", n_level\n",
      "-                    print len(episodeList)\n",
      "+                    print(\"n_level\", n_level)\n",
      "+                    print(len(episodeList))\n",
      "                     # embed()\n",
      "                     with open(filename, 'wb') as f:\n",
      "-                        cPickle.dump({'gameInfo':gameInfo,'modelParams':self.param_ID, 'episodes':episodeList, 'time_elapsed':time.time()-starttime}, f)\n",
      "+                        pickle.dump({'gameInfo':gameInfo,'modelParams':self.param_ID, 'episodes':episodeList, 'time_elapsed':time.time()-starttime}, f)\n",
      "                     f.close()\n",
      " \n",
      "                 ## will write video data at the end of each episode\n",
      "@@ -440,7 +440,7 @@\n",
      "                     gameInfo = {'gameString':self.gameString, 'levelString':self.levelString, 'gameName':self.gameFilename}\n",
      "                     fullStateList = [v for k,v in sorted(fullStateEpisodes.items())]\n",
      "                     with open(videofilename, 'wb') as f:\n",
      "-                        cPickle.dump({'gameInfo':gameInfo,'modelParams':self.param_ID, 'episodes':fullStateList, 'time_elapsed':time.time()-starttime}, f)\n",
      "+                        pickle.dump({'gameInfo':gameInfo,'modelParams':self.param_ID, 'episodes':fullStateList, 'time_elapsed':time.time()-starttime}, f)\n",
      "                     f.close()\n",
      " \n",
      "             # if i < 10:\n",
      "@@ -461,7 +461,7 @@\n",
      " \n",
      "             if flexible_goals:\n",
      "                 ## When you embed, you can manually input changes in theory. See flexible_goals.py for an example.\n",
      "-                print \"in main_agent; playing with flexible_goals\"\n",
      "+                print(\"in main_agent; playing with flexible_goals\")\n",
      "                 embed()\n",
      " \n",
      "         endtime = time.time()\n",
      "@@ -499,7 +499,7 @@\n",
      "                  'ended': ended,\n",
      "                  'win': win,\n",
      "                  'objects': [(colorDict[str(s.color)], (s.rect.left/gameObject.block_size, s.rect.top/gameObject.block_size), s.resources if s.name=='avatar' else {}) \n",
      "-                        for sublist in gameObject.sprite_groups.values() for s in sublist if s not in gameObject.kill_list]\n",
      "+                        for sublist in list(gameObject.sprite_groups.values()) for s in sublist if s not in gameObject.kill_list]\n",
      "                  }\n",
      "         return state\n",
      " \n",
      "@@ -509,8 +509,8 @@\n",
      "         from matplotlib.ticker import NullLocator\n",
      "         import numpy as np\n",
      " \n",
      "-        states = [s['objects']['avatar'].keys()[0] for s in statesEncountered\n",
      "-                  if s['objects']['avatar'].keys()]\n",
      "+        states = [list(s['objects']['avatar'].keys())[0] for s in statesEncountered\n",
      "+                  if list(s['objects']['avatar'].keys())]\n",
      "         width, height = self.rle._game.width, self.rle._game.height\n",
      "         correction_factor = self.rle._game.screensize[0]/width\n",
      "         corrected_states = [(s[0]/correction_factor, s[1]/correction_factor) for s in states]\n",
      "@@ -544,7 +544,7 @@\n",
      "         import importlib\n",
      " \n",
      "         mod = importlib('vgdl.colors')\n",
      "-        colors = [cl[0].color for cl in self.hypotheses[0].classes.values()]\n",
      "+        colors = [cl[0].color for cl in list(self.hypotheses[0].classes.values())]\n",
      "         for color in colors:\n",
      "             times_touched_per_level = []\n",
      "             for level in allEffectsEncountered:\n",
      "@@ -565,11 +565,11 @@\n",
      " \n",
      "     def makeMovie(self):\n",
      " \n",
      "-        print \"Creating Movie\"\n",
      "+        print(\"Creating Movie\")\n",
      "         movie_dir = \"videos/{}/{}\".format(self.param_ID, self.gameFilename)\n",
      " \n",
      "         if not os.path.exists(movie_dir):\n",
      "-            print movie_dir, \"didn't exist. making new dir\"\n",
      "+            print(movie_dir, \"didn't exist. making new dir\")\n",
      "             os.makedirs(movie_dir)\n",
      "         round_index = len([d for d in os.listdir(movie_dir) if d != '.DS_Store'])\n",
      "         video_dirname = movie_dir+\"/round\"+str(round_index)+\".mp4\"\n",
      "@@ -594,7 +594,7 @@\n",
      "             i+=1\n",
      "         VGDLParser.playGame(self.gameString, self.levelString, self.statesEncountered, \\\n",
      "             persist_movie=True, make_images=True, make_movie=False, movie_dir=\"videos/\"+self.gameFilename, padding=10)\n",
      "-        print \"Won {} out of {} episodes.\".format(sum(wins), i)\n",
      "+        print(\"Won {} out of {} episodes.\".format(sum(wins), i))\n",
      " \n",
      "         \"\"\"\n",
      "         def playEpisodeProfiler(self, gameObject, flexible_goals=False, first_time_playing_level=False):\n",
      "@@ -611,9 +611,9 @@\n",
      "         ## Initialize external environment\n",
      "         self.initializeEnvironment()\n",
      "         if self.display_text:\n",
      "-            print \"initializing RLE\"\n",
      "-        print \"Game name:\", self.gameFilename\n",
      "-        print self.rle.show(color='blue')\n",
      "+            print(\"initializing RLE\")\n",
      "+        print(\"Game name:\", self.gameFilename)\n",
      "+        print(self.rle.show(color='blue'))\n",
      " \n",
      "         self.quits = 0\n",
      "         self.longHorizonObservations = 0\n",
      "@@ -634,7 +634,7 @@\n",
      "             compactStates.append(self.compactify(self.rle))\n",
      "         ## Initialize memory of object positions\n",
      "         self.rle._game.objectMemoryDict, self.rle._game.previousPositions = {}, {}\n",
      "-        for k, v in self.rle._game.all_objects.iteritems():\n",
      "+        for k, v in self.rle._game.all_objects.items():\n",
      "             self.rle._game.objectMemoryDict[k] = (int(self.rle._game.all_objects[k]['sprite'].rect.x), int(self.rle._game.all_objects[k]['sprite'].rect.y))\n",
      "             self.rle._game.previousPositions[k] = (int(self.rle._game.all_objects[k]['sprite'].rect.x), int(self.rle._game.all_objects[k]['sprite'].rect.y))\n",
      " \n",
      "@@ -642,11 +642,11 @@\n",
      "         if len(self.hypotheses) == 0:\n",
      "             gameObject = self.initializeHypotheses(self.all_objects, statesEncountered, compactStates, learnSprites=True)\n",
      "             if self.display_text:\n",
      "-                print \"initializing hypotheses\"\n",
      "+                print(\"initializing hypotheses\")\n",
      "         else:\n",
      "             gameObject = self.completeHypotheses(self.all_objects, statesEncountered, compactStates, first_time_playing_level)\n",
      "             if self.display_text:\n",
      "-                print \"had hypotheses -- completing them.\"\n",
      "+                print(\"had hypotheses -- completing them.\")\n",
      "             # If theory is being carried over, falsify termination hypotheses\n",
      "             # given new level state\n",
      "             if not flexible_goals:\n",
      "@@ -655,7 +655,7 @@\n",
      " \n",
      "         ## Do beginning-of-episode resource-management.\n",
      "         resources = self.rle._game.getAvatars()[0].resources\n",
      "-        for resource, val in resources.items():\n",
      "+        for resource, val in list(resources.items()):\n",
      "             if resource not in self.seen_resources and val>0:\n",
      "                 self.seen_resources.append(resource)\n",
      "                 self.hypotheses[0].resource_limits[resource] = self.rle._game.resources_limits[resource]\n",
      "@@ -673,15 +673,15 @@\n",
      "             self.max_nodes = self.stored_max_nodes\n",
      " \n",
      "             # if self.display_text:\n",
      "-            print \"planning with hyperparameter index {}\".format(self.hyperparameter_index)\n",
      "-            print \"max_nodes: {}, short_horizon: {}\".format(self.max_nodes, self.shortHorizon)\n",
      "+            print(\"planning with hyperparameter index {}\".format(self.hyperparameter_index))\n",
      "+            print(\"max_nodes: {}, short_horizon: {}\".format(self.max_nodes, self.shortHorizon))\n",
      " \n",
      "             ## initialize one or many VRLEs according to hypothesis-selection method\n",
      "             theoryRLEs = self.VrleInitPhase(flexible_goals)\n",
      " \n",
      "             quitting = False\n",
      " \n",
      "-            planner_hyperparameters = dict((k, self.hyperparameters[k]) for k in self.hyperparameters.keys() if k not in ['short_horizon', 'first_order_horizon'])\n",
      "+            planner_hyperparameters = dict((k, self.hyperparameters[k]) for k in list(self.hyperparameters.keys()) if k not in ['short_horizon', 'first_order_horizon'])\n",
      " \n",
      "             ## also, you commented out the bottom part of the planner, where it will still return a high-reward sequence in shortHorizon. This could have a very detrimental effect on short-horizon games...\n",
      " \n",
      "@@ -698,7 +698,7 @@\n",
      "                 gameString_array = p.gameString_array\n",
      "                 objectPositionsArray = objectPositionsArray[::-1]\n",
      "                 if solution and self.display_text:\n",
      "-                    print \"got solution\"\n",
      "+                    print(\"got solution\")\n",
      "             else:\n",
      "                 solution = []\n",
      " \n",
      "@@ -707,12 +707,12 @@\n",
      "                 if self.checkForRepeatedDeaths(self.episodeRecord, 2):\n",
      "                     if self.hyperparameter_index == 1:\n",
      "                         if self.display_text:\n",
      "-                            print \"Repeated deaths. Switching to short-range planning\"\n",
      "+                            print(\"Repeated deaths. Switching to short-range planning\")\n",
      "                         new_index = 3 \n",
      "                         conservative = False\n",
      "                     elif self.hyperparameter_index == 3:\n",
      "                         if self.display_text:\n",
      "-                            print \"Repeated deaths. Switching to long-range planning\"\n",
      "+                            print(\"Repeated deaths. Switching to long-range planning\")\n",
      "                         new_index = 1\n",
      "                         conservative = False\n",
      "                     planner_hyperparameters = self.hyperparameterSwitch(new_index=new_index)\n",
      "@@ -724,15 +724,15 @@\n",
      "                     else:\n",
      "                         scoreChange = True\n",
      "                     # if self.display_text:\n",
      "-                    print \"moving types: {}\".format(movingTypes)\n",
      "-                    print \"noNewObjectsInAWhile: {}\".format(self.noNewObjectsInAWhile(self.rle, 55))\n",
      "-                    print \"scoreChange: {}\".format(scoreChange)\n",
      "+                    print(\"moving types: {}\".format(movingTypes))\n",
      "+                    print(\"noNewObjectsInAWhile: {}\".format(self.noNewObjectsInAWhile(self.rle, 55)))\n",
      "+                    print(\"scoreChange: {}\".format(scoreChange))\n",
      "                     # print \"self.max_game_time_observed>501: {}\".format(self.max_game_time_observed>501)\n",
      "                     if self.noNewObjectsInAWhile(self.rle, 55) and \\\n",
      "                             (not movingTypes or (movingTypes and not scoreChange)):\n",
      "                             # (not movingTypes or (movingTypes and self.max_game_time_observed>501)):\n",
      "                         # if self.display_text:\n",
      "-                        print \"switching to long-range planning\"\n",
      "+                        print(\"switching to long-range planning\")\n",
      "                         ## switch to long-range planning\n",
      "                         new_index = 1\n",
      "                         planner_hyperparameters = self.hyperparameterSwitch(new_index=new_index)\n",
      "@@ -740,7 +740,7 @@\n",
      "                         # embed()\n",
      "                     else:\n",
      "                         # if self.display_text:\n",
      "-                        print \"planning conservatively\"\n",
      "+                        print(\"planning conservatively\")\n",
      "                         new_index = 3\n",
      "                         planner_hyperparameters = self.hyperparameterSwitch(new_index=new_index)\n",
      "                         conservative = True\n",
      "@@ -750,8 +750,8 @@\n",
      "                     conservative = False\n",
      " \n",
      "                 if self.display_text:\n",
      "-                    print \"planning with hyperparameter index {}\".format(self.hyperparameter_index)\n",
      "-                    print \"max_nodes: {}, short_horizon: {}, conservative: {}\".format(self.max_nodes, self.shortHorizon, conservative)\n",
      "+                    print(\"planning with hyperparameter index {}\".format(self.hyperparameter_index))\n",
      "+                    print(\"max_nodes: {}, short_horizon: {}, conservative: {}\".format(self.max_nodes, self.shortHorizon, conservative))\n",
      "                 # embed()\n",
      " \n",
      "                 if conservative:\n",
      "@@ -767,7 +767,7 @@\n",
      "                         gameString_array = p.gameString_array\n",
      "                         objectPositionsArray = objectPositionsArray[::-1]\n",
      "                         if solution and self.display_text:\n",
      "-                            print \"got solution\"\n",
      "+                            print(\"got solution\")\n",
      "                     else:\n",
      "                         solution = []\n",
      " \n",
      "@@ -777,11 +777,11 @@\n",
      "                 # ran out of novelty. In the first case, you only wait longer,\n",
      "                 # in the second case, you also add a new atom to IW\n",
      "                 if p.exhausted_novelty and self.extra_atom_allowed:\n",
      "-                    print \"turning on extra atom\"\n",
      "+                    print(\"turning on extra atom\")\n",
      "                     self.extra_atom = True\n",
      "                 if self.longHorizonObservations<self.longHorizonObservationLimit:\n",
      "                     if self.display_text:\n",
      "-                        print \"Didn't get solution. Observing, then replanning.\"\n",
      "+                        print(\"Didn't get solution. Observing, then replanning.\")\n",
      "                     self.observe(self.rle, 5, self.bestSpriteTypeDict, statesEncountered, compactStates)\n",
      "                     solution = [] ## You may have gotten p.quitting but also a solution; make sure you don't try to act on that if the planner decided it wasn't worth it.\n",
      "                     self.longHorizonObservations += 1\n",
      "@@ -794,13 +794,13 @@\n",
      "             self.actionSeqLength += len(solution)\n",
      " \n",
      "             if solution and not p.quitting and self.display_states:\n",
      "-                print \"=============================================\"\n",
      "-                print \"got solution of length\", len(solution)\n",
      "-                print colored(p.gameString_array[0], 'green')\n",
      "+                print(\"=============================================\")\n",
      "+                print(\"got solution of length\", len(solution))\n",
      "+                print(colored(p.gameString_array[0], 'green'))\n",
      "                 for i,g in enumerate(p.gameString_array[1:]):\n",
      "-                    print actionDict[solution[i]]\n",
      "-                    print colored(g, 'green')\n",
      "-                print \"=============================================\"\n",
      "+                    print(actionDict[solution[i]])\n",
      "+                    print(colored(g, 'green'))\n",
      "+                print(\"=============================================\")\n",
      " \n",
      "             ##new 6/30/18\n",
      "             # if emptyPlans > self.emptyPlansLimit:\n",
      "@@ -826,11 +826,11 @@\n",
      "                         run_induction = not flexible_goals)\n",
      "                     \n",
      "                     if self.display_text:\n",
      "-                        print \"executeStep took {} seconds\".format(time.time()-t1)\n",
      "+                        print(\"executeStep took {} seconds\".format(time.time()-t1))\n",
      "                     sys.stdout.flush()\n",
      "                     \n",
      "                     self.rle._game.nextPositions = {}\n",
      "-                    for k, v in self.rle._game.all_objects.iteritems():\n",
      "+                    for k, v in self.rle._game.all_objects.items():\n",
      "                         self.rle._game.nextPositions[k] = (int(self.rle._game.all_objects[k]['sprite'].rect.x), int(self.rle._game.all_objects[k]['sprite'].rect.y))\n",
      "                         try:\n",
      "                             if self.rle._game.previousPositions[k] != self.rle._game.nextPositions[k]:\n",
      "@@ -870,8 +870,8 @@\n",
      " \n",
      "                     if self.reground_for_npcs: ## this is just exercising caution when near random objects, irrespective of whether they kill us or not\n",
      "                         try:\n",
      "-                            random_npc_colors = [self.hypotheses[0].classes[k][0].color for k in self.hypotheses[0].classes.keys() if self.hypotheses[0].classes[k] and 'Random' in str(self.hypotheses[0].classes[k][0].vgdlType)]\n",
      "-                            random_npc_classes = [k for k in self.rle._game.sprite_groups.keys() if self.rle._game.sprite_groups[k] and self.rle._game.sprite_groups[k][0].colorName in random_npc_colors]\n",
      "+                            random_npc_colors = [self.hypotheses[0].classes[k][0].color for k in list(self.hypotheses[0].classes.keys()) if self.hypotheses[0].classes[k] and 'Random' in str(self.hypotheses[0].classes[k][0].vgdlType)]\n",
      "+                            random_npc_classes = [k for k in list(self.rle._game.sprite_groups.keys()) if self.rle._game.sprite_groups[k] and self.rle._game.sprite_groups[k][0].colorName in random_npc_colors]\n",
      "                             random_npc_positions = []\n",
      " \n",
      "                             for c in random_npc_classes:\n",
      "@@ -901,9 +901,9 @@\n",
      "                 win, effects = False, []\n",
      "                 self.episodeRecord.insert(0, (win, effects))\n",
      "                 # self.updateMemory(self.rle)\n",
      "-                print colored('________________________________________________________________', 'white', 'on_red')\n",
      "-                print colored(\"Quitting\", 'white', 'on_red')\n",
      "-                print colored('________________________________________________________________', 'white', 'on_red')\n",
      "+                print(colored('________________________________________________________________', 'white', 'on_red'))\n",
      "+                print(colored(\"Quitting\", 'white', 'on_red'))\n",
      "+                print(colored('________________________________________________________________', 'white', 'on_red'))\n",
      "                 return gameObject, False, self.rle._game.score, steps, statesEncountered, effectsEncountered, compactStates\n",
      " \n",
      " \n",
      "@@ -917,7 +917,7 @@\n",
      "                 self.episodeRecord.insert(0, (win, effects))\n",
      "             \n",
      "             if ended and not win and self.rle._game.time==2000:\n",
      "-                print \"lost on timeout. switching hyperparameters\"\n",
      "+                print(\"lost on timeout. switching hyperparameters\")\n",
      "                 self.hyperparameterSwitch(new_index=1)\n",
      " \n",
      " \n",
      "@@ -929,15 +929,15 @@\n",
      " \n",
      "         output =          \"ended episode. Win={}                                           \".format(win)\n",
      "         if win:\n",
      "-            print colored('________________________________________________________________', 'white', 'on_green')\n",
      "-            print colored('________________________________________________________________', 'white', 'on_green')\n",
      "-\n",
      "-            print colored(output, 'white', 'on_green')\n",
      "-            print colored('________________________________________________________________', 'white', 'on_green')\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "+\n",
      "+            print(colored(output, 'white', 'on_green'))\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_green'))\n",
      "         else:\n",
      "-            print colored('________________________________________________________________', 'white', 'on_red')\n",
      "-            print colored(output, 'white', 'on_red')\n",
      "-            print colored('________________________________________________________________', 'white', 'on_red')\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_red'))\n",
      "+            print(colored(output, 'white', 'on_red'))\n",
      "+            print(colored('________________________________________________________________', 'white', 'on_red'))\n",
      " \n",
      " \n",
      "         return gameObject, win, score, steps, statesEncountered, effectsEncountered, compactStates\n",
      "@@ -960,7 +960,7 @@\n",
      "         else:\n",
      "             thingWeShoot = None         \n",
      "         \n",
      "-        min_age = min([item.lastmove for sublist in self.rle._game.sprite_groups.values() for item in sublist if (item not in self.rle._game.kill_list and item.name not in [thingWeShoot, 'avatar'])])\n",
      "+        min_age = min([item.lastmove for sublist in list(self.rle._game.sprite_groups.values()) for item in sublist if (item not in self.rle._game.kill_list and item.name not in [thingWeShoot, 'avatar'])])\n",
      " \n",
      "         try:\n",
      "             time_since_last_kill = self.rle._game.time - max([item.deathage for item in self.rle._game.kill_list if item.name!=thingWeShoot])\n",
      "@@ -977,11 +977,11 @@\n",
      "             thingWeShoot = self.hypotheses[0].classes['avatar'][0].args['stype']\n",
      "         else:\n",
      "             thingWeShoot = None    \n",
      "-        moving_types = [k for k in hypothesis.classes.keys() if k!=thingWeShoot and any([t in str(hypothesis.classes[k][0].vgdlType) for t in ['Missile', 'Random', 'Chaser']])]\n",
      "+        moving_types = [k for k in list(hypothesis.classes.keys()) if k!=thingWeShoot and any([t in str(hypothesis.classes[k][0].vgdlType) for t in ['Missile', 'Random', 'Chaser']])]\n",
      "         moving_colors = [hypothesis.classes[k][0].color for k in moving_types]\n",
      "         movingTypes = False\n",
      "         if moving_colors:\n",
      "-            for s in [item for sublist in self.rle._game.sprite_groups.values() for item in sublist if item not in self.rle._game.kill_list]:\n",
      "+            for s in [item for sublist in list(self.rle._game.sprite_groups.values()) for item in sublist if item not in self.rle._game.kill_list]:\n",
      "                 if s.colorName in moving_colors:\n",
      "                     movingTypes = True\n",
      "                     break\n",
      "@@ -993,7 +993,7 @@\n",
      "         killer_colors = [hypothesis.classes[k][0].color for k in moving_killer_types]\n",
      "         danger = False\n",
      "         if killer_colors:\n",
      "-            for s in [item for sublist in self.rle._game.sprite_groups.values() for item in sublist if item not in self.rle._game.kill_list]:\n",
      "+            for s in [item for sublist in list(self.rle._game.sprite_groups.values()) for item in sublist if item not in self.rle._game.kill_list]:\n",
      "                 if s.colorName in killer_colors:\n",
      "                     danger = True\n",
      "                     break\n",
      "@@ -1007,25 +1007,25 @@\n",
      " \n",
      "         rleDict, hypDict = {}, {}\n",
      " \n",
      "-        for s in [item for sublist in objectPositionsArray[i+1]._game.sprite_groups.values() for item in sublist if item not in objectPositionsArray[i+1]._game.kill_list]:\n",
      "+        for s in [item for sublist in list(objectPositionsArray[i+1]._game.sprite_groups.values()) for item in sublist if item not in objectPositionsArray[i+1]._game.kill_list]:\n",
      "             hypDict[s.ID2] = s\n",
      " \n",
      "         killer_types = [inter.slot2 for inter in hypothesis.interactionSet if inter.slot1=='avatar' and inter.interaction in ['killSprite']]\n",
      "         killer_colors = [hypothesis.classes[k][0].color for k in killer_types]\n",
      " \n",
      "-        for s in [item for sublist in self.rle._game.sprite_groups.values() for item in sublist if item not in self.rle._game.kill_list]:\n",
      "+        for s in [item for sublist in list(self.rle._game.sprite_groups.values()) for item in sublist if item not in self.rle._game.kill_list]:\n",
      "             ## If the object isn't in our predicted environment or the positions vary\n",
      "             ## if it's an object we're worried about\n",
      "             if s.name=='avatar' or s.colorName in killer_colors:\n",
      "                 if s.ID not in hypDict and manhattanDist(s.rect, self.rle._game.getAvatars()[0].rect) < self.safeDistance*s.rect.width:\n",
      " \n",
      "                     regroundingFlag=True\n",
      "-                    print colored(\"Regrounding because we didn't predict the appearance of {} and it's too close for comfort\".format(s), 'white', 'on_yellow')\n",
      "+                    print(colored(\"Regrounding because we didn't predict the appearance of {} and it's too close for comfort\".format(s), 'white', 'on_yellow'))\n",
      "                     break\n",
      "                 if s.ID in hypDict and s.rect!=hypDict[s.ID].rect and manhattanDist(s.rect, self.rle._game.getAvatars()[0].rect) < self.safeDistance*s.rect.width:\n",
      "-                    print colored(\"Regrounding because distance between {} and {} is {}, which is less than the safe distance of {}. We thought it would be at {}\".format(\n",
      "+                    print(colored(\"Regrounding because distance between {} and {} is {}, which is less than the safe distance of {}. We thought it would be at {}\".format(\n",
      "                             s, self.rle._game.getAvatars()[0], manhattanDist(s.rect, self.rle._game.getAvatars()[0].rect), self.safeDistance*s.rect.width, hypDict[s.ID]),\n",
      "-                            'white', 'on_yellow')\n",
      "+                            'white', 'on_yellow'))\n",
      "                     regroundingFlag=True\n",
      "                     break\n",
      "                 rleDict[s.ID] = s\n",
      "@@ -1045,7 +1045,7 @@\n",
      "             hypSlot1 = self.hypotheses[0].spriteObjects[event[1]].className\n",
      "             hypSlot2 = self.hypotheses[0].spriteObjects[event[2]].className\n",
      "         except:\n",
      "-            print \"hypslot problem in main agent\"\n",
      "+            print(\"hypslot problem in main agent\")\n",
      "             embed()\n",
      " \n",
      " \n",
      "@@ -1063,11 +1063,11 @@\n",
      "     def manageNewObjects(self, hypotheses):\n",
      "         ## Add newly-seen objects.\n",
      "         current_objects = self.rle._game.getObjects()\n",
      "-        for k in current_objects.keys():\n",
      "+        for k in list(current_objects.keys()):\n",
      "             spriteName = current_objects[k]['sprite'].name\n",
      "-            if spriteName not in [self.all_objects[key]['sprite'].name for key in self.all_objects.keys()]:\n",
      "+            if spriteName not in [self.all_objects[key]['sprite'].name for key in list(self.all_objects.keys())]:\n",
      "                 if self.display_text:\n",
      "-                    print \"new object\", spriteName\n",
      "+                    print(\"new object\", spriteName)\n",
      "                 self.all_objects[k] = current_objects[k]\n",
      "                 distributionInitSetup(self.rle._game, k)\n",
      "                 ## prevent spriteInduction from trying to infer anything about newly-appeared sprites in this timestep.\n",
      "@@ -1109,7 +1109,7 @@\n",
      "     \"\"\"\n",
      "     def observe(self, rle, obsSteps, bestSpriteTypeDict, statesEncountered, compactStates, display=False):\n",
      "         if display:\n",
      "-            print \"observing for {} steps\".format(obsSteps)\n",
      "+            print(\"observing for {} steps\".format(obsSteps))\n",
      "         if obsSteps>0:\n",
      "             for i in range(obsSteps):\n",
      "                 spriteInduction(rle._game, step=1, bestSpriteTypeDict=bestSpriteTypeDict)\n",
      "@@ -1120,11 +1120,11 @@\n",
      "                 if self.record_states:\n",
      "                     compactStates.append(self.compactify(self.rle))\n",
      "                 if display:\n",
      "-                    print \"score: {}, game tick: {}\".format(rle._game.score, rle._game.time)\n",
      "-                    print rle.show(color='blue')\n",
      "+                    print(\"score: {}, game tick: {}\".format(rle._game.score, rle._game.time))\n",
      "+                    print(rle.show(color='blue'))\n",
      " \n",
      "                 rle._game.nextPositions = {}\n",
      "-                for k, v in rle._game.all_objects.iteritems():\n",
      "+                for k, v in rle._game.all_objects.items():\n",
      "                     rle._game.nextPositions[k] = (int(rle._game.all_objects[k]['sprite'].rect.x), int(rle._game.all_objects[k]['sprite'].rect.y))\n",
      "                     try:\n",
      "                         if rle._game.previousPositions[k] != rle._game.nextPositions[k]:\n",
      "@@ -1187,7 +1187,7 @@\n",
      "                         agentState[changes['resource']] += 0\n",
      "                         ignored_negative_change = True\n",
      "             self.rle.agentStatePrev = agentState\n",
      "-        for k,v in agentState.items():\n",
      "+        for k,v in list(agentState.items()):\n",
      "             agentState[k] = max(0, v)\n",
      "         # print \"post-step understanding of pre-step agentState (passed to induction): {}\".format(agentState)\n",
      "         # print \"agentState stuff: {}\".format(time.time()-t1)\n",
      "@@ -1218,13 +1218,13 @@\n",
      "         #     embed()\n",
      " \n",
      "         if self.display_states:\n",
      "-            print \"score: {}, game tick: {}\".format(self.rle._game.score, self.rle._game.time)\n",
      "+            print(\"score: {}, game tick: {}\".format(self.rle._game.score, self.rle._game.time))\n",
      "         \n",
      "         # t1 = time.time()\n",
      "         if self.display_states:\n",
      "-            print \"\"\n",
      "-            print keyPresses[action]\n",
      "-            print self.rle.show(color='blue')\n",
      "+            print(\"\")\n",
      "+            print(keyPresses[action])\n",
      "+            print(self.rle.show(color='blue'))\n",
      "         # print \"rle.show: {}\".format(time.time()-t1)\n",
      "         \n",
      "         # event = {'agentState': agentState, 'agentAction': action, 'effectList': effects, \\\n",
      "@@ -1240,7 +1240,7 @@\n",
      "         # print self.finalEffectList\n",
      "         if effects:\n",
      "             if self.display_text:\n",
      "-                print effects\n",
      "+                print(effects)\n",
      "             # #  PRECONDITIONS HANDLING\n",
      "             # # Current assumptions:\n",
      "             # # - Only one resource can change for each timestep\n",
      "@@ -1276,7 +1276,7 @@\n",
      "                 if compactEvent not in self.finalEffectList:\n",
      "                     self.finalEffectList.add(compactEvent)\n",
      "                     if self.display_text:\n",
      "-                        print \"New event: {}\".format(compactEvent)\n",
      "+                        print(\"New event: {}\".format(compactEvent))\n",
      "                     newEffects = True\n",
      "             # newEffects = True\n",
      "         \n",
      "@@ -1296,7 +1296,7 @@\n",
      "         if ((newEffects or (random.random()<.2 and len(self.finalTimeStepList)<300)) and run_induction) or distributionsHaveChanged:\n",
      "             # print \"event\", (not all([e in all_effects for e in effects])), \"distributions changed\", distributionsHaveChanged\n",
      "             if self.display_text:\n",
      "-                print \"new event\", newEffects, \"distributions changed\", distributionsHaveChanged\n",
      "+                print(\"new event\", newEffects, \"distributions changed\", distributionsHaveChanged)\n",
      " \n",
      "             ## Delete fake interaction rules for events that were witnessed in this time step.\n",
      "             # oldFakeInteractionRules = copy.deepcopy(self.fakeInteractionRules)\n",
      "@@ -1363,7 +1363,7 @@\n",
      "         # if hypotheses[0].__dict__ != oldhypothesis.__dict__:\n",
      "         if set(hypotheses[0].terminationSet) != oldTerminationSet:\n",
      "             if self.display_text:\n",
      "-                print \"terminationSet Change\"\n",
      "+                print(\"terminationSet Change\")\n",
      "             theory_change_flag = True\n",
      "             # embed()\n",
      " \n",
      "@@ -1371,7 +1371,7 @@\n",
      "             # embed()\n",
      "         # print \"updateTerminations took {} seconds\".format(time.time()-t1)\n",
      "         if theory_change_flag and not distributionsHaveChanged and self.display_text:\n",
      "-            print \"changed theory:\"\n",
      "+            print(\"changed theory:\")\n",
      "             hypotheses[0].display()\n",
      " \n",
      "         return hypotheses, theory_change_flag, effects\n",
      "@@ -1408,7 +1408,7 @@\n",
      " \n",
      "     def gen_color():\n",
      "         from vgdl.colors import colorDict\n",
      "-        color_list = colorDict.values()\n",
      "+        color_list = list(colorDict.values())\n",
      "         color_list = [c for c in color_list if c not in ['UUWSWF']]\n",
      "         for color in color_list:\n",
      "             yield color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to sampleVGDLString.py\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: sampleVGDLString.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to outline.py\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: outline.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored taxonomy.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: taxonomy.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- taxonomy.py\t(original)\n",
      "+++ taxonomy.py\t(refactored)\n",
      "@@ -1,6 +1,6 @@\n",
      " # from theory_template_071416 import *\n",
      "-from ontology import *\n",
      "-from sampleVGDLString import *\n",
      "+from .ontology import *\n",
      "+from .sampleVGDLString import *\n",
      " # import pygraphviz as PG\n",
      " \n",
      " '''\n",
      "@@ -30,22 +30,22 @@\n",
      " \t\t\tself.ancestors.add(self)\n",
      " \n",
      " \tdef addChild(self, sprite):\n",
      "-\t\tif str(sprite) in self.members.keys():\n",
      "-\t\t\tprint \"{} already in tree.\".format(sprite)\n",
      "+\t\tif str(sprite) in list(self.members.keys()):\n",
      "+\t\t\tprint(\"{} already in tree.\".format(sprite))\n",
      " \t\t\treturn\n",
      " \t\t#if the parent is in the tree\t\n",
      "-\t\telif str(sprite.__base__) in self.members.keys():\n",
      "+\t\telif str(sprite.__base__) in list(self.members.keys()):\n",
      " \t\t\tparent = self.members[str(sprite.__base__)]\n",
      " \t\t\tt = Tree(parent.name, sprite, parent)\n",
      " \t\t\tt.head = self.head #inherit head from head of tree\n",
      " \t\t\tparent.children.append(t)\n",
      " \t\t\tt.head.members[str(sprite)] = t\n",
      " \t\t\t# A.add_edge(str(parent.VGDLType), str(sprite))\n",
      "-\t\t\tprint \"added self, {}\".format(sprite)\n",
      "+\t\t\tprint(\"added self, {}\".format(sprite))\n",
      " \t\t #Otherwise, add the sprite's parent and then add the sprite.\n",
      " \t\telse:\n",
      " \t\t\tself.addChild(sprite.__base__)\n",
      "-\t\t\tprint \"added parent, {}\".format(sprite.__base__)\n",
      "+\t\t\tprint(\"added parent, {}\".format(sprite.__base__))\n",
      " \t\t\tself.addChild(sprite)\n",
      " \n",
      " \tdef distance(self, n1, n2):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP8.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP8.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP8.py\t(original)\n",
      "+++ WBP8.py\t(refactored)\n",
      "@@ -1,6 +1,6 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "-from core import VGDLParser\n",
      "+from .planner import *\n",
      "+from .core import VGDLParser\n",
      " import itertools\n",
      " \n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      "@@ -12,12 +12,12 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display=1)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = defaultdict(lambda:0) #set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.objIDs = {}\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      "@@ -26,7 +26,7 @@\n",
      " \t\tself.padding = 5  ##5 is arbitrary; just to make sure we don't get overlap when we add positions\n",
      " \t\tself.theory = generateTheoryFromGame(rle, alterGoal=False)\n",
      " \t\ti=1\n",
      "-\t\tfor k in rle._game.all_objects.keys():\n",
      "+\t\tfor k in list(rle._game.all_objects.keys()):\n",
      " \t\t\tself.objIDs[k] = i * (rle.outdim[0]*rle.outdim[1]+self.padding)\n",
      " \t\t\ti+=1\n",
      " \t\tself.addSpaceBarToActions()\n",
      "@@ -36,7 +36,7 @@\n",
      " \t\t## Note: if an object that isn't instantiated in the beginning is of a class that\n",
      " \t\t## spacebar applies to, we won't pick up on it here.\n",
      " \t\tshootingClasses = ['MarioAvatar', 'ClimbingAvatar', 'ShootAvatar', 'Switch', 'FlakAvatar']\n",
      "-\t\tclasses = [str(o[0].__class__) for o in self.rle._game.sprite_groups.values() if len(o)>0]\n",
      "+\t\tclasses = [str(o[0].__class__) for o in list(self.rle._game.sprite_groups.values()) if len(o)>0]\n",
      " \t\tspacebarAvailable = False\n",
      " \t\tfor sc in shootingClasses:\n",
      " \t\t\tif any([sc in c for c in classes]):\n",
      "@@ -52,7 +52,7 @@\n",
      " \n",
      " \tdef calculateAtoms(self, rle):\n",
      " \t\tlst = []\n",
      "-\t\tfor k in rle._game.sprite_groups.keys():\n",
      "+\t\tfor k in list(rle._game.sprite_groups.keys()):\n",
      " \t\t\tfor o in rle._game.sprite_groups[k]:\n",
      " \t\t\t\tif o not in rle._game.kill_list:\n",
      " \t\t\t\t\t## turn location into vector posd2[ition (rows appended one after the other.)\n",
      "@@ -74,12 +74,12 @@\n",
      " \t\tlst.append(ind)\n",
      " \t\tif not self.vecSize:\n",
      " \t\t\tself.vecSize = len(lst)\n",
      "-\t\t\tprint \"Vector is length {}\".format(self.vecSize)\n",
      "+\t\t\tprint(\"Vector is length {}\".format(self.vecSize))\n",
      " \t\treturn set(lst)\n",
      " \n",
      " \tdef compareDicts(self, d1,d2):\n",
      " \t\t## only tells us what is in d2 that isn't in d1, as well as differences in values between shared keys\n",
      "-\t\treturn [k for k in d2.keys() if (k not in d1.keys() or d1[k]!=d2[k])]\n",
      "+\t\treturn [k for k in list(d2.keys()) if (k not in list(d1.keys()) or d1[k]!=d2[k])]\n",
      " \n",
      " \tdef delta(self, node1, node2):\n",
      " \t\tif node1 is None:\n",
      "@@ -100,7 +100,7 @@\n",
      " \n",
      " def rewardSelection(QReward, QNovelty):\n",
      " \t# acceptableNodes = QReward\n",
      "-\tacceptableNodes = filter(lambda n:n.novelty<3, QReward)\n",
      "+\tacceptableNodes = [n for n in QReward if n.novelty<3]\n",
      " \t# if len(acceptableNodes)==0:\n",
      " \t\t# acceptableNodes = QReward\n",
      " \t\t# print \"Removed filter\"\n",
      "@@ -137,13 +137,13 @@\n",
      " \t\t# Add node state to states encountered\n",
      " \t\tWBP.statesEncountered.append(current.lastState._game.getFullState())\n",
      " \n",
      "-\t\tprint current.novelty, current.intrinsic_reward, current.heuristicVal\n",
      "+\t\tprint(current.novelty, current.intrinsic_reward, current.heuristicVal)\n",
      " \t\t# print len(QNovelty), len(QReward)\n",
      " \t\t# if current==None:\n",
      " \t\t# \tpass\n",
      " \t\t# else:\n",
      " \t\t# print current.novelty\n",
      "-\t\tprint current.lastState.show()\n",
      "+\t\tprint(current.lastState.show())\n",
      " \n",
      " \t\tcurrent.updateNoveltyDict(QNovelty, QReward)\n",
      " \t\t# embed()\n",
      "@@ -216,9 +216,9 @@\n",
      " \t\t\t# print vrle.show()\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\ti+=1\n",
      "-\t\tprint \"______\"\n",
      "+\t\tprint(\"______\")\n",
      " \t\tif terminal and not win:\n",
      "-\t\t\tprint \"recursive call to rollout.\"\n",
      "+\t\t\tprint(\"recursive call to rollout.\")\n",
      " \t\t\treturn []\n",
      " \t\t\t# return self.rollout(vrle2)\n",
      " \t\t# else:\n",
      "@@ -350,11 +350,11 @@\n",
      " \t\t\t\t\tself.metabolic_cost = self.parent.metabolic_cost + self.metabolics(vrle, res['effectList'], a)\n",
      " \t\t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\ti=0\n",
      "@@ -416,12 +416,12 @@\n",
      " \t\ti = 0\n",
      " \t\tfor objType in vrle._game.sprite_groups:\n",
      " \t\t\tfor s in vrle._game.sprite_groups[objType]:\n",
      "-\t\t\t\tif s.ID not in self.WBP.objIDs.keys():\n",
      "+\t\t\t\tif s.ID not in list(self.WBP.objIDs.keys()):\n",
      " \t\t\t\t\tif s.name=='bullet':\n",
      " \t\t\t\t\t\ts.ID = len([o for o in vrle._game.sprite_groups[objType] if o not in vrle._game.kill_list])\n",
      " \t\t\t\t\telse:\n",
      " \t\t\t\t\t\ts.ID = len(vrle._game.sprite_groups[objType])\n",
      "-\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(self.WBP.objIDs.keys())+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      "+\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(list(self.WBP.objIDs.keys()))+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      " \t\t\t\t\ti+=1\n",
      " \t\treturn\n",
      " \n",
      "@@ -435,13 +435,13 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      "-\t\t\tprint actionDict[a]\n",
      "+\t\t\tprint(actionDict[a])\n",
      " \t\t\tvrle.step(a)\n",
      " \t\t\t# vrle.step(0)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\t# embed()\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\ti+=1\n",
      "@@ -520,8 +520,8 @@\n",
      " \tt1 = time.time()\n",
      " \tlast, visited, rejected = BFS3(rle, p)\n",
      " \n",
      "-\tprint time.time()-t1\n",
      "-\tprint len(visited), len(rejected)\n",
      "+\tprint(time.time()-t1)\n",
      "+\tprint(len(visited), len(rejected))\n",
      " \tembed()\n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Can't parse scraps.py: ParseError: bad input: type=5, value='\\t', context=('\\n\\t## from WBP.py\\n', (3, 0))\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: There was 1 error:\n",
      "RefactoringTool: Can't parse scraps.py: ParseError: bad input: type=5, value='\\t', context=('\\n\\t## from WBP.py\\n', (3, 0))\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored aStar.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: aStar.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- aStar.py\t(original)\n",
      "+++ aStar.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict, sys\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict, sys\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,13 +14,13 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " import curses\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      "@@ -66,13 +66,13 @@\n",
      " \t\ttry:\n",
      " \t\t\timmovables = self.rle.immovables\n",
      " \t\t\t# immovables = ['wall', 'poison']\n",
      "-\t\t\tprint \"immovables\", immovables\n",
      "+\t\t\tprint(\"immovables\", immovables)\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall', 'poison']\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self.rle._obstypes.keys():\n",
      "+\t\t\tif i in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -107,7 +107,7 @@\n",
      " \t\twhile len(rewardQueue)>0:\n",
      " \t\t\tloc = rewardQueue.popleft()\n",
      " \t\t\tif loc not in processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tprocessed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -134,7 +134,7 @@\n",
      " \t\t\t\t\tq.append((neighbor, path + [neighbor]))\n",
      " \n",
      " \t\tif node != goal_loc:\n",
      "-\t\t\tprint \"didn't find path to goal in getSubgoals (in getPathToGoal)\"\n",
      "+\t\t\tprint(\"didn't find path to goal in getSubgoals (in getPathToGoal)\")\n",
      " \t\t\treturn False\n",
      " \t\t\t# embed()\n",
      " \t\t\t# raise Exception(\"Didn't find a path to the goal location.\")\n",
      "@@ -145,13 +145,13 @@\n",
      " \t\t## find location of goal, add to rewardDict.\n",
      " \t\t## also add neighbors of goal rewardQueue.\n",
      " \t\t##TODO: update this if goal moves!!\n",
      "-\t\tif \"goal\" not in self.rle._obstypes.keys():\n",
      "-\t\t\tprint \"no goal to get subgoals to\"\n",
      "+\t\tif \"goal\" not in list(self.rle._obstypes.keys()):\n",
      "+\t\t\tprint(\"no goal to get subgoals to\")\n",
      " \t\t\treturn []\n",
      " \t\tgoal_code = 2**(1+sorted(self.rle._obstypes.keys())[::-1].index(\"goal\"))\n",
      " \t\tkillerObjectCodes = []\n",
      " \t\tfor o in self.rle.killerObjects:\n",
      "-\t\t\tif o in self.rle._obstypes.keys():\n",
      "+\t\t\tif o in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\tkillerObjectCodes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(o)))\n",
      " \t\tboard = np.reshape(self.rle._getSensors(), self.rle.outdim)\n",
      " \t\tgoal_loc = np.where(board==goal_code)\n",
      "@@ -192,13 +192,13 @@\n",
      " \t\t\t\t\t\t\t\t# print \"found altered path\", path[subgoal_index+i]\n",
      " \t\t\t\t\t\t\t\tbreak\n",
      " \t\t\t\t\t\texcept:\n",
      "-\t\t\t\t\t\t\tprint \"indices didn't work out in looking for different path\"\n",
      "+\t\t\t\t\t\t\tprint(\"indices didn't work out in looking for different path\")\n",
      " \t\t\t\t# self.subgoals.append(path[subgoal_index])\n",
      " \t\t\n",
      " \t\treturn self.subgoals\n",
      " \tdef findObjectInRLE(self, rle, objName):\n",
      "-\t\tif objName not in rle._obstypes.keys():\n",
      "-\t\t\tprint objName, \"not in rle.\"\n",
      "+\t\tif objName not in list(rle._obstypes.keys()):\n",
      "+\t\t\tprint(objName, \"not in rle.\")\n",
      " \t\t\treturn None\n",
      " \t\tobjCode = 2**(1+sorted(self.rle._obstypes.keys())[::-1].index(objName))\n",
      " \t\tobjLoc = np.where(np.reshape(self.rle._getSensors(), self.rle.outdim)==objCode)\n",
      "@@ -291,7 +291,7 @@\n",
      " \t\t\t\ttry:\n",
      " \t\t\t\t\th = self.manhattanDistance(avatarLoc, goalLoc)\n",
      " \t\t\t\texcept:\n",
      "-\t\t\t\t\tprint \"couldn't find h\"\n",
      "+\t\t\t\t\tprint(\"couldn't find h\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\twin = False\n",
      " \t\t\telse:\n",
      "@@ -314,7 +314,7 @@\n",
      " \t\ttry:\n",
      " \t\t\tnode = Node(rle, s, None, (0,0), 0., self.manhattanDistance(avatarLoc, goalLoc), terminal, win)\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"couldn't make node becuase of manhattan distance\"\n",
      "+\t\t\tprint(\"couldn't make node becuase of manhattan distance\")\n",
      " \t\t\tembed()\n",
      " \t\tself.open.add(node)\n",
      " \n",
      "@@ -324,7 +324,7 @@\n",
      " \t\t\t# print current.f(), len(self.open), [o.f() for o in self.open]\n",
      " \t\t\tbestChildSum += time.time() - t1\n",
      " \t\t\tif current.terminal and current.win:\n",
      "-\t\t\t\tprint 'bestChildSum, makeneighborsum, loopsum', bestChildSum, makeneighborSum, loopSum\n",
      "+\t\t\t\tprint('bestChildSum, makeneighborsum, loopsum', bestChildSum, makeneighborSum, loopSum)\n",
      " \t\t\t\treturn self.constructPath(current)\n",
      " \t\t\telse:\n",
      " \t\t\t\tself.open.remove(current)\n",
      "@@ -340,7 +340,7 @@\n",
      " \t\t\t\t\t\telse:\n",
      " \t\t\t\t\t\t\tneighbors = [n for n in self.open if n.state==neighbor.state]\n",
      " \t\t\t\t\t\t\tif len(neighbors)>1:\n",
      "-\t\t\t\t\t\t\t\tprint \"found more than one neighbor\"\n",
      "+\t\t\t\t\t\t\t\tprint(\"found more than one neighbor\")\n",
      " \t\t\t\t\t\t\t\tembed()\n",
      " \t\t\t\t\t\t\topenNeighbor = neighbors[0]\n",
      " \n",
      "@@ -352,7 +352,7 @@\n",
      " \t\t\tloopSum += time.time() - t1\n",
      " \t\t\t# print i\n",
      " \t\t\ti +=1\n",
      "-\t\tprint 'bestChildSum, makeneighborsum, loopsum', bestChildSum, makeneighborSum, loopSum\n",
      "+\t\tprint('bestChildSum, makeneighborsum, loopsum', bestChildSum, makeneighborSum, loopSum)\n",
      " \t\treturn False\n",
      " \n",
      " \tdef constructPath(self, node):\n",
      "@@ -366,7 +366,7 @@\n",
      " \n",
      " \tdef playPath(self, path):\n",
      " \t\tfor p in path:\n",
      "-\t\t\tprint p.rle.show()\n",
      "+\t\t\tprint(p.rle.show())\n",
      " \n",
      " if __name__ == \"__main__\":\n",
      " \t\n",
      "@@ -374,14 +374,14 @@\n",
      " \t# gameFilename = \"examples.gridphysics.simpleGame_teleport\"\n",
      " \t# gameFilename = \"examples.gridphysics.simpleGame_many_poisons_huge\"\n",
      " \tgameFilename = \"examples.gridphysics.movers2b\"\n",
      "-\tprint \"\"\n",
      "-\tprint \"initializing AStar on game\", gameFilename\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"initializing AStar on game\", gameFilename)\n",
      " \tgameString, levelString = defInputGame(gameFilename, randomize=True)\n",
      " \trleCreateFunc = lambda: createRLInputGame(gameFilename)\n",
      " \trle = rleCreateFunc()\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \tagent = AStar(rle, gameString, levelString)\n",
      " \tpath, actions = agent.search()\n",
      "-\tprint \"found path\"\n",
      "+\tprint(\"found path\")\n",
      " \n",
      " \tembed()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored qlearner_long.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: qlearner_long.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- qlearner_long.py\t(original)\n",
      "+++ qlearner_long.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict, sys\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict, sys\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,14 +14,14 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "-from planner import *\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      "+from .planner import *\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -61,14 +61,14 @@\n",
      " \t\ttry:\n",
      " \t\t\timmovables = self.rle.immovables\n",
      " \t\t\tself.immovables = immovables\n",
      "-\t\t\tprint \"immovables\", immovables\n",
      "+\t\t\tprint(\"immovables\", immovables)\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall', 'poison']\n",
      " \t\t\tself.immovables = immovables\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self.rle._obstypes.keys():\n",
      "+\t\t\tif i in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -103,7 +103,7 @@\n",
      " \t\twhile len(rewardQueue)>0:\n",
      " \t\t\tloc = rewardQueue.popleft()\n",
      " \t\t\tif loc not in processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tprocessed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -112,8 +112,8 @@\n",
      " \t\treturn\n",
      " \n",
      " \tdef findObjectInRLE(self, rle, objName):\n",
      "-\t\tif objName not in rle._obstypes.keys():\n",
      "-\t\t\tprint objName, \"not in rle.\"\n",
      "+\t\tif objName not in list(rle._obstypes.keys()):\n",
      "+\t\t\tprint(objName, \"not in rle.\")\n",
      " \t\t\treturn None\n",
      " \t\tobjCode = 2**(1+sorted(self.rle._obstypes.keys())[::-1].index(objName))\n",
      " \t\tobjLoc = np.where(np.reshape(self.rle._getSensors(), self.rle.outdim)==objCode)\n",
      "@@ -144,15 +144,15 @@\n",
      " \tdef findObjectInState(self, s, objName):\n",
      " \t\t##TODO: Finish last part of this function -- sometimes it can't access objloc[0][0], objloc[1][0]\n",
      " \t\tstate = np.reshape(np.fromstring(s,dtype=float), self.rle.outdim)\n",
      "-\t\tif objName not in rle._obstypes.keys():\n",
      "-\t\t\tprint objName, \"not in rle.\"\n",
      "+\t\tif objName not in list(rle._obstypes.keys()):\n",
      "+\t\t\tprint(objName, \"not in rle.\")\n",
      " \t\t\treturn None\n",
      " \t\tobjCode = 2**(1+sorted(self.rle._obstypes.keys())[::-1].index(objName))\n",
      " \t\tobjLoc = np.where(state==objCode)\n",
      " \t\ttry:\n",
      " \t\t\tobjLoc = objLoc[0][0], objLoc[1][0] #(y,x)\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"can't find objloc\"\n",
      "+\t\t\tprint(\"can't find objloc\")\n",
      " \t\t\tembed()\n",
      " \t\treturn objLoc\n",
      " \n",
      "@@ -167,7 +167,7 @@\n",
      " \t\telif policy == 'greedy':\n",
      " \t\t\tbestQVal, bestA, QValsAreAllEqual = self.bestSA(s, partitionWeights = [1,0], domainKnowledge = True)\n",
      " \t\t\tif printout:\n",
      "-\t\t\t\tprint bestQVal\n",
      "+\t\t\t\tprint(bestQVal)\n",
      " \t\t\tif QValsAreAllEqual:\n",
      " \t\t\t\treturn None\n",
      " \t\t\telse:\n",
      "@@ -182,9 +182,9 @@\n",
      " \t\t\tnextLoc = currentLoc[0]+a[1], currentLoc[1]+a[0] #again, locations are (y,x) and actions are (x,y)\n",
      " \t\telse:\n",
      " \t\t\treturn 0.\n",
      "-\t\tif nextLoc in self.rewardDict.keys():\n",
      "+\t\tif nextLoc in list(self.rewardDict.keys()):\n",
      " \t\t\treturn self.rewardDict[nextLoc]\n",
      "-\t\telif currentLoc in self.rewardDict.keys():\n",
      "+\t\telif currentLoc in list(self.rewardDict.keys()):\n",
      " \t\t\treturn self.rewardDict[currentLoc]\n",
      " \t\telse:\n",
      " \t\t\treturn 0.\n",
      "@@ -211,10 +211,10 @@\n",
      " \t\tbestAction = None\n",
      " \t\tQValsAreAllEqual = False\n",
      " \t\tif debug:\n",
      "-\t\t\tprint \"debugging bestSA\"\n",
      "+\t\t\tprint(\"debugging bestSA\")\n",
      " \t\t\tembed()\n",
      " \t\tfor a in actions:\n",
      "-\t\t\tif (s,a) not in self.QVals.keys():\n",
      "+\t\t\tif (s,a) not in list(self.QVals.keys()):\n",
      " \t\t\t\tself.QVals[(s,a)] = 0.\n",
      " \t\t\tsumQVal += abs(self.QVals[(s,a)])\n",
      " \t\t\tsumPseudoReward += self.getPseudoReward(s, a)\n",
      "@@ -245,8 +245,8 @@\n",
      " \t\t\t\tbestQVal = self.QVals[(s,a)]\n",
      " \t\t\t\tQValsAreAllEqual = True\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"actions array is empty. in bestSA\"\n",
      "-\t\t\t\tprint np.reshape(np.fromstring(s,dtype=float),self.rle.outdim)\n",
      "+\t\t\t\tprint(\"actions array is empty. in bestSA\")\n",
      "+\t\t\t\tprint(np.reshape(np.fromstring(s,dtype=float),self.rle.outdim))\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \t\t# QVals = [self.QVals[(s,a)] for a in actions]\n",
      "@@ -264,7 +264,7 @@\n",
      " \t\ttry:\n",
      " \t\t\tself.QVals[(s,a)] = self.QVals[(s,a)] + self.alpha * (r + self.gamma*bestQVal - self.QVals[(s,a)])\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"didn't find qvals[s,a]\"\n",
      "+\t\t\tprint(\"didn't find qvals[s,a]\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \tdef runEpisode(self):\n",
      "@@ -313,7 +313,7 @@\n",
      " \t\t\tif i%10==0:\n",
      " \t\t\t\ts = self.rle._getSensors().tostring()\n",
      " \t\t\t\ta = self.selectAction(s, policy='greedy', partitionWeights = self.partitionWeights)\n",
      "-\t\t\t\tprint i, self.QVals[(s,a)]\n",
      "+\t\t\t\tprint(i, self.QVals[(s,a)])\n",
      " \t\t\t\tif satisfice: ## see if values have propagated to start state; if so, return.\n",
      " \t\t\t\t\tactions = self.getBestActionsForPlayout()\n",
      " \t\t\t\t\tif len(actions)>0:\n",
      "@@ -343,19 +343,19 @@\n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tres = rle.step(a)\n",
      " \t\t\tif showActions:\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\tterminal = rle._isDone()[0]\n",
      " \t\t\ts = res['observation'].tostring()\n",
      " \t\treturn actions\n",
      " \n",
      " \tdef backwardsPlayback(self):\n",
      "-\t\tlst = [(k,v) for k,v in self.QVals.iteritems()]\n",
      "+\t\tlst = [(k,v) for k,v in self.QVals.items()]\n",
      " \t\tslist = sorted(lst, key=lambda x:x[1])\n",
      " \t\tslist.reverse()\n",
      " \t\tfor l in slist:\n",
      " \t\t\tif l[1]>0:\n",
      "-\t\t\t\tprint np.reshape(np.fromstring(l[0][0],dtype=float),self.rle.outdim)\n",
      "-\t\t\t\tprint l[1]\n",
      "+\t\t\t\tprint(np.reshape(np.fromstring(l[0][0],dtype=float),self.rle.outdim))\n",
      "+\t\t\t\tprint(l[1])\n",
      " \n",
      " if __name__ == \"__main__\":\n",
      " \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored ontology.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: ontology.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ontology.py\t(original)\n",
      "+++ ontology.py\t(refactored)\n",
      "@@ -6,16 +6,16 @@\n",
      " import random\n",
      " from random import choice\n",
      " from copy import deepcopy\n",
      "-from colors import *\n",
      "+from .colors import *\n",
      " import itertools\n",
      " from math import sqrt\n",
      " import pygame\n",
      " import numpy as np\n",
      " import scipy.stats\n",
      "-from tools import triPoints, unitVector, vectNorm, oncePerStep\n",
      "-from ai import AStarWorld\n",
      "+from .tools import triPoints, unitVector, vectNorm, oncePerStep\n",
      "+from .ai import AStarWorld\n",
      " from IPython import embed\n",
      "-import core\n",
      "+from . import core\n",
      " import copy\n",
      " # import ipdb\n",
      " import time\n",
      "@@ -162,7 +162,7 @@\n",
      " \n",
      " \n",
      "     def activeMovement(self, sprite, action, speed=None):\n",
      "-        print self.gridsize\n",
      "+        print(self.gridsize)\n",
      "         \"\"\" Here the assumption is that the controls determine the direction of\n",
      "         acceleration of the sprite. \"\"\"\n",
      "         if speed is None:\n",
      "@@ -528,7 +528,7 @@\n",
      "         # Will not update AStarChaser if there is nothing to chase\n",
      "         killed = [s.name for s in game.kill_list]\n",
      "         if 'avatar' in killed:\n",
      "-            print \"avatar is dead\"\n",
      "+            print(\"avatar is dead\")\n",
      "             return\n",
      " \n",
      "         if game.time % 5 == 0:\n",
      "@@ -537,24 +537,24 @@\n",
      "         # print 'in astar', [world.get_sprite_tile_position(p.sprite) for p in path]\n",
      "         # Uncomment below to draw debug paths.\n",
      "         # self._setDebugVariables(world,path)\n",
      "-        print 'updating'\n",
      "-        print len(self.path)\n",
      "+        print('updating')\n",
      "+        print(len(self.path))\n",
      "         if self.path:\n",
      "             # n = min(5, len(self.path)-1)\n",
      " \n",
      " \n",
      "             if self.next_move == None:\n",
      "-                print 'popping off next path node'\n",
      "+                print('popping off next path node')\n",
      "                 # self.path.pop(0)\n",
      "                 self.next_move = self.path.pop(0)\n",
      "-                print self.next_move.sprite.rect, self.rect\n",
      "+                print(self.next_move.sprite.rect, self.rect)\n",
      " \n",
      "             next_x, next_y = self.next_move.sprite.rect.x, self.next_move.sprite.rect.y\n",
      "             self_x, self_y = self.rect.x, self.rect.y\n",
      " \n",
      " \n",
      "-            print next_x, next_y\n",
      "-            print self_x, self_y\n",
      "+            print(next_x, next_y)\n",
      "+            print(self_x, self_y)\n",
      " \n",
      "             dx = abs(next_x - self_x)\n",
      "             dy = abs(next_y - self_y)\n",
      "@@ -567,7 +567,7 @@\n",
      "             if dx < error and dy < error:\n",
      "                 self.last_move = self.next_move\n",
      "                 self.next_move = None\n",
      "-            print dx, dy, movement\n",
      "+            print(dx, dy, movement)\n",
      " \n",
      "             self.physics.activeMovement(self, movement)\n",
      " \n",
      "@@ -1102,7 +1102,7 @@\n",
      "         self.limit = limit\n",
      "         self.win = win\n",
      "         self.bonus = bonus\n",
      "-        self.stypes = kwargs.values()\n",
      "+        self.stypes = list(kwargs.values())\n",
      "         self.name = 'MultiSpriteCounter'\n",
      " \n",
      "     def isDone(self, game):\n",
      "@@ -1612,7 +1612,7 @@\n",
      "             r = sprite.resourceType\n",
      "             partner.resources[r] = max(-1, min(partner.resources[r]+sprite.value, game.resources_limits[r]))\n",
      "         else:\n",
      "-            r = partner.resources.keys()[0]\n",
      "+            r = list(partner.resources.keys())[0]\n",
      "             value=1\n",
      "             partner.resources[r] = max(-1, min(partner.resources[r]+value, game.resources_limits[r]))\n",
      "     else:\n",
      "@@ -1623,7 +1623,7 @@\n",
      " \n",
      "     killSprite(sprite, partner, game)\n",
      "     args = {'resource':sprite.name, 'value':value, 'limit':game.resources_limits[sprite.name]}\n",
      "-    print \"collectResource\", partner.resources\n",
      "+    print(\"collectResource\", partner.resources)\n",
      "     #print 'Collected ', colorDict[str(sprite.color)]#partner.resources[r]\n",
      "     # return ('collectResource', colorDict[str(partner.color)], colorDict[str(sprite.color)])\n",
      "     return ('collectResource' , sprite.ID, partner.ID, args)\n",
      "@@ -1703,7 +1703,7 @@\n",
      "     try:\n",
      "         sprite._updatePos(v, partner.speed * sprite.physics.gridsize[0])\n",
      "     except:\n",
      "-        print \"problem in pullwithit\"\n",
      "+        print(\"problem in pullwithit\")\n",
      "         embed()\n",
      "     if isinstance(sprite.physics, ContinuousPhysics):\n",
      "         sprite.speed = partner.speed\n",
      "@@ -1901,7 +1901,7 @@\n",
      "     t1 = time.time()\n",
      "     if targetColor not in game.targetColorDict:\n",
      "         try:\n",
      "-            targetName = [k for k in game.sprite_groups.keys() if game.sprite_groups[k] and game.sprite_groups[k][0].colorName==targetColor][0]\n",
      "+            targetName = [k for k in list(game.sprite_groups.keys()) if game.sprite_groups[k] and game.sprite_groups[k][0].colorName==targetColor][0]\n",
      "             targets = [s for s in game.sprite_groups[targetName] if s not in game.kill_list]\n",
      "             # print \"target name: {}. target length: {}\".format(targetName, len(targets))\n",
      "         except:\n",
      "@@ -1965,7 +1965,7 @@\n",
      "                 # if left!=left1 or top!=top1:\n",
      "                     # print \"got different positions\"\n",
      "                     # embed()\n",
      "-                if (left, top) in position_options.keys():\n",
      "+                if (left, top) in list(position_options.keys()):\n",
      "                     position_options[(left, top)] += 1.0/len(options)\n",
      "                 else:\n",
      "                     position_options[(left, top)] = 1.0/len(options)\n",
      "@@ -1992,7 +1992,7 @@\n",
      "         for option in BASEDIRS:\n",
      "             # left, top = calculateSpriteMove(game, current_sprite, speed, option)\n",
      "             left, top = current_sprite.physics.calculateActiveMovement(current_sprite, option, speed=speed)\n",
      "-            if (left, top) in position_options.keys():\n",
      "+            if (left, top) in list(position_options.keys()):\n",
      "                 position_options[(left, top)] += 1.0/len(BASEDIRS)\n",
      "             else:\n",
      "                 position_options[(left, top)] = 1.0/len(BASEDIRS)\n",
      "@@ -2133,7 +2133,7 @@\n",
      "     'sprite' is an object ID\n",
      "     \"\"\"\n",
      "     objectColors = set()\n",
      "-    for k in game.sprite_constr.keys():\n",
      "+    for k in list(game.sprite_constr.keys()):\n",
      "         try:\n",
      "             if game.sprite_constr[k][1]['color'] not in ['BLACK', 'DARKGRAY', 'MPUYEI', 'NUPHKK', 'SCJPNE']:\n",
      "                 objectColors.add(colorDict[str(game.sprite_constr[k][1]['color'])])\n",
      "@@ -2141,14 +2141,14 @@\n",
      "             continue\n",
      "     objectColors = list(objectColors)\n",
      "     if 'DTIZDF' in objectColors:\n",
      "-        print \"found DTIZDF\"\n",
      "+        print(\"found DTIZDF\")\n",
      "         embed()\n",
      "     game.spriteDistribution[sprite] = initializeDistribution(sprite_types, objectColors) # Indexed by object ID\n",
      "     # game.object_token_spriteDistribution[sprite] = initializeDistribution(sprite_types, objectColors) # Indexed by object ID\n",
      "-    if sprite not in game.all_objects.keys():\n",
      "+    if sprite not in list(game.all_objects.keys()):\n",
      "         game.all_objects[sprite] = game.getObjects()[sprite]\n",
      " \n",
      "-    game.movement_options[sprite] = {k:{} for k in game.spriteDistribution[sprite].keys()}\n",
      "+    game.movement_options[sprite] = {k:{} for k in list(game.spriteDistribution[sprite].keys())}\n",
      "     # game.object_token_movement_options[sprite] = {k:{} for k in game.spriteDistribution[sprite].keys()}\n",
      " \n",
      " \n",
      "@@ -2198,9 +2198,9 @@\n",
      "         # embed()\n",
      " \n",
      " \n",
      "-    if sprite in curr_distribution.keys():\n",
      "-        for param_combination in curr_distribution[sprite].keys():\n",
      "-            if outcome in movement_options[sprite][param_combination].keys():\n",
      "+    if sprite in list(curr_distribution.keys()):\n",
      "+        for param_combination in list(curr_distribution[sprite].keys()):\n",
      "+            if outcome in list(movement_options[sprite][param_combination].keys()):\n",
      "                 if missileOrientationClustering and 'Missile' in str(param_combination[0][1]):\n",
      "                     normalization_ratio += curr_distribution[sprite][param_combination] * (movement_options[sprite][param_combination][outcome]**alpha)\n",
      "                 else:\n",
      "@@ -2208,9 +2208,9 @@\n",
      "             else:\n",
      "                 normalization_ratio += curr_distribution[sprite][param_combination] * epsilon_prob\n",
      " \n",
      "-    if sprite in curr_distribution.keys():\n",
      "-        for param_combination in curr_distribution[sprite].keys():\n",
      "-            if outcome in movement_options[sprite][param_combination].keys():\n",
      "+    if sprite in list(curr_distribution.keys()):\n",
      "+        for param_combination in list(curr_distribution[sprite].keys()):\n",
      "+            if outcome in list(movement_options[sprite][param_combination].keys()):\n",
      "                 if missileOrientationClustering and 'Missile' in str(param_combination[0][1]):\n",
      "                     curr_distribution[sprite][param_combination] *= ((movement_options[sprite][param_combination][outcome]**alpha) / normalization_ratio)\n",
      "                 else:\n",
      "@@ -2347,8 +2347,8 @@\n",
      " \n",
      "     import random\n",
      "     import numpy as np\n",
      "-    from class_theory_template import Sprite\n",
      "-    from ontology import ResourcePack\n",
      "+    from .class_theory_template import Sprite\n",
      "+    from .ontology import ResourcePack\n",
      " \n",
      "     distributionsHaveChanged = False\n",
      " \n",
      "@@ -2358,7 +2358,7 @@\n",
      "     ##remove avatar. For now let's just assume we know which one it is.\n",
      "     ##TODO: You need to do avatarInduction, unless there's a generic type that can cover all types.\n",
      "     non_avatar_keys = []\n",
      "-    for k in all_objects.keys():\n",
      "+    for k in list(all_objects.keys()):\n",
      "         if all_objects[k]['sprite'].name != 'avatar':\n",
      "             non_avatar_keys.append(k)\n",
      "         else:\n",
      "@@ -2384,7 +2384,7 @@\n",
      "                     exceptions.append(ao_color)\n",
      " \n",
      "                 except AttributeError:\n",
      "-                    print \"tried and failed to add a shooting avatar type\"\n",
      "+                    print(\"tried and failed to add a shooting avatar type\")\n",
      "                     # embed()\n",
      "                     # No args in avatar\n",
      "                     sample.append(Sprite(vgdlType=MovingAvatar, color=all_objects[k]['type']['color']))\n",
      "@@ -2437,23 +2437,23 @@\n",
      "         ## Integrate evidence across all episodes; pick best hypothesis.\n",
      " \n",
      "         numDict = defaultdict(lambda:[])\n",
      "-        for k in bestSpriteTypeDict[obj_type].keys():\n",
      "+        for k in list(bestSpriteTypeDict[obj_type].keys()):\n",
      "             numDict[spriteUpdateDict[k]].append(k)\n",
      " \n",
      "         try:\n",
      "-            param_sum = {k:0. for k in bestSpriteTypeDict[obj_type].values()[0].keys()}\n",
      "+            param_sum = {k:0. for k in list(bestSpriteTypeDict[obj_type].values())[0].keys()}\n",
      "         except IndexError:\n",
      "             # bestSpriteTypeDict has yet to be populated for this object type\n",
      "-            for k, v in game.getObjects().items():\n",
      "+            for k, v in list(game.getObjects().items()):\n",
      "                 if v['features']['color'] == obj_type:\n",
      "                     bestSpriteTypeDict[obj_type][k] = game.spriteDistribution[k]\n",
      "-            param_sum = {k:1. for k in bestSpriteTypeDict[obj_type].values()[0].keys()}\n",
      "+            param_sum = {k:1. for k in list(bestSpriteTypeDict[obj_type].values())[0].keys()}\n",
      " \n",
      "         param_z = 0\n",
      " \n",
      "-        for num, IDs in numDict.items():\n",
      "+        for num, IDs in list(numDict.items()):\n",
      "             # param_product = {k:1. for k in bestSpriteTypeDict[obj_type].values()[0].keys()}\n",
      "-            for param in param_sum.keys():\n",
      "+            for param in list(param_sum.keys()):\n",
      "                 # param_sum[param] += num*np.prod([bestSpriteTypeDict[obj_type][ID][param] for ID in IDs])/float(len(IDs))\n",
      "                 # param_z += num*np.prod([bestSpriteTypeDict[obj_type][ID][param] for ID in IDs])/float(len(IDs))\n",
      " \n",
      "@@ -2473,7 +2473,7 @@\n",
      "                         )\n",
      "                         tmp_prod *= bestSpriteTypeDict[obj_type][ID][randomnpc_param]\n",
      "                     else:\n",
      "-                        print \"problem in param_product\"\n",
      "+                        print(\"problem in param_product\")\n",
      "                         embed()\n",
      " \n",
      "                 param_sum[param] += num*tmp_prod\n",
      "@@ -2483,7 +2483,7 @@\n",
      "                 # param_z += param_product[param]\n",
      "         \n",
      "         if param_z != 0:\n",
      "-            for param,val in param_sum.items():\n",
      "+            for param,val in list(param_sum.items()):\n",
      "                 param_sum[param] /= param_z\n",
      " \n",
      " \n",
      "@@ -2616,40 +2616,40 @@\n",
      "                     if s.vgdlType!=matchingSprite.vgdlType:\n",
      "                         distributionsHaveChanged = True\n",
      "                         if display:\n",
      "-                            print (\"Distributions for {} have changed from sprite type {} to {}\".format(s.color, matchingSprite.vgdlType, s.vgdlType))\n",
      "+                            print((\"Distributions for {} have changed from sprite type {} to {}\".format(s.color, matchingSprite.vgdlType, s.vgdlType)))\n",
      "                     # If one of the args is None but not the other,\n",
      "                     # distributionsHaveChanged is true\n",
      "                     elif ((s.args==None and matchingSprite.args!=None) or\n",
      "                         (s.args!=None and matchingSprite.args==None)):\n",
      "                         distributionsHaveChanged = True\n",
      "                         if display:\n",
      "-                            print (\"Distribution args for {} have changed from {} to {}\".format(s.color, s.args, matchingSprite.args))\n",
      "+                            print((\"Distribution args for {} have changed from {} to {}\".format(s.color, s.args, matchingSprite.args)))\n",
      "                     elif (s.args and matchingSprite.args) != None:\n",
      "                         # If args are different, except for the case where only an\n",
      "                         # orientation is reversed (e.g. turnAround), then\n",
      "                         # distributionsHaveChanged is true\n",
      "-                        for key in s.args.keys() + matchingSprite.args.keys():\n",
      "+                        for key in list(s.args.keys()) + list(matchingSprite.args.keys()):\n",
      "                             try:\n",
      "                                 if not ((s.args[key] and matchingSprite.args[key])\n",
      "                                     in ([LEFT, RIGHT] or [UP, DOWN])):\n",
      "                                     if s.args[key] != matchingSprite.args[key]:\n",
      "                                         distributionsHaveChanged = True\n",
      "                                     if display:\n",
      "-                                        print (\"Distribution args for {} have changed from {} to {}\".format(s.color, s.args, matchingSprite.args))\n",
      "+                                        print((\"Distribution args for {} have changed from {} to {}\".format(s.color, s.args, matchingSprite.args)))\n",
      "                             except KeyError:\n",
      "                                 # If the new sprite has an arg that the old one\n",
      "                                 # doesn't, or vice-versa, then\n",
      "                                 # distributionsHaveChanged is true\n",
      "                                 distributionsHaveChanged = True\n",
      "                                 if display:\n",
      "-                                    print (\"Distribution args for {} have changed from {} to {}\".format(s.color, s.args, matchingSprite.args))\n",
      "+                                    print((\"Distribution args for {} have changed from {} to {}\".format(s.color, s.args, matchingSprite.args)))\n",
      " \n",
      "                 else:\n",
      "                     # print s.color, oldSpriteSet\n",
      "                     # embed()\n",
      "                     distributionsHaveChanged = True\n",
      "         except:\n",
      "-            print \"failed to find matching object in sampleFromDistribution\"\n",
      "+            print(\"failed to find matching object in sampleFromDistribution\")\n",
      "             embed()\n",
      " \n",
      "         # param = dict(best_param[1:])\n",
      "@@ -2667,7 +2667,7 @@\n",
      " \n",
      "     ## We don't do sprite inference for the avatar and for Flak\n",
      "     non_avatar_keys = []\n",
      "-    for k in all_objects.keys():\n",
      "+    for k in list(all_objects.keys()):\n",
      "         if all_objects[k]['sprite'].name is not 'avatar':\n",
      "             non_avatar_keys.append(k)\n",
      "         # else:\n",
      "@@ -2678,7 +2678,7 @@\n",
      "                                                                                                     ## and not doing inference about it.\n",
      "     for obj_type in types:\n",
      "         ## find the most-updated object, use that one for the sprite hypothesis.\n",
      "-        options = [k for k in all_objects.keys() if all_objects[k]['type']['color'] == obj_type]\n",
      "+        options = [k for k in list(all_objects.keys()) if all_objects[k]['type']['color'] == obj_type]\n",
      "         k = max(options, key=lambda x:spriteUpdateDict[x])\n",
      " \n",
      "         oldDistribution = bestSpriteTypeDict[obj_type]['distribution']\n",
      "@@ -2688,8 +2688,8 @@\n",
      "             ## always alphabetize the keys\n",
      "             ## sample multinomially from the spriteDistribution[key] dictionary, to get the spriteType\n",
      "             ## add that to the color info for that object.\n",
      "-            if k not in curr_distribution.keys():\n",
      "-                print k, \"not in curr_distribution\"\n",
      "+            if k not in list(curr_distribution.keys()):\n",
      "+                print(k, \"not in curr_distribution\")\n",
      "                 embed()\n",
      "             sprite_possibilities = curr_distribution[k]\n",
      "         else:\n",
      "@@ -2700,7 +2700,7 @@\n",
      "     return False\n",
      " \n",
      " def getKL(spriteDistribution1, spriteDistribution2):\n",
      "-    d1, d2 = [v['prob'] for v in spriteDistribution1.values()], [v['prob'] for v in spriteDistribution2.values()]\n",
      "+    d1, d2 = [v['prob'] for v in list(spriteDistribution1.values())], [v['prob'] for v in list(spriteDistribution2.values())]\n",
      "     return scipy.stats.entropy(d1,d2)\n",
      " \n",
      " \n",
      "@@ -2761,13 +2761,13 @@\n",
      "         # print \"len spriteDistribution: {}, objects: {}\".format(len(game.spriteDistribution.keys()), len(objects.keys()))\n",
      "         # print \"len filtered keys: {}\".format(len([s for s in game.spriteDistribution.keys() if s not in kill_list_keys]))\n",
      " \n",
      "-        for sprite in [s for s in game.spriteDistribution.keys() if s in objects.keys()]:                  # Keys are the IDs of the game objects\n",
      "+        for sprite in [s for s in list(game.spriteDistribution.keys()) if s in list(objects.keys())]:                  # Keys are the IDs of the game objects\n",
      "             sprite_count +=1\n",
      "             sprite_obj = objects[sprite][\"sprite\"]\n",
      " \n",
      "             if sprite_obj.name !='avatar':\n",
      "                 # print \"Updating {} because it moved. Lastrect:{}, rect: {}\".format(sprite_obj.colorName, sprite_obj.lastrect, sprite_obj.rect)\n",
      "-                for param_combination in game.spriteDistribution[sprite].keys(): # Check each potential sprite type\n",
      "+                for param_combination in list(game.spriteDistribution[sprite].keys()): # Check each potential sprite type\n",
      "                     if game.spriteDistribution[sprite][param_combination]> 0:    # Make sure sprite_type is an option for sprite, and sprite is not killed\n",
      "                         param_count +=1\n",
      "                         # sprite_obj = objects[sprite][\"sprite\"]\n",
      "@@ -2810,7 +2810,7 @@\n",
      "         objects = game.getObjects()\n",
      "         # notUpdated = [s for s in objects.keys() if objects[s]['sprite'].colorName not in ['DARKGRAY', 'MPUYEI', 'NUPHKK', 'SCJPNE'] and s not in game.spriteDistribution.keys()]\n",
      "         # t1 = time.time()\n",
      "-        for sprite in [s for s in game.spriteDistribution.keys() if s in objects.keys() and s not in [k.ID for k in game.kill_list]]:        # Keys are the IDs of the game objects\n",
      "+        for sprite in [s for s in list(game.spriteDistribution.keys()) if s in list(objects.keys()) and s not in [k.ID for k in game.kill_list]]:        # Keys are the IDs of the game objects\n",
      "             sprite_obj = objects[sprite][\"sprite\"]\n",
      "             # sprite_count +=1\n",
      " \n",
      "@@ -2834,7 +2834,7 @@\n",
      "                 #     embed()\n",
      "         # print \"step 3 updated {} sprites and {} param combinations, took {} seconds.\".format(sprite_count, calls_to_update_distribution, t1-time.time())\n",
      "         ## Update the global memory\n",
      "-        for k in game.spriteDistribution.keys():\n",
      "+        for k in list(game.spriteDistribution.keys()):\n",
      "             try:\n",
      "                 color = game.all_objects[k]['type']['color']\n",
      "             except KeyError:\n",
      "@@ -2865,13 +2865,13 @@\n",
      "     safe_colors = [c for c in all_colors if c not in exclude_colors]\n",
      "     if method=='random_then_nearest':\n",
      "         if len(unknown_colors)>0 and random.random()>epsilon:\n",
      "-            print \"selecting an unknown color\"\n",
      "+            print(\"selecting an unknown color\")\n",
      "             object_color = random.choice(unknown_colors)\n",
      "         else:\n",
      "             # in case we've interacted with everything once but want to randomly try things again\n",
      "-            print \"sometimes with probability\", epsilon, \"we select randomly from all safe colors. This just happened.\"\n",
      "+            print(\"sometimes with probability\", epsilon, \"we select randomly from all safe colors. This just happened.\")\n",
      "             object_color = random.choice(safe_colors)\n",
      "-        choices = [item for sublist in rle._game.sprite_groups.values() for item in sublist if colorDict[str(item.color)]==object_color]\n",
      "+        choices = [item for sublist in list(rle._game.sprite_groups.values()) for item in sublist if colorDict[str(item.color)]==object_color]\n",
      "         avatar_loc = rle._rect2pos(rle._game.sprite_groups['avatar'][0].rect)\n",
      "         choices = [(dist(rle._rect2pos(c.rect), avatar_loc), c) for c in choices]\n",
      "         choices = sorted(choices, key=lambda c:c[0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored agent.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: agent.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- agent.py\t(original)\n",
      "+++ agent.py\t(refactored)\n",
      "@@ -1,12 +1,12 @@\n",
      "-from util import *\n",
      "-from core import colorDict, VGDLParser, makeVideo, sys\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, \\\n",
      "+from .util import *\n",
      "+from .core import colorDict, VGDLParser, makeVideo, sys\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, \\\n",
      " MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt, generateSymbolDict\n",
      "-from metaplanner import *\n",
      "+from .metaplanner import *\n",
      " import importlib\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " \n",
      " ## For now, only implementing version of agent that can deal with single goals.\n",
      "@@ -77,7 +77,7 @@\n",
      " \t\ttry:\n",
      " \t\t\treturn self.getSpriteColor(rle._game.sprite_groups[spriteName][0])\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"getSpriteNameColor failed\"\n",
      "+\t\t\tprint(\"getSpriteNameColor failed\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \tdef objectSelectionPhase(self, unknownColors, allColors, rle):\n",
      "@@ -86,25 +86,25 @@\n",
      " \t\tepsilon = .1\n",
      " \t\t## With probability 1-epsilon, select known goal if it's known, otherwise unkown object.\n",
      " \t\tif len(self.goalColor)>0 and \\\n",
      "-\t\tlen([k for k in rle._game.sprite_groups.keys() if len(rle._game.sprite_groups[k])>0 and \\\n",
      "+\t\tlen([k for k in list(rle._game.sprite_groups.keys()) if len(rle._game.sprite_groups[k])>0 and \\\n",
      " \t\t\tself.getSpriteNameColor(k, rle) in self.goalColor])>0 and \\\n",
      " \t\t\trandom.random()>epsilon:\n",
      "-\t\t\tkey = random.choice([k for k in rle._game.sprite_groups.keys() if len(rle._game.sprite_groups[k])>0\\\n",
      "+\t\t\tkey = random.choice([k for k in list(rle._game.sprite_groups.keys()) if len(rle._game.sprite_groups[k])>0\\\n",
      " \t\t\t and self.getSpriteNameColor(k, rle) in self.goalColor])\n",
      " \t\t\tobjectGoal = rle._game.sprite_groups[key][0]\n",
      " \t\t\t# actualGoal = objectGoal\n",
      " \t\t\t# objectGoalLocation = rle._rect2posFlipCoords(objectGoal.rect)\n",
      "-\t\t\tprint \"Selecting from known goals\", list(set(self.goalColor))\n",
      "-\t\t\tprint \"\"\n",
      "+\t\t\tprint(\"Selecting from known goals\", list(set(self.goalColor)))\n",
      "+\t\t\tprint(\"\")\n",
      " \t\telse:\n",
      " \t\t\ttry:\n",
      " \t\t\t\tobjectGoal = selectObjectGoal(rle, unknownColors, allColors, self.killerObjects, method=\"random_then_nearest\")\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tprint(\"\")\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"no unknown objects and no goal? Embedding so you can debug.\"\n",
      "+\t\t\t\tprint(\"no unknown objects and no goal? Embedding so you can debug.\")\n",
      " \t\t\t\tembed()\n",
      " \t\tobjectGoalLocation = rle._rect2posFlipCoords(objectGoal.rect)\n",
      "-\t\tprint \"object goal is\", self.getSpriteColor(objectGoal), \"at location\", objectGoalLocation\n",
      "+\t\tprint(\"object goal is\", self.getSpriteColor(objectGoal), \"at location\", objectGoalLocation)\n",
      " \t\treturn objectGoal, objectGoalLocation\n",
      " \n",
      " \tdef initializeVrle(self, hypothesis, objectGoalLocation, rle):\n",
      "@@ -112,7 +112,7 @@\n",
      " \t\tgameString, levelString, symbolDict, immovables, killerObjects = writeTheoryToTxt(rle, hypothesis, self.symbolDict,\\\n",
      " \t\t\t\t \"./examples/gridphysics/theorytest.py\", objectGoalLocation)\n",
      " \n",
      "-\t\tprint \"Initializing mental theory *with* object goal\"\n",
      "+\t\tprint(\"Initializing mental theory *with* object goal\")\n",
      " \t\tVrle = createMindEnv(gameString, levelString, output=True)\n",
      " \t\tVrle.immovables, Vrle.killerObjects = immovables, killerObjects\n",
      " \t\treturn Vrle\n",
      "@@ -120,7 +120,7 @@\n",
      " \tdef VrleInitPhase(self, objectGoalLocation, rle):\n",
      " \t\t## Initialize multiple VRLEs, each corresponding to one hypothesis in self.hypotheses\n",
      " \t\tVRLEs = []\n",
      "-\t\tprint \"in VrleInitPhase.\", len(self.hypotheses), \"hypotheses\"\n",
      "+\t\tprint(\"in VrleInitPhase.\", len(self.hypotheses), \"hypotheses\")\n",
      " \t\tfor hypothesis in self.hypotheses:\n",
      " \t\t\tVRLEs.append(self.initializeVrle(hypothesis, objectGoalLocation, rle))\n",
      " \t\treturn VRLEs\n",
      "@@ -132,17 +132,17 @@\n",
      " \n",
      " \t\tgameObject = None\n",
      " \t\tfor i in range(numEpisodes):\n",
      "-\t\t\tprint \"Starting episode\", i\n",
      "+\t\t\tprint(\"Starting episode\", i)\n",
      " \t\t\tself.pickNewLevel(index=i)\n",
      " \t\t\tgameObject, won, eventList, statesEncountered = self.playEpisode(gameObject, finalEventList)\n",
      " \t\t\tfinalEventList.extend(eventList)\n",
      " \t\t\tVGDLParser.playGame(self.gameString, self.levelString, statesEncountered, persist_movie=True, make_images=True, make_movie=False, movie_dir=\"videos/\"+self.gameName, padding=10)\n",
      " \t\t\ttotalStatesEncountered.append(statesEncountered)\n",
      " \t\t\ttally.append(won)\n",
      "-\t\t\tprint \"Episode ended. Won:\", won\n",
      "-\t\t\tprint \"Have won\", sum(tally), \"out of\", len(tally), \"episodes\"\n",
      "+\t\t\tprint(\"Episode ended. Won:\", won)\n",
      "+\t\t\tprint(\"Have won\", sum(tally), \"out of\", len(tally), \"episodes\")\n",
      " \t\t\n",
      "-\t\tprint \"Won\", sum(tally), \"out of \", len(tally), \"episodes.\"\n",
      "+\t\tprint(\"Won\", sum(tally), \"out of \", len(tally), \"episodes.\")\n",
      " \t\tmakeVideo(movie_dir=\"videos/\"+self.gameName)\n",
      " \t\t# empty image directory\n",
      " \t\t# shutil.rmtree(\"images/tmp\")\n",
      "@@ -159,12 +159,12 @@\n",
      " \t\t# embed()\n",
      " \t\t# allColors = [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in rle._game.sprite_groups.keys()]\n",
      " \t\t##select only non-moving objects as goals. Avoids chasing, which takes forever at the moment.\n",
      "-\t\tallColors = [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in rle._game.sprite_groups.keys() if len(rle._game.getSprites(k))>0 and rle._game.sprite_groups[k][0].speed is None ]\n",
      "+\t\tallColors = [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in list(rle._game.sprite_groups.keys()) if len(rle._game.getSprites(k))>0 and rle._game.sprite_groups[k][0].speed is None ]\n",
      " \t\tallColors = [c for c in allColors if c!='DARKBLUE']\n",
      " \t\tunknownColors = [c for c in allColors if c not in self.knownColors]\n",
      " \n",
      "-\t\tprint \"unknown colors:\", unknownColors\n",
      "-\t\tprint \"Known colors:\", self.knownColors\n",
      "+\t\tprint(\"unknown colors:\", unknownColors)\n",
      "+\t\tprint(\"Known colors:\", self.knownColors)\n",
      " \n",
      " \t\tended, won = rle._isDone()\n",
      " \t\t## Start storing encountered states.\n",
      "@@ -174,10 +174,10 @@\n",
      " \t\t## initialize theory if necessary.\n",
      " \t\tif len(self.hypotheses) == 0:\n",
      " \t\t\tgameObject = self.initializeHypotheses(rle, allObjects, learnSprites=True)\n",
      "-\t\t\tprint \"initializing hypotheses\"\n",
      "+\t\t\tprint(\"initializing hypotheses\")\n",
      " \t\telse:\n",
      " \t\t\tgameObject = self.completeHypotheses(rle, allObjects)\n",
      "-\t\t\tprint \"had hypotheses -- completing them.\"\n",
      "+\t\t\tprint(\"had hypotheses -- completing them.\")\n",
      " \n",
      " \t\twhile not ended:\n",
      " \n",
      "@@ -192,7 +192,7 @@\n",
      " \t\t\t\t## VRLEs, hypothesis-selection-method .....\n",
      " \t\t\t\t## figures out plan determined as above\n",
      " \t\t\t\t## carries out plan.\n",
      "-\t\t\tprint \"calling getToObjecGoal\"\n",
      "+\t\t\tprint(\"calling getToObjecGoal\")\n",
      " \t\t\trle, self.hypotheses, finalEventList, candidateNewColors, statesEncountered, gameObject = \\\n",
      " \t\t\t\tgetToObjectGoal(rle, VRLEs[0], self.plannerType, gameObject, self.hypotheses[0], self.gameString, self.levelString, \\\n",
      " \t\t\t\t\tobjectGoal, allObjects, finalEventList, symbolDict=self.symbolDict)\n",
      "@@ -211,10 +211,10 @@\n",
      " \t\t\tfor col in candidateNewColors:\n",
      " \t\t\t\tif col not in self.knownColors:\n",
      " \t\t\t\t\tself.knownColors.append(col)\n",
      "-\t\t\t\t\tprint \"added\", col, \"to knownColors\"\n",
      "-\t\t\tprint \"updated known colors\", self.knownColors\n",
      "+\t\t\t\t\tprint(\"added\", col, \"to knownColors\")\n",
      "+\t\t\tprint(\"updated known colors\", self.knownColors)\n",
      " \t\t\tunknownColors = [c for c in unknownColors if c not in self.knownColors]\n",
      "-\t\t\tprint \"updated unknownColors\", unknownColors\n",
      "+\t\t\tprint(\"updated unknownColors\", unknownColors)\n",
      " \t\treturn gameObject, won, finalEventList, totalStatesEncountered\n",
      " \n",
      " if __name__ == \"__main__\":\n",
      "@@ -251,10 +251,10 @@\n",
      " \tplannerType = \"IW\"\n",
      " \t# plannerType = \"QLearning\"\n",
      " \t# plannerType = \"AStar\"\n",
      "-\tprint \"\"\n",
      "-\tprint \"Playing {} with {}\".format(filename, plannerType)\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"Playing {} with {}\".format(filename, plannerType))\n",
      " \tagent = Agent(filename, plannerType)\n",
      " \tt1 = time.time()\n",
      " \tnumEpisodes = 5\n",
      " \tagent.playMultipleEpisodes(numEpisodes)\n",
      "-\tprint \"Ended {} episodes of {} with planner {} in {} seconds\".format(numEpisodes, filename, plannerType, time.time()-t1)\n",
      "+\tprint(\"Ended {} episodes of {} with planner {} in {} seconds\".format(numEpisodes, filename, plannerType, time.time()-t1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Can't parse core_comments.py: ParseError: bad input: type=1, value='left', context=('            ', (1013, 12))\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: There was 1 error:\n",
      "RefactoringTool: Can't parse core_comments.py: ParseError: bad input: type=1, value='left', context=('            ', (1013, 12))\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored make_videos.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: make_videos.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- make_videos.py\t(original)\n",
      "+++ make_videos.py\t(refactored)\n",
      "@@ -1,12 +1,12 @@\n",
      "-import cPickle\n",
      "+import pickle\n",
      " import os, subprocess, shutil\n",
      " from IPython import embed\n",
      "-from core import VGDLParser\n",
      "+from .core import VGDLParser\n",
      " \n",
      " def makeMovies(directory):\n",
      " \tfor pickleFile in [file for file in os.listdir(directory) if 'DS_Store' not in file]:\n",
      " \t\twith open(directory+'/'+pickleFile, 'r') as f:\n",
      "-\t\t\tgameData = cPickle.load(f)\n",
      "+\t\t\tgameData = pickle.load(f)\n",
      " \t\t\tgameName, gameString, levelString, episodes, param_ID = gameData['gameInfo']['gameName'], \\\n",
      " \t\t\t\t\tgameData['gameInfo']['gameString'], gameData['gameInfo']['levelString'], gameData['episodes'], gameData['modelParams']\n",
      " \t\t\tfor statesEncountered in episodes:\n",
      "@@ -19,11 +19,11 @@\n",
      "         persist_movie=True, make_images=True, make_movie=False, movie_dir=\"videos/\"+gameName, gameName = gameName, parameter_string=param_ID, padding=10)\n",
      " \n",
      " def makeMovie(param_ID, gameName, filename):\n",
      "-    print \"Creating Movie\"\n",
      "+    print(\"Creating Movie\")\n",
      "     movie_dir = \"videos/{}/{}\".format(param_ID, gameName)\n",
      " \n",
      "     if not os.path.exists(movie_dir):\n",
      "-        print movie_dir, \"didn't exist. making new dir\"\n",
      "+        print(movie_dir, \"didn't exist. making new dir\")\n",
      "         os.makedirs(movie_dir)\n",
      "     # round_index = len([d for d in os.listdir(movie_dir) if d != '.DS_Store'])\n",
      "     # video_dirname = movie_dir+\"/round\"+str(round_index)+\".mp4\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored stateobs.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: stateobs.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stateobs.py\t(original)\n",
      "+++ stateobs.py\t(refactored)\n",
      "@@ -9,9 +9,9 @@\n",
      " import pygame\n",
      " from pybrain.utilities import setAllArgs\n",
      " \n",
      "-from ontology import RotatingAvatar, BASEDIRS, GridPhysics, ShootAvatar, kill_effects\n",
      "-from core import Avatar\n",
      "-from tools import listRotate\n",
      "+from .ontology import RotatingAvatar, BASEDIRS, GridPhysics, ShootAvatar, kill_effects\n",
      "+from .core import Avatar\n",
      "+from .tools import listRotate\n",
      " \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored mcts_teleport.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: mcts_teleport.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mcts_teleport.py\t(original)\n",
      "+++ mcts_teleport.py\t(refactored)\n",
      "@@ -1,15 +1,15 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      " import math\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " from threading import Thread\n",
      " import time\n",
      " \n",
      "@@ -124,7 +124,7 @@\n",
      " \t\t\trle = self.rleCreateFunc(OBSERVATION_GLOBAL)\n",
      " \t\t\t# rle = q.get()\n",
      " \t\t\tif i%10==0:\n",
      "-\t\t\t\tprint \"Training cycle: %i\"%i\n",
      "+\t\t\t\tprint(\"Training cycle: %i\"%i)\n",
      " \n",
      " \t\t\t# rle = self.rleCreateFunc(self.obsType)\n",
      " \t\t\treward, vl, iters = self.treePolicy(self.root, rle, step_horizon)\n",
      "@@ -136,9 +136,9 @@\n",
      " \n",
      " \t\t# for worker in workers:\n",
      " \t\t# \tworker.join()\n",
      "-\t\tprint \"Tree policy iters:\", tree_policy_iters\n",
      "-\t\tprint \"Default policy iters:\", default_policy_iters\n",
      "-\t\tprint \"Total time: %f\"%(time.time()-oldTime)\n",
      "+\t\tprint(\"Tree policy iters:\", tree_policy_iters)\n",
      "+\t\tprint(\"Default policy iters:\", default_policy_iters)\n",
      "+\t\tprint(\"Total time: %f\"%(time.time()-oldTime))\n",
      " \n",
      " \n",
      " \tdef getBestActionsForPlayout(self):\n",
      "@@ -168,18 +168,18 @@\n",
      " \n",
      " \tdef debug(self):\n",
      " \t\tv = self.root\n",
      "-\t\tprint np.reshape(v.state, self.outdim)\n",
      "+\t\tprint(np.reshape(v.state, self.outdim))\n",
      " \t\tactions, nodes = [], []\n",
      " \t\twhile v and not v.terminal:\n",
      " \t\t\t# print v.children.iteritems()\n",
      "-\t\t\tprint [(k,c.qVal) for k,c in v.children.iteritems()]\n",
      "+\t\t\tprint([(k,c.qVal) for k,c in v.children.items()])\n",
      " \t\t\ta, v = self.bestChild(v,0)\n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tnodes.append(v)\n",
      " \t\t\tif v:\n",
      "-\t\t\t\tprint a\n",
      "-\t\t\t\tprint np.reshape(v.state, self.outdim)\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tprint(a)\n",
      "+\t\t\t\tprint(np.reshape(v.state, self.outdim))\n",
      "+\t\t\t\tprint(\"\")\n",
      " \n",
      " \t\treturn actions, nodes\n",
      " \n",
      "@@ -242,7 +242,7 @@\n",
      " \n",
      " \n",
      " \tdef expand(self, v, rle):\n",
      "-\t\tprint \"in expand\"\n",
      "+\t\tprint(\"in expand\")\n",
      " \t\t# embed()\n",
      " \t\texpan_action = None\n",
      " \t\tchild = None\n",
      "@@ -276,7 +276,7 @@\n",
      " \t\tmaxFuncVal = -float('inf')\n",
      " \t\tbestChild = None\n",
      " \t\tbestAction = None\n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tfuncVal = -float('inf')\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -332,9 +332,9 @@\n",
      " \t\t\tfor vec in vecDist:\n",
      " \t\t\t\tvecDist[vec] /= vecDistSum\n",
      " \n",
      "-\t\t\tsamples = np.random.multinomial(1, vecDist.values(), size=1)\n",
      "+\t\t\tsamples = np.random.multinomial(1, list(vecDist.values()), size=1)\n",
      " \t\t\tsample_index = np.nonzero(samples)[1][0]\n",
      "-\t\t\tsample = vecDist.keys()[sample_index]\n",
      "+\t\t\tsample = list(vecDist.keys())[sample_index]\n",
      " \t\t\t# print vecDist, samples, sample_index, sample\n",
      " \t\t\t# sample = np.random.choice(vecDist.keys(), 1, vecDist.values())[0]\n",
      " \t\t\ta = sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP_class.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP_class.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP_class.py\t(original)\n",
      "+++ WBP_class.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      "@@ -11,12 +11,12 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display=1)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.objIDs = {}\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      "@@ -25,7 +25,7 @@\n",
      " \t\tself.addWaitAction = True\n",
      " \t\tself.padding = 5  ##5 is arbitrary; just to make sure we don't get overlap when we add positions\n",
      " \t\ti=1\n",
      "-\t\tfor k in rle._game.all_objects.keys():\n",
      "+\t\tfor k in list(rle._game.all_objects.keys()):\n",
      " \t\t\tself.objIDs[k] = i * (rle.outdim[0]*rle.outdim[1]+self.padding)\n",
      " \t\t\ti+=1\n",
      " \t\tself.addSpaceBarToActions()\n",
      "@@ -34,7 +34,7 @@\n",
      " \t\t## Note: if an object that isn't instantiated in the beginning is of a class that \n",
      " \t\t## spacebar applies to, we won't pick up on it here.\n",
      " \t\tshootingClasses = ['MarioAvatar', 'ClimbingAvatar', 'ShootAvatar', 'Switch', 'FlakAvatar']\n",
      "-\t\tclasses = [str(o[0].__class__) for o in self.rle._game.sprite_groups.values() if len(o)>0]\n",
      "+\t\tclasses = [str(o[0].__class__) for o in list(self.rle._game.sprite_groups.values()) if len(o)>0]\n",
      " \t\tspacebarAvailable = False\n",
      " \t\tfor sc in shootingClasses:\n",
      " \t\t\tif any([sc in c for c in classes]):\n",
      "@@ -50,7 +50,7 @@\n",
      " \n",
      " \tdef calculateAtoms(self, rle):\n",
      " \t\tlst = []\n",
      "-\t\tfor k in rle._game.sprite_groups.keys():\n",
      "+\t\tfor k in list(rle._game.sprite_groups.keys()):\n",
      " \t\t\tfor o in rle._game.sprite_groups[k]:\n",
      " \t\t\t\tif o not in rle._game.kill_list:\n",
      " \t\t\t\t\t## turn location into vector posd2[ition (rows appended one after the other.)\n",
      "@@ -72,12 +72,12 @@\n",
      " \t\tlst.append(ind)\n",
      " \t\tif not self.vecSize:\n",
      " \t\t\tself.vecSize = len(lst)\n",
      "-\t\t\tprint \"Vector is length {}\".format(self.vecSize)\n",
      "+\t\t\tprint(\"Vector is length {}\".format(self.vecSize))\n",
      " \t\treturn set(lst)\n",
      " \t\n",
      " \tdef compareDicts(self, d1,d2):\n",
      " \t\t## only tells us what is in d2 that isn't in d1, as well as differences in values between shared keys\n",
      "-\t\treturn [k for k in d2.keys() if (k not in d1.keys() or d1[k]!=d2[k])]\n",
      "+\t\treturn [k for k in list(d2.keys()) if (k not in list(d1.keys()) or d1[k]!=d2[k])]\n",
      " \n",
      " \tdef delta(self, node1, node2):\n",
      " \t\tif node1 is None:\n",
      "@@ -146,7 +146,7 @@\n",
      " \t\t\t\t \telse:\n",
      " \t\t\t\t \t\treturn random.choice(bestNodes)\n",
      " \t\t\t\telse:\n",
      "-\t\t\t\t\tprint \"found 0 nodes in noveltyHeuristic\"\n",
      "+\t\t\t\t\tprint(\"found 0 nodes in noveltyHeuristic\")\n",
      " \t\t\t\t\tembed()\n",
      " \n",
      " \tdef rewardHeuristic(self, lst, WBP, k, surrogateCall=False):\n",
      "@@ -164,7 +164,7 @@\n",
      " \t\t \telse:\n",
      " \t\t \t\treturn random.choice(bestNodes)\n",
      " \t\telse:\n",
      "-\t\t\tprint \"found 0 nodes in rewardHeuristic\"\n",
      "+\t\t\tprint(\"found 0 nodes in rewardHeuristic\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \tdef BFS(self, rle):\n",
      "@@ -195,7 +195,7 @@\n",
      " \t\t\t\t\tQ.put(child)\n",
      " \t\t\telse:\n",
      " \t\t\t\trejected.append(current)\n",
      "-\t\tprint \"no more states in queue\"\n",
      "+\t\tprint(\"no more states in queue\")\n",
      " \t\tembed()\n",
      " \t\treturn Q, visited, rejected\n",
      " \n",
      "@@ -221,7 +221,7 @@\n",
      " \t\t\t# embed()\n",
      " \t\t\t## This is not nec. right.\n",
      " \t\t\tif current is None:\n",
      "-\t\t\t\tprint \"got no node\"\n",
      "+\t\t\t\tprint(\"got no node\")\n",
      " \t\t\t\tembed()\n",
      " \t\t\t\treturn Q, visited, rejected\n",
      " \t\t\telse:\n",
      "@@ -265,11 +265,11 @@\n",
      " \t\t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\texcept:\n",
      " \t\t\t\t# pass\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\ti=0\n",
      "@@ -278,9 +278,9 @@\n",
      " \t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\t\ti += 1\n",
      " \t\tif len(self.actionSeq)>0:\n",
      "-\t\t\tprint actionDict[self.actionSeq[-1]]\n",
      "+\t\t\tprint(actionDict[self.actionSeq[-1]])\n",
      " \t\tself.updateObjIDs(vrle)\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\t# if len([o for o in vrle._game.sprite_groups['bullet'] if o not in vrle._game.kill_list]) > 1:\n",
      " \t\t\t# print \"multiple bullets\"\n",
      " \t\t\t# embed()\n",
      "@@ -297,14 +297,14 @@\n",
      " \t\ti = 0\n",
      " \t\tfor objType in vrle._game.sprite_groups:\n",
      " \t\t\tfor s in vrle._game.sprite_groups[objType]:\n",
      "-\t\t\t\tif s.ID not in self.WBP.objIDs.keys():\n",
      "+\t\t\t\tif s.ID not in list(self.WBP.objIDs.keys()):\n",
      " \t\t\t\t\t# print \"in update IDs\"\n",
      " \t\t\t\t\t# embed()\n",
      " \t\t\t\t\tif s.name=='bullet':\n",
      " \t\t\t\t\t\ts.ID = len([o for o in vrle._game.sprite_groups[objType] if o not in vrle._game.kill_list])\n",
      " \t\t\t\t\telse:\n",
      " \t\t\t\t\t\ts.ID = len(vrle._game.sprite_groups[objType])\n",
      "-\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(self.WBP.objIDs.keys())+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      "+\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(list(self.WBP.objIDs.keys()))+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      " \t\t\t\t\ti+=1\n",
      " \t\t# print len(self.WBP.objIDs.keys())\n",
      " \t\t# print \"updated {} objects\".format(i)\n",
      "@@ -320,12 +320,12 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      "-\t\t\tprint actionDict[a]\n",
      "+\t\t\tprint(actionDict[a])\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\t# vrle.step((0,0))\n",
      " \t\t\t# print vrle.show()\n",
      " \t\t\t# embed()\n",
      "@@ -390,7 +390,7 @@\n",
      " \tlast, visited, rejected = p.BFS2(rle)\n",
      " \t# last, visited, rejected = BFS(rle, p)\n",
      " \t# last, visited, rejected = BFS2(rle, p)\n",
      "-\tprint time.time()-t1\n",
      "+\tprint(time.time()-t1)\n",
      " \t# print len(visited), len(rejected)\n",
      " \tembed()\n",
      " \t# if not hasattr(last, 'actionSeq'):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored olets_original.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: olets_original.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- olets_original.py\t(original)\n",
      "+++ olets_original.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,14 +14,14 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from qlearner import *\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .qlearner import *\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -56,7 +56,7 @@\n",
      " \tdef __init__(self, existing_rle=False, game = None, level = None, partitionWeights=[1,0,1],\\\n",
      " \t\t         rleCreateFunc=False, obsType = OBSERVATION_GLOBAL, decay_factor=.8, num_workers=1):\n",
      " \t\tif not existing_rle and not rleCreateFunc:\n",
      "-\t\t\tprint \"You must pass either an existing rle or an rleCreateFunc\"\n",
      "+\t\t\tprint(\"You must pass either an existing rle or an rleCreateFunc\")\n",
      " \t\t\treturn\n",
      " \t\t# assumption: not starting on terminal state\n",
      " \t\t\"\"\"\n",
      "@@ -113,7 +113,7 @@\n",
      " \n",
      " \t\tself.visitedLocations = defaultdict(lambda:0)\n",
      " \n",
      "-\t\tif 'avatar' in self._obstypes.keys():\n",
      "+\t\tif 'avatar' in list(self._obstypes.keys()):\n",
      " \t\t\tinverted_avatar_loc=self._obstypes['avatar'][0]\n",
      " \t\t\tavatar_loc = (inverted_avatar_loc[1], inverted_avatar_loc[0])\n",
      " \t\t\tself.avatar_code = np.reshape(self.rle._getSensors(), self.outdim)[avatar_loc[0]][avatar_loc[1]]\n",
      "@@ -151,10 +151,10 @@\n",
      " \t\t\t# print \"immovables\", immovables\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall', 'poison']\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self._obstypes.keys():\n",
      "+\t\t\tif i in list(self._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -187,7 +187,7 @@\n",
      " \t\twhile len(self.rewardQueue)>0:\n",
      " \t\t\tloc = self.rewardQueue.popleft()\n",
      " \t\t\tif loc not in self.processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tself.processed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -234,8 +234,8 @@\n",
      " \t\t\tVrle = copy.deepcopy(self.rle)\n",
      " \n",
      " \t\t\tif i%100==0 and len(rewards)>0:\n",
      "-\t\t\t\tprint \"Training cycle: %i\"%i\n",
      "-\t\t\t\tprint \"avg. rewards for last group of 100\", np.mean(rewards[-100:])\n",
      "+\t\t\t\tprint(\"Training cycle: %i\"%i)\n",
      "+\t\t\t\tprint(\"avg. rewards for last group of 100\", np.mean(rewards[-100:]))\n",
      " \n",
      " \t\t\treward, v, iters, terminal = self.runSimulation(self.root, Vrle, step_horizon)\n",
      " \n",
      "@@ -244,7 +244,7 @@\n",
      " \tdef getBestActionsForPlayout(self, partitionWeights, debug=False):\n",
      " \t\tv = self.root\n",
      " \t\tactions = []\n",
      "-\t\twhile len(v.children.keys())>0:\n",
      "+\t\twhile len(list(v.children.keys()))>0:\n",
      " \t\t\ta, v = self.pickAction(v)\n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tv = v.children[a]\n",
      "@@ -264,8 +264,8 @@\n",
      " \t\t\t\tbestAction = a\n",
      " \n",
      " \t\tif bestAction == None:\t## Tiebreaker\n",
      "-\t\t\tbestAction = random.choice(v.children.keys())\n",
      "-\t\tprint bestAction, bestVal\n",
      "+\t\t\tbestAction = random.choice(list(v.children.keys()))\n",
      "+\t\tprint(bestAction, bestVal)\n",
      " \t\treturn bestAction\n",
      " \n",
      " \tdef runSimulation(self, v, rle, step_horizon, solveSteps = None):\n",
      "@@ -278,9 +278,9 @@\n",
      " \t\t\tcount += 1\n",
      " \t\t\tavatarLoc = self.findAvatarInRLE(rle)\n",
      " \t\t\tself.visitedLocations[avatarLoc] +=1\n",
      "-\t\t\tif len(v.children.keys()) != len(self.actionDict[avatarLoc]):\n",
      "-\t\t\t\tprint \"not expanded\"\n",
      "-\t\t\t\tprint v.children.keys(), self.actionDict[avatarLoc]\n",
      "+\t\t\tif len(list(v.children.keys())) != len(self.actionDict[avatarLoc]):\n",
      "+\t\t\t\tprint(\"not expanded\")\n",
      "+\t\t\t\tprint(list(v.children.keys()), self.actionDict[avatarLoc])\n",
      " \t\t\t\treward, c = self.expand(v, rle, domain_knowledge=True)\n",
      " \t\t\t\treturn reward, c, iters, terminal\n",
      " \t\t\telse:\n",
      "@@ -291,13 +291,13 @@\n",
      " \t\t\t\treward = res['reward']\n",
      " \t\t\t\tavatarLoc = self.findAvatarInRLE(rle)\n",
      " \t\t\t\tself.visitedLocations[avatarLoc] += 1\n",
      "-\t\t\t\tprint rle.show()\n",
      "-\t\t\t\tif a not in v.children.keys():\n",
      "-\t\t\t\t\tprint a, \"not in children.keys\"\n",
      "+\t\t\t\tprint(rle.show())\n",
      "+\t\t\t\tif a not in list(v.children.keys()):\n",
      "+\t\t\t\t\tprint(a, \"not in children.keys\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\tv = v.children[a]\n",
      " \t\t\t\tterminal = rle._isDone()[0]\n",
      "-\t\tprint \"ended run\"\n",
      "+\t\tprint(\"ended run\")\n",
      " \t\t# embed()\t\t\n",
      " \t\tv.n_e += 1\n",
      " \t\tv.R_e += reward\n",
      "@@ -313,13 +313,13 @@\n",
      " \t\telse:\n",
      " \t\t\taction_choices = self.actions\n",
      " \n",
      "-\t\tprint \"in expand\", len(action_choices), len(v.children.keys())\n",
      "+\t\tprint(\"in expand\", len(action_choices), len(list(v.children.keys())))\n",
      " \t\tfor a in action_choices:\n",
      "-\t\t\tif a not in v.children.keys():\n",
      "+\t\t\tif a not in list(v.children.keys()):\n",
      " \t\t\t\texpand_action = a\n",
      " \t\t\t\tres = rle.step(a)\n",
      "-\t\t\t\tprint a\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(a)\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\t\tavatarLoc = self.findAvatarInRLE(rle)\n",
      " \t\t\t\tself.visitedLocations[avatarLoc] +=1\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      "@@ -343,7 +343,7 @@\n",
      " \t\t\t# embed()\n",
      " \t\t\tv.n_s += 1\n",
      " \t\t\ttry:\n",
      "-\t\t\t\tv.R_m = v.R_e/v.n_s + ((1-v.n_e)/v.n_s) * max([v.children[k].R_m for k in v.children.keys()])\n",
      "+\t\t\t\tv.R_m = v.R_e/v.n_s + ((1-v.n_e)/v.n_s) * max([v.children[k].R_m for k in list(v.children.keys())])\n",
      " \t\t\texcept:\n",
      " \t\t\t\tv.R_m = v.R_e/v.n_s + ((1-v.n_e)/v.n_s)\n",
      " \t\t\tv = v.parent\n",
      "@@ -380,7 +380,7 @@\n",
      " \t\tif action not in self.children:\n",
      " \t\t    self.children[action] = child\n",
      " \t\t    # if domain_knowledge:\n",
      "-\t\t    self.expanded = len(self.children.keys()) == len(self.tree.actionDict[avatar_loc])\n",
      "+\t\t    self.expanded = len(list(self.children.keys())) == len(self.tree.actionDict[avatar_loc])\n",
      " \t\t    # print\"creating Child\"\n",
      " \t\t    # print action\n",
      " \t\t    # print self.children.keys(), self.tree.actionDict[avatar_loc]\n",
      "@@ -388,7 +388,7 @@\n",
      " \t\t\t# else:\n",
      " \t\t\t\t# self.expanded = len(self.children.keys()) == len(self.actions)\n",
      " \t\telse:\n",
      "-\t\t\tprint \"createChild got called but with an existing action.\"\n",
      "+\t\t\tprint(\"createChild got called but with an existing action.\")\n",
      " \n",
      " if __name__ == \"__main__\":\n",
      " \t\n",
      "@@ -403,23 +403,23 @@\n",
      " \tgameString, levelString = defInputGame(gameFilename, randomize=True)\n",
      " \trleCreateFunc = lambda: createRLInputGame(gameFilename)\n",
      " \trle = rleCreateFunc()\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \t# rle.immovables = ['wall', 'poison1', 'poison2']\n",
      "-\tprint \"\"\n",
      "-\tprint \"Initializing learner. Playing\", gameFilename\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"Initializing learner. Playing\", gameFilename)\n",
      " \n",
      " \tagent = OLETS_agent(rle, gameString, levelString)\n",
      " \n",
      " \tt1 = time.time()\n",
      " \tagent.OLETS(numTrainingCycles=100, step_horizon=100)\n",
      " \tt2 = time.time() - t1\n",
      "-\tprint \"done in {} seconds\".format(t2)\n",
      "+\tprint(\"done in {} seconds\".format(t2))\n",
      " \n",
      " \tself=agent\n",
      " \tv=agent.root\n",
      " \tavatarLoc = self.findAvatarInRLE(rle)\n",
      " \tactions = self.actionDict[avatarLoc]\n",
      "-\tprint [(a, self.OLE(v, a, avatarLoc)) for a in actions]\n",
      "+\tprint([(a, self.OLE(v, a, avatarLoc)) for a in actions])\n",
      " \tembed()\n",
      " \n",
      " \tbestAction = agent.pickAction(agent.root, rle)\n",
      "@@ -427,8 +427,8 @@\n",
      " \tagent.rle = rle\n",
      " \tagent.root = agent.root.children[bestAction]\n",
      " \tagent.OLETS(numTrainingCycles=500, step_horizon=100)\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \tv=agent.root\n",
      " \tavatarLoc = self.findAvatarInRLE(rle)\n",
      " \tactions = self.actionDict[avatarLoc]\n",
      "-\tprint [(a, self.OLE(v, a, avatarLoc)) for a in actions]\n",
      "+\tprint([(a, self.OLE(v, a, avatarLoc)) for a in actions])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored sprite_induction.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: sprite_induction.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- sprite_induction.py\t(original)\n",
      "+++ sprite_induction.py\t(refactored)\n",
      "@@ -1,7 +1,7 @@\n",
      " \"\"\"\n",
      " Sprite Induction\n",
      " \"\"\"\n",
      "-from ontology import *\n",
      "+from .ontology import *\n",
      " \n",
      " \n",
      " # Create dictionary with transition updates: (TODO) should we do this manually, or can we do it automatically? \n",
      "@@ -16,7 +16,7 @@\n",
      " \t\"\"\"\n",
      " \tinitial_distribution = {}\n",
      " \tfor sprite in sprite_types:\n",
      "-\t\tinital_distribution[sprite] = 1.0/len(sprite_types.keys()) # uniform distribution\n",
      "+\t\tinital_distribution[sprite] = 1.0/len(list(sprite_types.keys())) # uniform distribution\n",
      " \treturn initial_distribution\n",
      " \n",
      " # Create function that takes in object last state and new state and updates the object distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP_stable.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP_stable.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP_stable.py\t(original)\n",
      "+++ WBP_stable.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      "@@ -11,12 +11,12 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display=1)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.objIDs = {}\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      "@@ -24,7 +24,7 @@\n",
      " \t\tself.addWaitAction = True\n",
      " \t\tself.padding = 5  ##5 is arbitrary; just to make sure we don't get overlap when we add positions\n",
      " \t\ti=1\n",
      "-\t\tfor k in rle._game.all_objects.keys():\n",
      "+\t\tfor k in list(rle._game.all_objects.keys()):\n",
      " \t\t\tself.objIDs[k] = i * (rle.outdim[0]*rle.outdim[1]+self.padding)\n",
      " \t\t\ti+=1\n",
      " \t\tself.addSpaceBarToActions()\n",
      "@@ -33,7 +33,7 @@\n",
      " \t\t## Note: if an object that isn't instantiated in the beginning is of a class that \n",
      " \t\t## spacebar applies to, we won't pick up on it here.\n",
      " \t\tshootingClasses = ['MarioAvatar', 'ClimbingAvatar', 'ShootAvatar', 'Switch', 'FlakAvatar']\n",
      "-\t\tclasses = [str(o[0].__class__) for o in self.rle._game.sprite_groups.values() if len(o)>0]\n",
      "+\t\tclasses = [str(o[0].__class__) for o in list(self.rle._game.sprite_groups.values()) if len(o)>0]\n",
      " \t\tspacebarAvailable = False\n",
      " \t\tfor sc in shootingClasses:\n",
      " \t\t\tif any([sc in c for c in classes]):\n",
      "@@ -49,7 +49,7 @@\n",
      " \n",
      " \tdef calculateAtoms(self, rle):\n",
      " \t\tlst = []\n",
      "-\t\tfor k in rle._game.sprite_groups.keys():\n",
      "+\t\tfor k in list(rle._game.sprite_groups.keys()):\n",
      " \t\t\tfor o in rle._game.sprite_groups[k]:\n",
      " \t\t\t\tif o not in rle._game.kill_list:\n",
      " \t\t\t\t\t## turn location into vector posd2[ition (rows appended one after the other.)\n",
      "@@ -71,12 +71,12 @@\n",
      " \t\tlst.append(ind)\n",
      " \t\tif not self.vecSize:\n",
      " \t\t\tself.vecSize = len(lst)\n",
      "-\t\t\tprint \"Vector is length {}\".format(self.vecSize)\n",
      "+\t\t\tprint(\"Vector is length {}\".format(self.vecSize))\n",
      " \t\treturn set(lst)\n",
      " \t\n",
      " \tdef compareDicts(self, d1,d2):\n",
      " \t\t## only tells us what is in d2 that isn't in d1, as well as differences in values between shared keys\n",
      "-\t\treturn [k for k in d2.keys() if (k not in d1.keys() or d1[k]!=d2[k])]\n",
      "+\t\treturn [k for k in list(d2.keys()) if (k not in list(d1.keys()) or d1[k]!=d2[k])]\n",
      " \n",
      " \tdef delta(self, node1, node2):\n",
      " \t\tif node1 is None:\n",
      "@@ -146,7 +146,7 @@\n",
      " \t\t\tbestNodes = [n for n in lst if n.novelty==minNovelty]\n",
      " \t \t\treturn random.choice(bestNodes)\n",
      " \t\telse:\n",
      "-\t\t\tprint \"found 0 nodes in noveltyHeuristic\"\n",
      "+\t\t\tprint(\"found 0 nodes in noveltyHeuristic\")\n",
      " \t\t\tembed()\n",
      " \n",
      " def rewardHeuristic(lst, WBP, k, surrogateCall=False):\n",
      "@@ -164,7 +164,7 @@\n",
      " \t \telse:\n",
      " \t \t\treturn random.choice(bestNodes)\n",
      " \telse:\n",
      "-\t\tprint \"found 0 nodes in rewardHeuristic\"\n",
      "+\t\tprint(\"found 0 nodes in rewardHeuristic\")\n",
      " \t\tembed()\n",
      " \n",
      " def BFS_noNovelty(rle, WBP):\n",
      "@@ -186,7 +186,7 @@\n",
      " \t\t\t\tQ.put(child)\n",
      " \t\telse:\n",
      " \t\t\trejected.append(current)\n",
      "-\tprint \"no more states in queue\"\n",
      "+\tprint(\"no more states in queue\")\n",
      " \tembed()\n",
      " \treturn Q, visited, rejected\n",
      " \n",
      "@@ -210,7 +210,7 @@\n",
      " \t\t\t\tQ.put(child)\n",
      " \t\telse:\n",
      " \t\t\trejected.append(current)\n",
      "-\tprint \"no more states in queue\"\n",
      "+\tprint(\"no more states in queue\")\n",
      " \tembed()\n",
      " \treturn Q, visited, rejected\n",
      " \n",
      "@@ -227,7 +227,7 @@\n",
      " \ti=0\n",
      " \n",
      " \tdef noveltySelection():\n",
      "-\t\tbestNodes = sorted(filter(lambda n: n.novelty>0, QNovelty), key=lambda n: (n.novelty, -n.metabolic_reward))\n",
      "+\t\tbestNodes = sorted([n for n in QNovelty if n.novelty>0], key=lambda n: (n.novelty, -n.metabolic_reward))\n",
      " \t\t# print [n.novelty for n in bestNodes]\n",
      " \t\t# embed()\n",
      " \t\tif len(bestNodes)>0:\n",
      "@@ -260,7 +260,7 @@\n",
      " \t\t\t# return random.choice([n for n in bestNodes if n.metabolic_reward==maxReward])\n",
      " \n",
      " \tdef rewardSelection():\n",
      "-\t\tbestNodes = sorted(filter(lambda n:n.novelty>0, QReward), key=lambda n: (-n.metabolic_reward, n.novelty))\n",
      "+\t\tbestNodes = sorted([n for n in QReward if n.novelty>0], key=lambda n: (-n.metabolic_reward, n.novelty))\n",
      " \t\t# print \"in rewardselection\"\n",
      " \t\t# print [n.reward for n in bestNodes]\n",
      " \t\t# embed()\n",
      "@@ -296,8 +296,8 @@\n",
      " \t\t\t\tQNovelty.remove(current)\t\t\n",
      " \t\t\telse:\n",
      " \t\t\t\tQReward.remove(current)\n",
      "-\t\t\tprint current.novelty\n",
      "-\t\t\tprint current.lastState.show()\n",
      "+\t\t\tprint(current.novelty)\n",
      "+\t\t\tprint(current.lastState.show())\n",
      " \t\t\t# QNovelty.remove(current)\n",
      " \t\t\t# QReward.remove(current)\n",
      " \t\t\tcurrent.eval(updateNoveltyDict=True)\n",
      "@@ -339,11 +339,11 @@\n",
      " \t\t# embed()\n",
      " \t\t## This is not nec. right.\n",
      " \t\tif current is None:\n",
      "-\t\t\tprint \"got no node\"\n",
      "+\t\t\tprint(\"got no node\")\n",
      " \t\t\tembed()\n",
      " \t\t\treturn Q, visited, rejected\n",
      " \t\telse:\n",
      "-\t\t\tprint current.lastState.show()\n",
      "+\t\t\tprint(current.lastState.show())\n",
      " \t\t\tQ.remove(current)\n",
      " \t\t\tcurrent.eval(updateNoveltyDict=True)\n",
      " \t\t\tvisited.append(current)\n",
      "@@ -383,11 +383,11 @@\n",
      " \t\t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\texcept:\n",
      " \t\t\t\t# pass\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\ti=0\n",
      "@@ -415,14 +415,14 @@\n",
      " \t\ti = 0\n",
      " \t\tfor objType in vrle._game.sprite_groups:\n",
      " \t\t\tfor s in vrle._game.sprite_groups[objType]:\n",
      "-\t\t\t\tif s.ID not in self.WBP.objIDs.keys():\n",
      "+\t\t\t\tif s.ID not in list(self.WBP.objIDs.keys()):\n",
      " \t\t\t\t\t# print \"in update IDs\"\n",
      " \t\t\t\t\t# embed()\n",
      " \t\t\t\t\tif s.name=='bullet':\n",
      " \t\t\t\t\t\ts.ID = len([o for o in vrle._game.sprite_groups[objType] if o not in vrle._game.kill_list])\n",
      " \t\t\t\t\telse:\n",
      " \t\t\t\t\t\ts.ID = len(vrle._game.sprite_groups[objType])\n",
      "-\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(self.WBP.objIDs.keys())+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      "+\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(list(self.WBP.objIDs.keys()))+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      " \t\t\t\t\ti+=1\n",
      " \t\t# print len(self.WBP.objIDs.keys())\n",
      " \t\t# print \"updated {} objects\".format(i)\n",
      "@@ -438,12 +438,12 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      "-\t\t\tprint actionDict[a]\n",
      "+\t\t\tprint(actionDict[a])\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\t# vrle.step((0,0))\n",
      " \t\t\t# print vrle.show()\n",
      " \t\t\t# embed()\n",
      "@@ -511,8 +511,8 @@\n",
      " \t# last, visited, rejected = BFS2(rle, p)\n",
      " \tlast, visited, rejected = BFS3(rle, p)\n",
      " \n",
      "-\tprint time.time()-t1\n",
      "-\tprint len(visited), len(rejected)\n",
      "+\tprint(time.time()-t1)\n",
      "+\tprint(len(visited), len(rejected))\n",
      " \tembed()\n",
      " \t# if not hasattr(last, 'actionSeq'):\n",
      " \t# \tprint \"Failed without tracking tokens. re-trying\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP3.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP3.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP3.py\t(original)\n",
      "+++ WBP3.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " ACTIONS = [(1,0), (-1,0), (0,1), (0,-1)]\n",
      "@@ -8,12 +8,12 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename, display):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      " \n",
      "@@ -87,7 +87,7 @@\n",
      " \t\tif n%2==1:\n",
      " \t\t\tdecomposition.append(0)\n",
      " \t\t\tn = n-1\n",
      "-\t\ti = len(rle._obstypes.keys())\n",
      "+\t\ti = len(list(rle._obstypes.keys()))\n",
      " \t\twhile i>0:\n",
      " \t\t\tif n>=2**i:\n",
      " \t\t\t\tdecomposition.append(i)\n",
      "@@ -113,7 +113,7 @@\n",
      " \t\treturn mergedList\n",
      " \n",
      " \tdef factorizeBoolean(self, rle, n):\n",
      "-\t\tlistLen = len(rle._obstypes.keys())+1\n",
      "+\t\tlistLen = len(list(rle._obstypes.keys()))+1\n",
      " \t\treturn self.indicesToBooleans(self.T, self.factorize(rle, n))\n",
      " \n",
      " \tdef findTrueTuples(self, node, k):\n",
      "@@ -250,12 +250,12 @@\n",
      " \t\t\t\t\tvrle.step(self.actionSeq[-1])\n",
      " \t\t\t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t# except:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\t# embed()\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      "@@ -265,8 +265,8 @@\n",
      " \t\t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\t\ti += 1\n",
      " \t\tif len(self.actionSeq)>0:\n",
      "-\t\t\tprint self.actionSeq[-1]\n",
      "-\t\tprint vrle.show()\n",
      "+\t\t\tprint(self.actionSeq[-1])\n",
      "+\t\tprint(vrle.show())\n",
      " \t\t# if len(vrle._game.sprite_groups['probe'])==0:\n",
      " \t\t\t# self.WBP.findAvatarInRLE(vrle) == (2,4):\n",
      " \t\t\t# embed()\n",
      "@@ -285,11 +285,11 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\ti+=1\n",
      " \n",
      "@@ -333,16 +333,16 @@\n",
      " \tt1 = time.time()\n",
      " \tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      " \t# winNodes = BFS(rle, p, 2)\n",
      "-\tprint time.time()-t1\n",
      "+\tprint(time.time()-t1)\n",
      " \tembed()\n",
      " \t# print len(visited), len(rejected)\n",
      " \tif not hasattr(last, 'actionSeq'):\n",
      "-\t\tprint \"Failed without tracking tokens. re-trying\"\n",
      "+\t\tprint(\"Failed without tracking tokens. re-trying\")\n",
      " \t\tp.trackTokens = True\n",
      " \t\tt1 = time.time()\n",
      " \t\t# winNodes = BFS(rle, p, 2)\n",
      " \t\tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      "-\t\tprint time.time()-t1\n",
      "+\t\tprint(time.time()-t1)\n",
      " \t\t# print len(visited), len(rejected)\n",
      " \tembed()\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored rtdp.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: rtdp.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- rtdp.py\t(original)\n",
      "+++ rtdp.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict, sys\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict, sys\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,13 +14,13 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " import curses\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      "@@ -71,14 +71,14 @@\n",
      " \t\ttry:\n",
      " \t\t\timmovables = self.rle.immovables\n",
      " \t\t\tself.immovables = immovables\n",
      "-\t\t\tprint \"immovables\", immovables\n",
      "+\t\t\tprint(\"immovables\", immovables)\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall']#,'poison']\n",
      " \t\t\tself.immovables = immovables\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self.rle._obstypes.keys():\n",
      "+\t\t\tif i in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -111,7 +111,7 @@\n",
      " \t\twhile len(self.rewardQueue)>0:\n",
      " \t\t\tloc = self.rewardQueue.popleft()\n",
      " \t\t\tif loc not in self.processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tself.processed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -141,8 +141,8 @@\n",
      " \t# \treturn\n",
      " \n",
      " \tdef findObjectInRLE(self, rle, objName):\n",
      "-\t\tif objName not in rle._obstypes.keys():\n",
      "-\t\t\tprint objName, \"not in rle.\"\n",
      "+\t\tif objName not in list(rle._obstypes.keys()):\n",
      "+\t\t\tprint(objName, \"not in rle.\")\n",
      " \t\t\t# return None\n",
      " \t\tobjLocs = []\n",
      " \t\tobjCode = 2**(1+sorted(self.rle._obstypes.keys())[::-1].index(objName))\n",
      "@@ -181,15 +181,15 @@\n",
      " \tdef findObjectInState(self, s, objName):\n",
      " \t\t##TODO: Finish last part of this function -- sometimes it can't access objloc[0][0], objloc[1][0]\n",
      " \t\tstate = np.reshape(np.fromstring(s,dtype=float), self.rle.outdim)\n",
      "-\t\tif objName not in rle._obstypes.keys():\n",
      "-\t\t\tprint objName, \"not in rle.\"\n",
      "+\t\tif objName not in list(rle._obstypes.keys()):\n",
      "+\t\t\tprint(objName, \"not in rle.\")\n",
      " \t\t\treturn None\n",
      " \t\tobjCode = 2**(1+sorted(self.rle._obstypes.keys())[::-1].index(objName))\n",
      " \t\tobjLoc = np.where(state==objCode)\n",
      " \t\ttry:\n",
      " \t\t\tobjLoc = objLoc[0][0], objLoc[1][0] #(y,x)\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"can't find objloc\"\n",
      "+\t\t\tprint(\"can't find objloc\")\n",
      " \t\t\tembed()\n",
      " \t\treturn objLoc\n",
      " \n",
      "@@ -212,7 +212,7 @@\n",
      " \t\t\t\t\tq.append((neighbor, path + [neighbor]))\n",
      " \n",
      " \t\tif node != goal_loc:\n",
      "-\t\t\tprint \"didn't find path to goal in getSubgoals (in getPathToGoal)\"\n",
      "+\t\t\tprint(\"didn't find path to goal in getSubgoals (in getPathToGoal)\")\n",
      " \t\t\treturn False\n",
      " \t\t\t# embed()\n",
      " \t\t\t# raise Exception(\"Didn't find a path to the goal location.\")\n",
      "@@ -223,14 +223,14 @@\n",
      " \t\t## find location of goal, add to rewardDict.\n",
      " \t\t## also add neighbors of goal rewardQueue.\n",
      " \t\t##TODO: update this if goal moves!!\n",
      "-\t\tif \"goal\" not in self.rle._obstypes.keys():\n",
      "-\t\t\tprint \"no goal to get subgoals to\"\n",
      "+\t\tif \"goal\" not in list(self.rle._obstypes.keys()):\n",
      "+\t\t\tprint(\"no goal to get subgoals to\")\n",
      " \t\t\treturn []\n",
      " \t\tgoal_code = 2**(1+sorted(self.rle._obstypes.keys())[::-1].index(\"goal\"))\n",
      " \t\tkillerObjectCodes = []\n",
      " \t\tif hasattr(self.rle, 'killerObjects'):\n",
      " \t\t\tfor o in self.rle.killerObjects:\n",
      "-\t\t\t\tif o in self.rle._obstypes.keys():\n",
      "+\t\t\t\tif o in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\t\tkillerObjectCodes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(o)))\n",
      " \t\tboard = np.reshape(self.rle._getSensors(), self.rle.outdim)\n",
      " \t\tgoal_loc = np.where(board==goal_code)\n",
      "@@ -272,7 +272,7 @@\n",
      " \t\t\t\t\t\t\t\t# print \"found altered path\", path[subgoal_index+i]\n",
      " \t\t\t\t\t\t\t\tbreak\n",
      " \t\t\t\t\t\texcept:\n",
      "-\t\t\t\t\t\t\tprint \"indices didn't work out in looking for different path\"\n",
      "+\t\t\t\t\t\t\tprint(\"indices didn't work out in looking for different path\")\n",
      " \t\t\t\t# self.subgoals.append(path[subgoal_index])\n",
      " \n",
      " \t\t\n",
      "@@ -303,9 +303,9 @@\n",
      " \t\t\tnextLoc = currentLoc[0]+a[1], currentLoc[1]+a[0] #again, locations are (y,x) and actions are (x,y)\n",
      " \t\telse:\n",
      " \t\t\treturn 0.\n",
      "-\t\tif nextLoc in self.rewardDict.keys():\n",
      "+\t\tif nextLoc in list(self.rewardDict.keys()):\n",
      " \t\t\treturn self.rewardDict[nextLoc]\n",
      "-\t\telif currentLoc in self.rewardDict.keys():\n",
      "+\t\telif currentLoc in list(self.rewardDict.keys()):\n",
      " \t\t\treturn self.rewardDict[currentLoc]\n",
      " \t\telse:\n",
      " \t\t\treturn 0.\n",
      "@@ -499,7 +499,7 @@\n",
      " \t\t\tself.V[s] = self.QVals[(s,a)]\n",
      " \t\t\trle = simulationResults[a]\n",
      " \t\t\t## UNCOMMENT HERE IF YOU WANT TO WATCH Q-learner learning.\n",
      "-\t\t\tprint rle.show()\n",
      "+\t\t\tprint(rle.show())\n",
      " \t\t\tterminal = rle._isDone()[0]\n",
      " \t\t\ti += 1\n",
      " \t\t\tif not terminal:\n",
      "@@ -541,19 +541,19 @@\n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tres = rle.step(a)\n",
      " \t\t\tif showActions:\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\tterminal = rle._isDone()[0]\n",
      " \t\t\ts = res['observation'].tostring()\n",
      " \t\treturn actions\n",
      " \n",
      " \tdef backwardsPlayback(self):\n",
      "-\t\tlst = [(k,v) for k,v in self.QVals.iteritems()]\n",
      "+\t\tlst = [(k,v) for k,v in self.QVals.items()]\n",
      " \t\tslist = sorted(lst, key=lambda x:x[1])\n",
      " \t\tslist.reverse()\n",
      " \t\tfor l in slist:\n",
      " \t\t\tif l[1]>0:\n",
      "-\t\t\t\tprint np.reshape(np.fromstring(l[0][0],dtype=float),self.rle.outdim)\n",
      "-\t\t\t\tprint l[1]\n",
      "+\t\t\t\tprint(np.reshape(np.fromstring(l[0][0],dtype=float),self.rle.outdim))\n",
      "+\t\t\t\tprint(l[1])\n",
      " \n",
      " if __name__ == \"__main__\":\n",
      " \t\n",
      "@@ -567,10 +567,10 @@\n",
      " \tgameString, levelString = defInputGame(gameFilename, randomize=True)\n",
      " \trleCreateFunc = lambda: createRLInputGame(gameFilename)\n",
      " \trle = rleCreateFunc()\n",
      "-\tprint rle.show()\n",
      "+\tprint(rle.show())\n",
      " \t# rle.immovables = ['wall', 'poison1', 'poison2']\n",
      "-\tprint \"\"\n",
      "-\tprint \"Initializing learner. Playing\", gameFilename\n",
      "+\tprint(\"\")\n",
      "+\tprint(\"Initializing learner. Playing\", gameFilename)\n",
      " \tql = QLearner(rle, gameString, levelString, alpha=1, epsilon=.5, gamma=.9, episodes=1000)\n",
      " \t# ql.getBestActionsForPlayout(False, True)\n",
      " \t# for x in range(10):\n",
      "@@ -581,7 +581,7 @@\n",
      " \t# ql.runEpisode()\n",
      " \tql.learn(50, satisfice=10)\n",
      " \tt2 = time.time() - t1\n",
      "-\tprint \"done in {} seconds\".format(t2)\n",
      "+\tprint(\"done in {} seconds\".format(t2))\n",
      " \t# ql.learn(100, satisfice=False)\n",
      " \n",
      " \tembed()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored mdpmap.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: mdpmap.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mdpmap.py\t(original)\n",
      "+++ mdpmap.py\t(refactored)\n",
      "@@ -20,8 +20,8 @@\n",
      " \n",
      " from scipy import zeros\n",
      " from pybrain.utilities import flood\n",
      "-from ontology import BASEDIRS\n",
      "-from interfaces import GameEnvironment\n",
      "+from .ontology import BASEDIRS\n",
      "+from .interfaces import GameEnvironment\n",
      " \n",
      " \n",
      " class MDPconverter(object):\n",
      "@@ -45,14 +45,14 @@\n",
      "     def convert(self, observations=True):\n",
      "         if self.verbose:\n",
      "             if observations:\n",
      "-                print 'Number of features:', 5 * len(self.env._obstypes)\n",
      "+                print('Number of features:', 5 * len(self.env._obstypes))\n",
      "         initSet = [self.env._initstate]\n",
      "         self.states = sorted(flood(self.tryMoves, None, initSet))\n",
      "         dim = len(self.states)        \n",
      "         if self.verbose:\n",
      "-            print 'Actual states:', dim\n",
      "-            print 'Non-zero rewards:', self.rewards\n",
      "-            print 'Initial state', initSet[0]\n",
      "+            print('Actual states:', dim)\n",
      "+            print('Non-zero rewards:', self.rewards)\n",
      "+            print('Initial state', initSet[0])\n",
      "         Ts = [zeros((dim, dim)) for _ in self.env._actionset]\n",
      "         R = zeros(dim)\n",
      "         statedic = {}\n",
      "@@ -61,7 +61,7 @@\n",
      "             statedic[pos] = si\n",
      "         for ai, a in enumerate(self.env._actionset):\n",
      "             actiondic[a] = ai\n",
      "-        for pos, val in self.rewards.items():\n",
      "+        for pos, val in list(self.rewards.items()):\n",
      "             R[statedic[pos]] += val\n",
      "         for pos, a, dest in self.sas_tuples:\n",
      "             ai = actiondic[a]\n",
      "@@ -69,7 +69,7 @@\n",
      "             di = statedic[dest]\n",
      "             Ts[ai][si, di] += 1. / self.avgOver\n",
      "         if self.verbose:\n",
      "-            print 'Built Ts.'\n",
      "+            print('Built Ts.')\n",
      "         for T in Ts:\n",
      "             for ti, row in enumerate(T):\n",
      "                 if sum(row) > 0:  \n",
      "@@ -77,14 +77,14 @@\n",
      "                 else:\n",
      "                     row[ti] = 1\n",
      "         if self.verbose:\n",
      "-            print 'Normalized Ts.'\n",
      "+            print('Normalized Ts.')\n",
      "         if observations:\n",
      "             # one observation for current position and each of the 4 neighbors.\n",
      "             fMap = zeros((len(self.env._obstypes) * 5, dim))\n",
      "             for si, state in enumerate(self.states):\n",
      "                 fMap[:, si] = self.env.getSensors(state)                \n",
      "             if self.verbose:\n",
      "-                print 'Built features.'        \n",
      "+                print('Built features.')        \n",
      "             return Ts, R, fMap\n",
      "         else:\n",
      "             return Ts, R\n",
      "@@ -104,7 +104,7 @@\n",
      "             dest = self.env.getState()\n",
      "             res.append(dest)\n",
      "             if self.verbose:\n",
      "-                print state, 'do', a, '>', dest\n",
      "+                print(state, 'do', a, '>', dest)\n",
      "             self.sas_tuples.append((state, a, dest))            \n",
      "             # remember reward if the final state ends the game\n",
      "             ended, win = self.env._isDone()\n",
      "@@ -114,37 +114,37 @@\n",
      "                 else:\n",
      "                     self.rewards[dest] = -1\n",
      "                 if self.verbose:\n",
      "-                    print 'Ends with', win\n",
      "+                    print('Ends with', win)\n",
      "         # pass on the list of neighboring states\n",
      "         return res\n",
      "         \n",
      "                         \n",
      " def testMaze():\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     from examples.gridphysics.mazes import polarmaze_game, maze_level_1\n",
      "     game_str, map_str = polarmaze_game, maze_level_1\n",
      "     g = VGDLParser().parseGame(game_str)\n",
      "     g.buildLevel(map_str)\n",
      "     C = MDPconverter(g, verbose=True)\n",
      "     Ts, R, fMap = C.convert()\n",
      "-    print C.states\n",
      "-    print R\n",
      "+    print(C.states)\n",
      "+    print(R)\n",
      "     for T in Ts:\n",
      "-        print T\n",
      "-    print fMap\n",
      "+        print(T)\n",
      "+    print(fMap)\n",
      " \n",
      " def testStochMaze():\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     from examples.gridphysics.mazes.stochastic import stoch_game, stoch_level\n",
      "     g = VGDLParser().parseGame(stoch_game)\n",
      "     g.buildLevel(stoch_level)\n",
      "     C = MDPconverter(g, verbose=True)\n",
      "     Ts, R, fMap = C.convert()\n",
      "-    print C.states\n",
      "-    print R\n",
      "+    print(C.states)\n",
      "+    print(R)\n",
      "     for T in Ts:\n",
      "-        print T\n",
      "-    print fMap\n",
      "+        print(T)\n",
      "+    print(fMap)\n",
      " \n",
      "     \n",
      " if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Can't parse benchmarking: ParseError: bad input: type=1, value='copies', context=(' ', (1, 4))\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: There was 1 error:\n",
      "RefactoringTool: Can't parse benchmarking: ParseError: bad input: type=1, value='copies', context=(' ', (1, 4))\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP_grid.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP_grid.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP_grid.py\t(original)\n",
      "+++ WBP_grid.py\t(refactored)\n",
      "@@ -3,10 +3,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame\n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict, sys\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic\n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict, sys\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic\n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " import math\n",
      "@@ -16,17 +16,17 @@\n",
      " import ipdb\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "-from util import *\n",
      "+from queue import Queue\n",
      "+from .util import *\n",
      " import multiprocessing\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " NoveltyRule, generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " # from line_profiler import LineProfiler\n",
      "-import cPickle\n",
      "+import pickle\n",
      " \n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      " NONE = 0\n",
      "@@ -39,12 +39,12 @@\n",
      "         firstOrderHorizon=False):\n",
      "         self.rle = rle\n",
      "         self.gameFilename = gameFilename\n",
      "-        self.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+        self.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "         self.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      "         self.trueAtoms = defaultdict(lambda:0) #set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-        self.objectTypes = rle._game.sprite_groups.keys()\n",
      "+        self.objectTypes = list(rle._game.sprite_groups.keys())\n",
      "         self.objectTypes.sort()\n",
      "-        self.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+        self.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      "         self.seen_limits = seen_limits\n",
      "         self.objIDs = {}\n",
      "         self.solution = None\n",
      "@@ -70,13 +70,13 @@\n",
      "             self.theory=copy.deepcopy(theory)\n",
      "             self.theory.interactionSet.extend(fakeInteractionRules)\n",
      "             self.theory.updateTerminations()\n",
      "-        print 'max nodes', self.max_nodes\n",
      "+        print('max nodes', self.max_nodes)\n",
      " \n",
      "         # for rule in self.theory.interactionSet:\n",
      "         #     if 'stepBack'==rule.interaction:\n",
      "         #         ipdb.set_trace()\n",
      "         i=1\n",
      "-        for k in rle._game.all_objects.keys():\n",
      "+        for k in list(rle._game.all_objects.keys()):\n",
      "             self.objIDs[k] = i * 100 * (rle.outdim[0]*rle.outdim[1]+self.padding)\n",
      "             i+=1\n",
      "         self.addSpaceBarToActions()\n",
      "@@ -90,8 +90,8 @@\n",
      " \n",
      "         ## Ignore objects we don't want to track (i.e., non-moving immovables.)\n",
      "         self.objectsToTrack = []\n",
      "-        for k in rle._game.sprite_groups.keys():\n",
      "-            if ((k in self.theory.classes.keys() and ('Resource' or 'Immovable') in str(self.theory.classes[k][0].vgdlType) and not \\\n",
      "+        for k in list(rle._game.sprite_groups.keys()):\n",
      "+            if ((k in list(self.theory.classes.keys()) and ('Resource' or 'Immovable') in str(self.theory.classes[k][0].vgdlType) and not \\\n",
      "             (('bounceForward' or 'pullWithIt') in [rule.interaction for rule in self.theory.interactionSet if k in [rule.slot1, rule.slot2]])) or\n",
      "             len(rle._game.sprite_groups[k])>self.objectNumberTrackingLimit):\n",
      "                 pass# self.objectsToNotTrackInAtomList.append(k)\n",
      "@@ -104,8 +104,8 @@\n",
      "             if len(rle._game.sprite_groups[k])>self.objectLocationTrackingLimit:\n",
      "                 self.classesWhoseLocationsWeIgnore.append(k)\n",
      " \n",
      "-        print \"ignoring presences for\", self.classesWhosePresenceWeIgnore\n",
      "-        print \"ignoring locations for\", self.classesWhoseLocationsWeIgnore\n",
      "+        print(\"ignoring presences for\", self.classesWhosePresenceWeIgnore)\n",
      "+        print(\"ignoring locations for\", self.classesWhoseLocationsWeIgnore)\n",
      "         # Compute starting number of each SpriteCounter stype\n",
      "         self.firstOrderHorizon = firstOrderHorizon\n",
      "         self.starting_stype_n = {}\n",
      "@@ -131,7 +131,7 @@\n",
      "         ## Note: if an object that isn't instantiated in the beginning is of a class that\n",
      "         ## spacebar applies to, we won't pick up on it here.\n",
      "         shootingClasses = ['MarioAvatar', 'ClimbingAvatar', 'ShootAvatar', 'Switch', 'FlakAvatar']\n",
      "-        classes = [str(o[0].__class__) for o in self.rle._game.sprite_groups.values() if len(o)>0]\n",
      "+        classes = [str(o[0].__class__) for o in list(self.rle._game.sprite_groups.values()) if len(o)>0]\n",
      "         spacebarAvailable = False\n",
      "         for sc in shootingClasses:\n",
      "             if any([sc in c for c in classes]):\n",
      "@@ -154,7 +154,7 @@\n",
      "             ## a function of the Flicker's presence is being taken care of by that. Otherwise the agent can keep exploring states that have no actual effect\n",
      "             ## on the game state.\n",
      "             if ((len(rle._game.sprite_groups[k])>0 and\n",
      "-                    rle._game.sprite_groups[k][0].colorName in self.theory.spriteObjects.keys() and\n",
      "+                    rle._game.sprite_groups[k][0].colorName in list(self.theory.spriteObjects.keys()) and\n",
      "                     any([obj in str(self.theory.spriteObjects[rle._game.sprite_groups[k][0].colorName].vgdlType) for obj in self.objectsWhoseLocationsWeIgnore])) or\n",
      "                 k in self.classesWhoseLocationsWeIgnore):\n",
      " \n",
      "@@ -197,7 +197,7 @@\n",
      "             ## a function of the Flicker's presence is being taken care of by that. Otherwise the agent can keep exploring states that have no actual effect\n",
      "             ## on the game state.\n",
      "             if (len(rle._game.sprite_groups[k])>0 and\n",
      "-                    rle._game.sprite_groups[k][0].colorName in self.theory.spriteObjects.keys() and\n",
      "+                    rle._game.sprite_groups[k][0].colorName in list(self.theory.spriteObjects.keys()) and\n",
      "                     any([obj in str(self.theory.spriteObjects[rle._game.sprite_groups[k][0].colorName].vgdlType) for obj in self.objectsWhosePresenceWeIgnore]) or\n",
      "                     k in self.classesWhosePresenceWeIgnore):\n",
      "                 pass\n",
      "@@ -216,7 +216,7 @@\n",
      " \n",
      "     def compareDicts(self, d1,d2):\n",
      "         ## only tells us what is in d2 that isn't in d1, as well as differences in values between shared keys\n",
      "-        return [k for k in d2.keys() if (k not in d1.keys() or d1[k]!=d2[k])]\n",
      "+        return [k for k in list(d2.keys()) if (k not in list(d1.keys()) or d1[k]!=d2[k])]\n",
      " \n",
      "     def delta(self, node1, node2):\n",
      "         if node1 is None:\n",
      "@@ -237,9 +237,9 @@\n",
      " \n",
      "     def rewardSelection(self, QReward, QNovelty):\n",
      "         # acceptableNodes = QReward\n",
      "-        acceptableNodes = filter(lambda n:n.novelty<3, QReward)\n",
      "-        acceptableNodes = filter(lambda n: (not n.terminal or n.win), acceptableNodes)\n",
      "-        print \"accetable:\", len(acceptableNodes)\n",
      "+        acceptableNodes = [n for n in QReward if n.novelty<3]\n",
      "+        acceptableNodes = [n for n in acceptableNodes if (not n.terminal or n.win)]\n",
      "+        print(\"accetable:\", len(acceptableNodes))\n",
      "         # if len(acceptableNodes)==0:\n",
      "             # acceptableNodes = QReward\n",
      "             # print \"Removed filter\"\n",
      "@@ -286,7 +286,7 @@\n",
      "             \"\"\"\n",
      "             # current = self.noveltySelection(QNovelty, QReward)\n",
      "             current = self.rewardSelection(QReward, QNovelty)\n",
      "-            print \"visited:\", len(visited)\n",
      "+            print(\"visited:\", len(visited))\n",
      "             # print(\"node chosen has position score {}\".format(current.position_score()))\n",
      "             # print embed()\n",
      "             if current in [None, 'pickMaxNode']:\n",
      "@@ -316,7 +316,7 @@\n",
      " \n",
      "             self.statesEncountered.append(current.rle._game.getFullState())\n",
      " \n",
      "-            print current.rle.show(indent=True)\n",
      "+            print(current.rle.show(indent=True))\n",
      " \n",
      "             current.updateNoveltyDict(QNovelty, QReward)\n",
      "             # embed()\n",
      "@@ -355,7 +355,7 @@\n",
      "                                 stypes = []\n",
      "                             for stype in stypes:\n",
      "                                 n_stypes = len([0 for sprite in self.findObjectsInRLE(child.rle, stype)])\n",
      "-                                if stype in self.starting_stype_n.keys() and self.starting_stype_n[stype] > n_stypes:\n",
      "+                                if stype in list(self.starting_stype_n.keys()) and self.starting_stype_n[stype] > n_stypes:\n",
      "                                     child.terminal, child.win = True, True\n",
      "                                     foundWin = True\n",
      "                                     break\n",
      "@@ -380,7 +380,7 @@\n",
      "                         ended, win, t = child.rle._isDone(getTermination=True)\n",
      "                         self.solution = child.actionSeq\n",
      "                         self.statesEncountered.append(child.rle._game.getFullState())\n",
      "-                        print \"win\"\n",
      "+                        print(\"win\")\n",
      "                         # print t\n",
      "                         # embed()\n",
      "                         # return child, gameString_array\n",
      "@@ -390,7 +390,7 @@\n",
      "             i+=1\n",
      " \n",
      "             if self.winning_states:\n",
      "-                print \"we have {} winning states\".format(len(self.winning_states))\n",
      "+                print(\"we have {} winning states\".format(len(self.winning_states)))\n",
      "                 bestNodes = sorted(self.winning_states, key=lambda n: (-n.intrinsic_reward))\n",
      "                 bestNode = bestNodes[0]\n",
      "                 # gameString_array.append(bestNode.rle.show())\n",
      "@@ -401,11 +401,11 @@\n",
      "         self.solution = []#Node(self.rle, self, [], None)\n",
      "         if i>=self.max_nodes:\n",
      "             if self.short_horizon:\n",
      "-                print \"playing with short horizon; reached max of {} nodes\".format(self.max_nodes)\n",
      "+                print(\"playing with short horizon; reached max of {} nodes\".format(self.max_nodes))\n",
      "                 node = max(visited, key=lambda n:n.intrinsic_reward)\n",
      "                 parentNode = copy.deepcopy(node)\n",
      "                 self.solution = node.actionSeq\n",
      "-                print self.solution\n",
      "+                print(self.solution)\n",
      "                 gameString_array, object_positions_array = [], []\n",
      "                 while parentNode is not None:\n",
      "                     gameString_array.append(parentNode.rle.show())\n",
      "@@ -418,7 +418,7 @@\n",
      "                 return node, gameString_array, object_positions_array\n",
      "             else:\n",
      "                 # self.quitting = True\n",
      "-                print \"Got no plan after searching {} nodes\".format(self.max_nodes)\n",
      "+                print(\"Got no plan after searching {} nodes\".format(self.max_nodes))\n",
      "         return None, None, None\n",
      " \n",
      " class Node():\n",
      "@@ -475,12 +475,12 @@\n",
      "             rolloutArray = []\n",
      "             i=0\n",
      "             terminal, win = vrle._isDone()\n",
      "-            print \"in rollout\"\n",
      "+            print(\"in rollout\")\n",
      "             while i<self.rolloutDepth and not terminal:\n",
      "                 a = random.choice([K_UP, K_DOWN, K_LEFT, K_RIGHT])\n",
      "                 # print a\n",
      "                 vrle.step(a)\n",
      "-                print vrle.show(indent=True)\n",
      "+                print(vrle.show(indent=True))\n",
      "                 currHeuristicVal = self.heuristics(vrle)\n",
      "                 heuristicVal = currHeuristicVal-prevHeuristicVal\n",
      "                 rolloutArray.append(heuristicVal)\n",
      "@@ -503,7 +503,7 @@\n",
      "             ## we want optimistic estimates of the future value of a shot. Take up to 100 samples but don't get caught in an infinite loop.\n",
      "             if terminal and not win and j<100:\n",
      "                 successfulRollout = False\n",
      "-                print \"rolling out again\"\n",
      "+                print(\"rolling out again\")\n",
      "                 j+=1\n",
      "                 # embed()\n",
      "             else:\n",
      "@@ -576,7 +576,7 @@\n",
      "                 current_resource = rle._game.sprite_groups[avatar[0]][0].resources[precondition.item]\n",
      "                 ## If we satisfy the precondiiton, append to tmp_list, then to killer_types (meaning we are capable of killing stype now)\n",
      "                 if eval(\"{}{}{}\".format(current_resource, true_operator, num)):\n",
      "-                    print \"reached resource limit\"\n",
      "+                    print(\"reached resource limit\")\n",
      "                     tmp_list.append(avatar)\n",
      "             except IndexError:\n",
      "                 pass\n",
      "@@ -654,7 +654,7 @@\n",
      "                     resource_yielder_names = [[inter.slot2 if (inter.interaction=='changeResource' and inter.args['resource']==res) else res if (inter.interaction=='collectResource' and res==inter.slot1) else None\n",
      "                     for inter in theory.interactionSet] for res in resource_names]\n",
      "                 except:\n",
      "-                    print \"failure with resource_yielder_names\"\n",
      "+                    print(\"failure with resource_yielder_names\")\n",
      "                     embed()\n",
      " \n",
      "                 resource_yielder_names = [[r for r in ryn if r] for ryn in resource_yielder_names] ## Remove 'None' yielded by last else condition above\n",
      "@@ -682,7 +682,7 @@\n",
      "                                 for obj1 in obj1_positions\n",
      "                                 for obj2 in obj2_positions])\n",
      "                         except:\n",
      "-                            print \"failure with obj1_positions\"\n",
      "+                            print(\"failure with obj1_positions\")\n",
      "                             embed()\n",
      " \n",
      "                         precondition_distances.append(min(possiblePairList))\n",
      "@@ -698,7 +698,7 @@\n",
      "                     # print distance\n",
      "                 except ValueError:\n",
      "                     if avatar_preconditions and avatars[0]:\n",
      "-                        print \"valueError in spritecounter_val\"\n",
      "+                        print(\"valueError in spritecounter_val\")\n",
      "                         embed()\n",
      "                     pass\n",
      "                     # effective_distance = 0\n",
      "@@ -899,7 +899,7 @@\n",
      "             # print factor * self.WBP.visited_positions[x, y]\n",
      "             return factor * self.WBP.visited_positions[x, y]\n",
      "         except IndexError:\n",
      "-            print \"index error in position score\"\n",
      "+            print(\"index error in position score\")\n",
      "             return 0\n",
      " \n",
      "     \"\"\"\n",
      "@@ -916,7 +916,7 @@\n",
      "             ## try to copy parent lastState. Then take action and store as current lastState.\n",
      "             ## if that fails, replay from beginning and store as current lastState\n",
      "             try:\n",
      "-                vrle = cPickle.loads(cPickle.dumps(self.parent.rle, -1))\n",
      "+                vrle = pickle.loads(pickle.dumps(self.parent.rle, -1))\n",
      "                 # vrle = copy.deepcopy(self.parent.rle)\n",
      "                 if len(self.actionSeq)>0:\n",
      "                     a = self.actionSeq[-1]\n",
      "@@ -926,12 +926,12 @@\n",
      "                     self.metabolic_cost = self.parent.metabolic_cost + self.metabolics(vrle, res['effectList'], a)\n",
      "                     self.terminal, self.win = vrle._isDone()\n",
      "             except:\n",
      "-                print \"conditions met but copy failed\"\n",
      "+                print(\"conditions met but copy failed\")\n",
      "                 embed()\n",
      "         else:\n",
      "             self.reconstructed=True\n",
      "             # print \"copy failed; replaying from top\"\n",
      "-            vrle = cPickle.loads(cPickle.dumps(self.rle, -1))\n",
      "+            vrle = pickle.loads(pickle.dumps(self.rle, -1))\n",
      "             # vrle = copy.deepcopy(self.rle)\n",
      "             self.terminal, self.win = vrle._isDone()\n",
      "             i=0\n",
      "@@ -996,7 +996,7 @@\n",
      " \n",
      "         try:\n",
      "             ## Planner should return a plan when the agent has reached the limit of any particular resource (because we now should be curious about new objects, which we're taking care of in main_agent)\n",
      "-            if any([self.rle._game.getAvatars()[0].resources[k]==self.WBP.theory.resource_limits[k] for k in self.rle._game.getAvatars()[0].resources.keys() if k not in self.WBP.seen_limits]):\n",
      "+            if any([self.rle._game.getAvatars()[0].resources[k]==self.WBP.theory.resource_limits[k] for k in list(self.rle._game.getAvatars()[0].resources.keys()) if k not in self.WBP.seen_limits]):\n",
      "                 self.win=True\n",
      "         except IndexError:\n",
      "             pass\n",
      "@@ -1040,12 +1040,12 @@\n",
      "         i = 0\n",
      "         for objType in vrle._game.sprite_groups:\n",
      "             for s in vrle._game.sprite_groups[objType]:\n",
      "-                if s.ID not in self.WBP.objIDs.keys():\n",
      "+                if s.ID not in list(self.WBP.objIDs.keys()):\n",
      "                     if s.name=='bullet':\n",
      "                         s.ID = len([o for o in vrle._game.sprite_groups[objType] if o not in vrle._game.kill_list])\n",
      "                     else:\n",
      "                         s.ID = len(vrle._game.sprite_groups[objType])\n",
      "-                    self.WBP.objIDs[s.ID] = (len(self.WBP.objIDs.keys())+1) * 100 * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      "+                    self.WBP.objIDs[s.ID] = (len(list(self.WBP.objIDs.keys()))+1) * 100 * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      "                     i+=1\n",
      "         return\n",
      " \n",
      "@@ -1061,13 +1061,13 @@\n",
      "         terminal = vrle._isDone()[0]\n",
      "         i=0\n",
      "         if not make_movie:\n",
      "-            print vrle.show()\n",
      "+            print(vrle.show())\n",
      "         while not terminal and i<len(self.actionSeq):\n",
      "             a = self.actionSeq[i]\n",
      "             vrle.step(a)\n",
      "             if not make_movie:\n",
      "-                print actionDict[a]\n",
      "-                print vrle.show()\n",
      "+                print(actionDict[a])\n",
      "+                print(vrle.show())\n",
      "             else:\n",
      "                 self.finalStatesEncountered.append(vrle._game.getFullState())\n",
      "             terminal = vrle._isDone()[0]\n",
      "@@ -1095,14 +1095,14 @@\n",
      "     # embed()\n",
      "     t1 = time.time()\n",
      "     last, gameString_array = p.BFS()\n",
      "-    from core import VGDLParser\n",
      "+    from .core import VGDLParser\n",
      "     # embed()\n",
      "     last.playBack(make_movie=True)\n",
      "     # VGDLParser.playGame(gameString, levelString, p.statesEncountered, persist_movie=True, make_images=True, make_movie=True, movie_dir=\"videos/\"+gameFilename, padding=0)\n",
      "     # VGDLParser.playGame(gameString, levelString, last.finalStatesEncountered, persist_movie=True, make_images=True, make_movie=True, movie_dir=\"videos/\"+gameFilename, padding=0)\n",
      " \n",
      " \n",
      "-    print time.time()-t1\n",
      "+    print(time.time()-t1)\n",
      "     # embed()\n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored bigloop.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: bigloop.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- bigloop.py\t(original)\n",
      "+++ bigloop.py\t(refactored)\n",
      "@@ -1,12 +1,12 @@\n",
      "-from mcts_pseudoreward_heuristic import *\n",
      "-from util import *\n",
      "-from core import colorDict\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, \\\n",
      "+from .mcts_pseudoreward_heuristic import *\n",
      "+from .util import *\n",
      "+from .core import colorDict\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, \\\n",
      " MultiSpriteCounterRule, ruleCluster, Theory, Game, writeTheoryToTxt, generateSymbolDict\n",
      " import importlib\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " \n",
      " # from vgdl.mcts_pseudoreward_heuristic import *\n",
      "@@ -40,14 +40,14 @@\n",
      " \n",
      " \tnoHypotheses = len(hypotheses)==0\n",
      " \n",
      "-\tprint \"\"\n",
      "+\tprint(\"\")\n",
      " \tif unknown_colors==False:\n",
      "-\t\tunknown_objects = [k for k in rle._game.sprite_groups.keys() if k!='avatar']\n",
      "+\t\tunknown_objects = [k for k in list(rle._game.sprite_groups.keys()) if k!='avatar']\n",
      " \t\tunknown_colors = [colorDict[str(rle._game.sprite_groups[k][0].color)] for k in unknown_objects]\n",
      "-\t\tprint \"unknown objects:\", unknown_colors\n",
      "+\t\tprint(\"unknown objects:\", unknown_colors)\n",
      " \telse:\n",
      "-\t\tprint \"already know some objects. Unknown:\"\n",
      "-\t\tprint unknown_colors\n",
      "+\t\tprint(\"already know some objects. Unknown:\")\n",
      "+\t\tprint(unknown_colors)\n",
      " \n",
      " \tended, won = rle._isDone()\n",
      " \t\n",
      "@@ -77,29 +77,29 @@\n",
      " \t\t\tVrle = createMindEnv(game, level, output=False)\n",
      " \t\t\tVrle.immovables = immovables\n",
      " \t\tif goalColor:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t## Select known goal if it's known, otherwise unkown object.\n",
      "-\t\t\tkey = [k for k in rle._game.sprite_groups.keys() if \\\n",
      "+\t\t\tkey = [k for k in list(rle._game.sprite_groups.keys()) if \\\n",
      " \t\t\tcolorDict[str(rle._game.sprite_groups[k][0].color)]==goalColor][0]\n",
      " \t\t\tactual_goal = rle._game.sprite_groups[key][0]\n",
      " \t\t\tobject_goal = actual_goal\n",
      " \t\t\tobject_goal_location = Vrle._rect2pos(object_goal.rect)\n",
      " \t\t\tobject_goal_location = object_goal_location[1], object_goal_location[0]\n",
      "-\t\t\tprint \"goal is known:\", goalColor\n",
      "-\t\t\tprint \"\"\n",
      "+\t\t\tprint(\"goal is known:\", goalColor)\n",
      "+\t\t\tprint(\"\")\n",
      " \t\telse:\n",
      " \t\t\ttry:\n",
      " \t\t\t\tobject_goal = selectObjectGoal(Vrle, unknown_colors, method=\"random_then_nearest\")\n",
      " \t\t\t\tobject_goal_location = Vrle._rect2pos(object_goal.rect)\n",
      " \t\t\t\tobject_goal_location = object_goal_location[1], object_goal_location[0]\n",
      "-\t\t\t\tprint \"object goal is\", colorDict[str(object_goal.color)], \"at location\", (rle._rect2pos(object_goal.rect)[1], rle._rect2pos(object_goal.rect)[0])\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tprint(\"object goal is\", colorDict[str(object_goal.color)], \"at location\", (rle._rect2pos(object_goal.rect)[1], rle._rect2pos(object_goal.rect)[0]))\n",
      "+\t\t\t\tprint(\"\")\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"no unknown objects and no goal? Embedding so you can debug.\"\n",
      "+\t\t\t\tprint(\"no unknown objects and no goal? Embedding so you can debug.\")\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \t\tgame, level, symbolDict, immovables = writeTheoryToTxt(rle, hypotheses[0], symbolDict,\\\n",
      " \t\t \"./examples/gridphysics/theorytest.py\", object_goal_location)\n",
      " \n",
      "-\t\tprint \"Initializing mental theory *with* object goal\"\n",
      "+\t\tprint(\"Initializing mental theory *with* object goal\")\n",
      " \t\t# print \"immovables\", immovables\n",
      " \t\tVrle = createMindEnv(game, level, output=True)\t\t\t\t\t\t\t## World in agent's head, including object goal\n",
      " \t\tVrle.immovables = immovables\n",
      "@@ -124,7 +124,7 @@\n",
      " \t\t\thypotheses[0].goalColor=goalColor\n",
      " \n",
      " \tif playback:\t\t\t## TODO: Aritro cleans this up.\n",
      "-\t\tprint \"in playback\"\n",
      "+\t\tprint(\"in playback\")\n",
      " \t\tfrom vgdl.core import VGDLParser\n",
      " \t\tfrom examples.gridphysics.simpleGame4 import level, game\n",
      " \t\tplaybackGame = game\n",
      "@@ -162,7 +162,7 @@\n",
      " \t\t\tunknown_colors=unknown_colors, goalColor=goalColor, finalEventList=finalEventList, \\\n",
      " \t\t\tplayback=True)\n",
      " \t\ttally.append(won)\n",
      "-\t\tprint \"episode ended. Win:\", won\n",
      "-\t\tprint \"__________________________________________________\"\n",
      "-\tprint \"Won\", sum(tally), \"out of \", len(tally), \"episodes.\"\n",
      "+\t\tprint(\"episode ended. Win:\", won)\n",
      "+\t\tprint(\"__________________________________________________\")\n",
      "+\tprint(\"Won\", sum(tally), \"out of \", len(tally), \"episodes.\")\n",
      " \t# embed()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP2.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP2.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP2.py\t(original)\n",
      "+++ WBP2.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " ACTIONS = [(1,0), (-1,0), (0,1), (0,-1)]\n",
      "@@ -8,12 +8,12 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename, display):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys()])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys())])\n",
      " \t\tself.maxNumObjects = 6\n",
      " \n",
      " \tdef calculateAtoms(self, rle):\n",
      "@@ -106,7 +106,7 @@\n",
      " \t\tif n%2==1:\n",
      " \t\t\tdecomposition.append(0)\n",
      " \t\t\tn = n-1\n",
      "-\t\ti = len(rle._obstypes.keys())\n",
      "+\t\ti = len(list(rle._obstypes.keys()))\n",
      " \t\twhile i>0:\n",
      " \t\t\tif n>=2**i:\n",
      " \t\t\t\tdecomposition.append(i)\n",
      "@@ -132,7 +132,7 @@\n",
      " \t\treturn mergedList\n",
      " \n",
      " \tdef factorizeBoolean(self, rle, n):\n",
      "-\t\tlistLen = len(rle._obstypes.keys())+1\n",
      "+\t\tlistLen = len(list(rle._obstypes.keys()))+1\n",
      " \t\treturn self.indicesToBooleans(self.T, self.factorize(rle, n))\n",
      " \n",
      " \tdef findTrueTuples(self, node, k):\n",
      "@@ -276,12 +276,12 @@\n",
      " \t\t\t\t\tvrle.step(self.actionSeq[-1])\n",
      " \t\t\t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t# except:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\t# embed()\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      "@@ -291,8 +291,8 @@\n",
      " \t\t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\t\ti += 1\n",
      " \t\tif len(self.actionSeq)>0:\n",
      "-\t\t\tprint self.actionSeq[-1]\n",
      "-\t\tprint vrle.show()\n",
      "+\t\t\tprint(self.actionSeq[-1])\n",
      "+\t\tprint(vrle.show())\n",
      " \t\t# if len(vrle._game.sprite_groups['probe'])==0:\n",
      " \t\t\t# self.WBP.findAvatarInRLE(vrle) == (2,4):\n",
      " \t\t\t# embed()\n",
      "@@ -311,11 +311,11 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\ti+=1\n",
      " \n",
      "@@ -396,7 +396,7 @@\n",
      " \n",
      " \tt1 = time.time()\n",
      " \tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      "-\tprint len(visited), len(rejected)\n",
      "-\tprint time.time()-t1\n",
      "+\tprint(len(visited), len(rejected))\n",
      "+\tprint(time.time()-t1)\n",
      " \tembed()\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored core2.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: core2.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- core2.py\t(original)\n",
      "+++ core2.py\t(refactored)\n",
      "@@ -6,7 +6,7 @@\n",
      " \n",
      " import pygame\n",
      " from random import choice\n",
      "-from tools import Node, indentTreeParser\n",
      "+from .tools import Node, indentTreeParser\n",
      " from collections import defaultdict\n",
      " from vgdl.tools import roundedPoints\n",
      " import os\n",
      "@@ -68,9 +68,9 @@\n",
      "     @staticmethod\n",
      "     def playSubjectiveGame(game_str, map_str):\n",
      "         from pybrain.rl.experiments.episodic import EpisodicExperiment\n",
      "-        from interfaces import GameTask\n",
      "-        from subjective import SubjectiveGame\n",
      "-        from agents import InteractiveAgent, UserTiredException\n",
      "+        from .interfaces import GameTask\n",
      "+        from .subjective import SubjectiveGame\n",
      "+        from .agents import InteractiveAgent, UserTiredException\n",
      "         g = VGDLParser().parseGame(game_str)\n",
      "         g.buildLevel(map_str)\n",
      "         senv = SubjectiveGame(g, actionDelay=100, recordingEnabled=True)\n",
      "@@ -103,7 +103,7 @@\n",
      "         \"\"\" Whatever is visible in the global namespace (after importing the ontologies)\n",
      "         can be used in the VGDL, and is evaluated.\n",
      "         \"\"\"\n",
      "-        from ontology import * #@UnusedWildImport\n",
      "+        from .ontology import * #@UnusedWildImport\n",
      "         return eval(estr)\n",
      " \n",
      "     def parseInteractions(self, inodes):\n",
      "@@ -114,14 +114,14 @@\n",
      "                 self.game.collision_eff.append(tuple([x.strip() for x in pair.split(\" \") if len(x)>0]\n",
      "                                                      +[eclass, args]))\n",
      "                 if self.verbose:\n",
      "-                    print \"Collision\", pair, \"has effect:\", edef\n",
      "+                    print(\"Collision\", pair, \"has effect:\", edef)\n",
      "         #print self.game.collision_eff\n",
      " \n",
      "     def parseTerminations(self, tnodes):\n",
      "         for tn in tnodes:\n",
      "             sclass, args = self._parseArgs(tn.content)\n",
      "             if self.verbose:\n",
      "-                print \"Adding:\", sclass, args\n",
      "+                print(\"Adding:\", sclass, args)\n",
      "             self.game.terminations.append(sclass(**args))\n",
      " \n",
      "     def parseSprites(self, snodes, parentclass=None, parentargs={}, parenttypes=[]):\n",
      "@@ -138,7 +138,7 @@\n",
      " \n",
      "             if len(sn.children) == 0:\n",
      "                 if self.verbose:\n",
      "-                    print \"Defining:\", key, sclass, args, stypes\n",
      "+                    print(\"Defining:\", key, sclass, args, stypes)\n",
      "                 self.game.sprite_constr[key] = (sclass, args, stypes)\n",
      "                 if key in self.game.sprite_order:\n",
      "                     # last one counts\n",
      "@@ -154,7 +154,7 @@\n",
      "             # a char can map to multiple sprites\n",
      "             keys = [x.strip() for x in val.split(\" \") if len(x)>0]\n",
      "             if self.verbose:\n",
      "-                print \"Mapping\", c, keys\n",
      "+                print(\"Mapping\", c, keys)\n",
      "             self.game.char_mapping[c] = keys\n",
      " \n",
      "     def _parseArgs(self, s,  sclass=None, args=None):\n",
      "@@ -188,13 +188,13 @@\n",
      "     load_save_enabled = True\n",
      " \n",
      "     def __init__(self, **kwargs):\n",
      "-        from ontology import Immovable, DARKGRAY, MovingAvatar, GOLD\n",
      "-        for name, value in kwargs.iteritems():\n",
      "-            print \"NAME: \", name\n",
      "+        from .ontology import Immovable, DARKGRAY, MovingAvatar, GOLD\n",
      "+        for name, value in kwargs.items():\n",
      "+            print(\"NAME: \", name)\n",
      "             if hasattr(self, name):\n",
      "                 self.__dict__[name] = value\n",
      "             else:\n",
      "-                print \"WARNING: undefined parameter '%s' for game! \"%(name)\n",
      "+                print(\"WARNING: undefined parameter '%s' for game! \"%(name))\n",
      " \n",
      "         # contains mappings to constructor (just a few defaults are known)\n",
      "         self.sprite_constr = {'wall': (Immovable, {'color': DARKGRAY}, ['wall']),\n",
      "@@ -231,9 +231,9 @@\n",
      "         self.kill_list=[]\n",
      " \n",
      "     def buildLevel(self, lstr):\n",
      "-        from ontology import stochastic_effects\n",
      "+        from .ontology import stochastic_effects\n",
      "         lines = [l for l in lstr.split(\"\\n\") if len(l)>0]\n",
      "-        lengths = map(len, lines)\n",
      "+        lengths = list(map(len, lines))\n",
      "         assert min(lengths)==max(lengths), \"Inconsistent line lengths.\"\n",
      "         self.width = lengths[0]\n",
      "         self.height = len(lines)\n",
      "@@ -243,7 +243,7 @@\n",
      "         self.screensize = (self.width*self.block_size, self.height*self.block_size)\n",
      " \n",
      "         # set up resources\n",
      "-        for res_type, (sclass, args, _) in self.sprite_constr.iteritems():\n",
      "+        for res_type, (sclass, args, _) in self.sprite_constr.items():\n",
      "             if issubclass(sclass, Resource):\n",
      "                 if 'res_type' in args:\n",
      "                     res_type = args['res_type']\n",
      "@@ -293,7 +293,7 @@\n",
      "         res = []\n",
      "         for key in keys:\n",
      "             if self.num_sprites > self.MAX_SPRITES:\n",
      "-                print \"Sprite limit reached.\"\n",
      "+                print(\"Sprite limit reached.\")\n",
      "                 return\n",
      "             sclass, args, stypes = self.sprite_constr[key]\n",
      "             # verify the singleton condition\n",
      "@@ -330,7 +330,7 @@\n",
      "             self.screen = pygame.display.set_mode((1,1))\n",
      "             self.background = pygame.Surface(size)\n",
      "         else:\n",
      "-            from ontology import LIGHTGRAY\n",
      "+            from .ontology import LIGHTGRAY\n",
      "             pygame.init()\n",
      "             self.screen = pygame.display.set_mode(size)\n",
      "             self.background = pygame.Surface(size)\n",
      "@@ -363,7 +363,7 @@\n",
      "     def getAvatars(self):\n",
      "         \"\"\" The currently alive avatar(s) \"\"\"\n",
      "         res = []\n",
      "-        for ss in self.sprite_groups.values():\n",
      "+        for ss in list(self.sprite_groups.values()):\n",
      "             if ss and isinstance(ss[0], Avatar):\n",
      "                 res.extend([s for s in ss if s not in self.kill_list])\n",
      "         return res\n",
      "@@ -415,7 +415,7 @@\n",
      "                     ss[str(pos)] = attrs\n",
      "                 else:\n",
      "                     ss[pos] = attrs\n",
      "-                for a, val in s.__dict__.iteritems():\n",
      "+                for a, val in s.__dict__.items():\n",
      "                     if a not in ias:\n",
      "                         attrs[a] = val\n",
      "                 if s.resources:\n",
      "@@ -432,17 +432,17 @@\n",
      "         self.reset()\n",
      "         self.score = fs['score']\n",
      "         self.ended = fs['ended']\n",
      "-        for key, ss in fs['objects'].iteritems():\n",
      "+        for key, ss in fs['objects'].items():\n",
      "             self.sprite_groups[key] = []\n",
      "-            for pos, attrs in ss.iteritems():\n",
      "+            for pos, attrs in ss.items():\n",
      "                 if as_string:\n",
      "                     p = eval(pos)\n",
      "                 else:\n",
      "                     p = pos\n",
      "                 s = self._createSprite_cheap(key, p)\n",
      "-                for a, val in attrs.iteritems():\n",
      "+                for a, val in attrs.items():\n",
      "                     if a == 'resources':\n",
      "-                        for r, v in val.iteritems():\n",
      "+                        for r, v in val.items():\n",
      "                             s.resources[r] = v\n",
      "                     else:\n",
      "                         s.__setattr__(a, val)\n",
      "@@ -624,7 +624,7 @@\n",
      "                         self.keystate = emptyKeyState\n",
      "                     else:\n",
      "                         lastKeyPress = self.keystate\n",
      "-                        if lastKeyPress.index(1) in keyPresses.keys():\n",
      "+                        if lastKeyPress.index(1) in list(keyPresses.keys()):\n",
      "                             keyPressType = keyPresses[lastKeyPress.index(1)]\n",
      "                             # print keyPressType\n",
      " \n",
      "@@ -691,7 +691,7 @@\n",
      "             VGDLSprite.dirtyrects = []\n",
      " \n",
      "         if(persist_movie):\n",
      "-            print \"Creating Movie\"\n",
      "+            print(\"Creating Movie\")\n",
      "             self.video_file = \"./videos/\" +  str(self.uiud) + \".mp4\"\n",
      "             subprocess.call([\"ffmpeg\",\"-y\",  \"-r\", \"30\", \"-b\", \"800\", \"-i\", tmpl, self.video_file ])\n",
      "             [os.remove(f) for f in glob.glob(tmp_dir + \"*\" + str(self.uiud) + \"*\")]\n",
      "@@ -711,7 +711,7 @@\n",
      "         # print finalEventList[-1]\n",
      "         # print \"],\\n{}\\n)\\n\\n\".format(terminationCondition)\n",
      " \n",
      "-        print \"Expecting {} events\".format(len(finalEventList))\n",
      "+        print(\"Expecting {} events\".format(len(finalEventList)))\n",
      " \n",
      "         if win:\n",
      "             # winning a game always gives a positive score.\n",
      "@@ -719,10 +719,10 @@\n",
      "                 self.score = 1\n",
      " \n",
      "             self.win = True\n",
      "-            print \"Game won, with score %s\" % self.score\n",
      "+            print(\"Game won, with score %s\" % self.score)\n",
      "         else:\n",
      "             self.win = False\n",
      "-            print \"Game lost. Score=%s\" % self.score\n",
      "+            print(\"Game lost. Score=%s\" % self.score)\n",
      " \n",
      "         # ipdb.set_trace()\n",
      " \n",
      "@@ -809,7 +809,7 @@\n",
      "     shrinkfactor=0\n",
      " \n",
      "     def __init__(self, pos, size=(10,10), color=None, speed=None, cooldown=None, physicstype=None, **kwargs):\n",
      "-        from ontology import GridPhysics\n",
      "+        from .ontology import GridPhysics\n",
      "         self.rect = pygame.Rect(pos, size)\n",
      "         self.lastrect = self.rect\n",
      "         self.physicstype = physicstype or self.physicstype or GridPhysics\n",
      "@@ -822,11 +822,11 @@\n",
      "         self.color = color or self.color or (140, 20, 140)\n",
      " \n",
      "         #self.color = color or self.color or (choice(self.COLOR_DISC), choice(self.COLOR_DISC), choice(self.COLOR_DISC))\n",
      "-        for name, value in kwargs.iteritems():\n",
      "+        for name, value in kwargs.items():\n",
      "             try:\n",
      "                 self.__dict__[name] = value\n",
      "             except:\n",
      "-                print \"WARNING: undefined parameter '%s' for sprite '%s'! \"%(name, self.__class__.__name__)\n",
      "+                print(\"WARNING: undefined parameter '%s' for sprite '%s'! \"%(name, self.__class__.__name__))\n",
      "         # how many timesteps ago was the last move?\n",
      "         self.lastmove = 0\n",
      " \n",
      "@@ -860,7 +860,7 @@\n",
      "         return (self.rect[0]-self.lastrect[0], self.rect[1]-self.lastrect[1])\n",
      " \n",
      "     def _draw(self, game):\n",
      "-        from ontology import LIGHTGREEN\n",
      "+        from .ontology import LIGHTGREEN\n",
      "         screen = game.screen\n",
      "         if self.shrinkfactor != 0:\n",
      "             shrunk = self.rect.inflate(-self.rect.width*self.shrinkfactor,\n",
      "@@ -885,7 +885,7 @@\n",
      " \n",
      "     def _drawResources(self, game, screen, rect):\n",
      "         \"\"\" Draw progress bars on the bottom third of the sprite \"\"\"\n",
      "-        from ontology import BLACK\n",
      "+        from .ontology import BLACK\n",
      "         tot = len(self.resources)\n",
      "         barheight = rect.height/3.5/tot\n",
      "         offset = rect.top+2*rect.height/3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored ai_algorithms.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: ai_algorithms.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ai_algorithms.py\t(original)\n",
      "+++ ai_algorithms.py\t(refactored)\n",
      "@@ -137,7 +137,7 @@\n",
      " \n",
      " def query_player(game, state):\n",
      "     \"Make a move by querying standard input.\"\n",
      "-    move_string = input('Your move? ')\n",
      "+    move_string = eval(input('Your move? '))\n",
      "     try:\n",
      "         move = eval(move_string)\n",
      "     except NameError:\n",
      "@@ -284,8 +284,8 @@\n",
      "         board = state.board\n",
      "         for x in range(1, self.h + 1):\n",
      "             for y in range(1, self.v + 1):\n",
      "-                print board.get((x, y), '.')\n",
      "-            print\n",
      "+                print(board.get((x, y), '.'))\n",
      "+            print()\n",
      " \n",
      "     def compute_utility(self, board, move, player):\n",
      "         \"If 'X' wins with this move, return 1; if 'O' wins return -1; else return 0.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored mcts.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: mcts.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mcts.py\t(original)\n",
      "+++ mcts.py\t(refactored)\n",
      "@@ -1,10 +1,10 @@\n",
      " import numpy as np\n",
      " from numpy import zeros\n",
      " import pygame    \n",
      "-from ontology import BASEDIRS\n",
      "-from core import VGDLSprite, colorDict\n",
      "-from stateobsnonstatic import StateObsHandlerNonStatic \n",
      "-from rlenvironmentnonstatic import *\n",
      "+from .ontology import BASEDIRS\n",
      "+from .core import VGDLSprite, colorDict\n",
      "+from .stateobsnonstatic import StateObsHandlerNonStatic \n",
      "+from .rlenvironmentnonstatic import *\n",
      " import argparse\n",
      " import random\n",
      " from IPython import embed\n",
      "@@ -14,14 +14,14 @@\n",
      " import time\n",
      " import copy\n",
      " from threading import Lock\n",
      "-from Queue import Queue\n",
      "+from queue import Queue\n",
      " import multiprocessing\n",
      "-from qlearner import *\n",
      "-from ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "-from ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "-from theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      "+from .qlearner import *\n",
      "+from .ontology import Immovable, Passive, Resource, ResourcePack, RandomNPC, Chaser, AStarChaser, OrientedSprite, Missile\n",
      "+from .ontology import initializeDistribution, updateDistribution, updateOptions, sampleFromDistribution, spriteInduction, selectObjectGoal\n",
      "+from .theory_template import TimeStep, Precondition, InteractionRule, TerminationRule, TimeoutRule, SpriteCounterRule, MultiSpriteCounterRule, \\\n",
      " generateSymbolDict, ruleCluster, Theory, Game, writeTheoryToTxt, generateTheoryFromGame\n",
      "-from rlenvironmentnonstatic import createRLInputGame\n",
      "+from .rlenvironmentnonstatic import createRLInputGame\n",
      " \n",
      " #A hack to display things to the terminal conveniently.\n",
      " np.core.arrayprint._line_width=250\n",
      "@@ -45,7 +45,7 @@\n",
      " \tdef __init__(self, existing_rle=False, game = None, level = None, partitionWeights=[1,0,1],\\\n",
      " \t\t         rleCreateFunc=False, obsType = OBSERVATION_GLOBAL, decay_factor=.8, num_workers=1):\n",
      " \t\tif not existing_rle and not rleCreateFunc:\n",
      "-\t\t\tprint \"You must pass either an existing rle or an rleCreateFunc\"\n",
      "+\t\t\tprint(\"You must pass either an existing rle or an rleCreateFunc\")\n",
      " \t\t\treturn\n",
      " \t\t# assumption: not starting on terminal state\n",
      " \t\t\"\"\"\n",
      "@@ -102,7 +102,7 @@\n",
      " \t\tgoal_loc = np.where(np.reshape(self.rle._getSensors(), self.outdim)==goal_code)\n",
      " \t\tgoal_loc = goal_loc[0][0], goal_loc[1][0] #(y,x)\n",
      " \n",
      "-\t\tif 'avatar' in self._obstypes.keys():\n",
      "+\t\tif 'avatar' in list(self._obstypes.keys()):\n",
      " \t\t\tinverted_avatar_loc=self._obstypes['avatar'][0]\n",
      " \t\t\tavatar_loc = (inverted_avatar_loc[1], inverted_avatar_loc[0])\n",
      " \t\t\tself.avatar_code = np.reshape(self.rle._getSensors(), self.outdim)[avatar_loc[0]][avatar_loc[1]]\n",
      "@@ -143,7 +143,7 @@\n",
      " \t\tgoal_code = 2**(1+sorted(self._obstypes.keys())[::-1].index(\"goal\"))\n",
      " \t\tkillerObjectCodes = []\n",
      " \t\tfor o in self.rle.killerObjects:\n",
      "-\t\t\tif o in self.rle._obstypes.keys():\n",
      "+\t\t\tif o in list(self.rle._obstypes.keys()):\n",
      " \t\t\t\tkillerObjectCodes.append(2**(1+sorted(self.rle._obstypes.keys())[::-1].index(o)))\n",
      " \t\tboard = np.reshape(self.rle._getSensors(), self.rle.outdim)\n",
      " \t\tgoal_loc = np.where(board==goal_code)\n",
      "@@ -178,7 +178,7 @@\n",
      " \t\t\t\t\t\t\t\t# print \"found altered path\", path[subgoal_index+i]\n",
      " \t\t\t\t\t\t\t\tbreak\n",
      " \t\t\t\t\t\texcept:\n",
      "-\t\t\t\t\t\t\tprint \"indices didn't work out in looking for different path\"\n",
      "+\t\t\t\t\t\t\tprint(\"indices didn't work out in looking for different path\")\n",
      " \n",
      " \t\t\t\t# self.subgoals.append(path[subgoal_index])\n",
      " \t\treturn self.subgoals\n",
      "@@ -194,10 +194,10 @@\n",
      " \t\t\t# print \"immovables\", immovables\n",
      " \t\texcept:\n",
      " \t\t\timmovables = ['wall', 'poison']\n",
      "-\t\t\tprint \"Using defaults as immovables\", immovables\n",
      "+\t\t\tprint(\"Using defaults as immovables\", immovables)\n",
      " \n",
      " \t\tfor i in immovables:\n",
      "-\t\t\tif i in self._obstypes.keys():\n",
      "+\t\t\tif i in list(self._obstypes.keys()):\n",
      " \t\t\t\timmovable_codes.append(2**(1+sorted(self._obstypes.keys())[::-1].index(i)))\n",
      " \n",
      " \t\tactionDict = defaultdict(list)\n",
      "@@ -230,7 +230,7 @@\n",
      " \t\twhile len(self.rewardQueue)>0:\n",
      " \t\t\tloc = self.rewardQueue.popleft()\n",
      " \t\t\tif loc not in self.processed:\n",
      "-\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in self.rewardDict.keys()]\n",
      "+\t\t\t\tvalid_neighbors = [n for n in self.neighborDict[loc] if n in list(self.rewardDict.keys())]\n",
      " \t\t\t\tself.rewardDict[loc] = max([self.rewardDict[n] for n in valid_neighbors]) * self.pseudoRewardDecay\n",
      " \t\t\t\tself.processed.append(loc)\n",
      " \t\t\t\tfor n in self.neighborDict[loc]:\n",
      "@@ -274,8 +274,8 @@\n",
      " \t\t\tVrle = copy.deepcopy(VRLE)\n",
      " \n",
      " \t\t\tif i%100==0 and len(rewards)>0:\n",
      "-\t\t\t\tprint \"Training cycle: %i\"%i\n",
      "-\t\t\t\tprint \"avg. rewards for last group of 100\", np.mean(rewards[-100:])\n",
      "+\t\t\t\tprint(\"Training cycle: %i\"%i)\n",
      "+\t\t\t\tprint(\"avg. rewards for last group of 100\", np.mean(rewards[-100:]))\n",
      " \n",
      " \t\t\tif defaultPolicySolveStep:\n",
      " \t\t\t\treward, v, iters = self.treePolicy(self.root, Vrle, step_horizon, \\\n",
      "@@ -311,7 +311,7 @@\n",
      " \tdef getBestActionsForPlayout(self, partitionWeights, debug=False):\n",
      " \t\tv = self.root\n",
      " \t\tactions = []\n",
      "-\t\twhile v and not v.terminal and len(v.children.keys())>0:\n",
      "+\t\twhile v and not v.terminal and len(list(v.children.keys()))>0:\n",
      " \t\t\ta, v = self.bestChild(v,partitionWeights, debug=debug)\n",
      " \t\t\tactions.append(a)\n",
      " \t\treturn actions\n",
      "@@ -331,24 +331,24 @@\n",
      " \t\tcntr=0\n",
      " \t\tv = self.root\n",
      " \t\tif output:\n",
      "-\t\t\tprint \"current state\"\n",
      "-\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "+\t\t\tprint(\"current state\")\n",
      "+\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      " \t\tactions, nodes = [], []\n",
      " \t\twhile v and not v.terminal and cntr<numActions:\n",
      " \t\t\tif output:\n",
      "-\t\t\t\tprint \"options\"\n",
      "-\t\t\t\tprint [(ACTIONS[k],c.qVal) for k,c in v.children.iteritems()]\n",
      "+\t\t\t\tprint(\"options\")\n",
      "+\t\t\t\tprint([(ACTIONS[k],c.qVal) for k,c in v.children.items()])\n",
      " \t\t\ta, v = self.bestChild(v,(1,0,0))\n",
      " \n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tnodes.append(v)\n",
      " \t\t\tif output:\n",
      " \t\t\t\tif v:\n",
      "-\t\t\t\t\tprint \"selected\"\n",
      "-\t\t\t\t\tprint ACTIONS[a]\n",
      "-\t\t\t\t\tprint \"resulted in\"\n",
      "-\t\t\t\t\tprint np.reshape(v.state, rle.outdim)\n",
      "-\t\t\t\t\tprint \"\"\n",
      "+\t\t\t\t\tprint(\"selected\")\n",
      "+\t\t\t\t\tprint(ACTIONS[a])\n",
      "+\t\t\t\t\tprint(\"resulted in\")\n",
      "+\t\t\t\t\tprint(np.reshape(v.state, rle.outdim))\n",
      "+\t\t\t\t\tprint(\"\")\n",
      " \t\t\tcntr+=1\n",
      " \t\t# if v.terminal:\n",
      " \t\t# \tdistance = 0\n",
      "@@ -389,7 +389,7 @@\n",
      " \t\t\t\tterminal = rle._isDone()[0]\n",
      " \n",
      " \t\t\t\tif terminal != v.terminal or not np.array_equal(v.state, rle._getSensors()):\n",
      "-\t\t\t\t\tprint \"inconsistency in node and rle\"\n",
      "+\t\t\t\t\tprint(\"inconsistency in node and rle\")\n",
      " \t\t\t\t\tembed()\n",
      " \t\t\t\t# terminal = (not res['pcontinue']) or (rle._avatar is None)\n",
      " \t\t\t\tif terminal:\n",
      "@@ -415,7 +415,7 @@\n",
      " \t\t\taction_choices = self.actions\n",
      " \n",
      " \t\tfor a in action_choices:\n",
      "-\t\t\tif a not in v.children.keys():\n",
      "+\t\t\tif a not in list(v.children.keys()):\n",
      " \t\t\t\texpand_action = a\n",
      " \t\t\t\tres = rle.step(a)\n",
      " \t\t\t\tnew_state = res[\"observation\"]\n",
      "@@ -440,13 +440,13 @@\n",
      " \tdef maxChild(self, v):\n",
      " \t\ttmp = np.where(np.reshape(v.state, self.outdim)==1)\n",
      " \t\tavatar_loc = tmp[0][0], tmp[1][0]\n",
      "-\t\tqVals = [v.children[a].qVal for a in v.children.keys()]\n",
      "-\t\tif len(qVals)>0 and avatar_loc in self.neighborDict.keys() and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      "+\t\tqVals = [v.children[a].qVal for a in list(v.children.keys())]\n",
      "+\t\tif len(qVals)>0 and avatar_loc in list(self.neighborDict.keys()) and len(qVals)>=len(self.neighborDict[avatar_loc])-1: #  -1, since (0,0) is not an action.\n",
      " \t\t\t\tmaxVal = max(qVals)\n",
      "-\t\t\t\tchoices = [(a,c) for (a,c) in v.children.items() if c.qVal==maxVal]\n",
      "-\t\t\t\tfor (a,c) in v.children.items():\n",
      "-\t\t\t\t\tprint a, c.qVal\n",
      "-\t\t\t\tprint \"\"\n",
      "+\t\t\t\tchoices = [(a,c) for (a,c) in list(v.children.items()) if c.qVal==maxVal]\n",
      "+\t\t\t\tfor (a,c) in list(v.children.items()):\n",
      "+\t\t\t\t\tprint(a, c.qVal)\n",
      "+\t\t\t\tprint(\"\")\n",
      " \t\t\t\treturn random.choice(choices)\n",
      " \t\telse:\n",
      " \t\t\treturn (None, None)\n",
      "@@ -487,7 +487,7 @@\n",
      " \t\t\tself.printexplorationweight = exploration_coefficient\n",
      " \t\t\t# print \"heuristic coefficient is now\", heuristic_coefficient\n",
      " \n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tcontinue\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -496,7 +496,7 @@\n",
      " \t\t\t\tif self.avatar_code not in [l%2 for l in c.state]: ##we're in a terminal losing state\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t## Don't give pseudoreward\n",
      " \t\t\t\t\tif not c.terminal:\n",
      "-\t\t\t\t\t\tprint \"terminal state but avatar not in c.state\"\n",
      "+\t\t\t\t\t\tprint(\"terminal state but avatar not in c.state\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \t\t\t\t\t# vLoc = np.where(np.reshape(v.state, self.outdim)%2==self.avatar_code)\n",
      "@@ -526,15 +526,15 @@\n",
      " \t\t\t\t\t\tsumVisitCount += abs(math.sqrt(2*math.log(v.visitCount)/c.visitCount))\n",
      " \t\t\t\t\t\tsumPseudoReward += abs(transform(loc))/c.visitCount\n",
      " \t\t\t\t\texcept TypeError:\n",
      "-\t\t\t\t\t\tprint \"loc is weird type\"\n",
      "+\t\t\t\t\t\tprint(\"loc is weird type\")\n",
      " \t\t\t\t\t\tembed()\n",
      " \n",
      " \n",
      " \n",
      " \t\t# print \"\"\n",
      " \t\tif debug:\n",
      "-\t\t\tprint np.reshape(v.state, self.outdim)\n",
      "-\t\tfor a,c in v.children.items():\n",
      "+\t\t\tprint(np.reshape(v.state, self.outdim))\n",
      "+\t\tfor a,c in list(v.children.items()):\n",
      " \t\t\tif v.equals(c):\n",
      " \t\t\t\tfuncVal = -float('inf')\n",
      " \t\t\telif c.visitCount == 0:\n",
      "@@ -589,18 +589,18 @@\n",
      " \t\t\t\t\t        + exploration_coefficient*math.sqrt(2*math.log(v.visitCount)/c.visitCount)/sumVisitCount \\\n",
      " \t\t\t\t\t        + heuristic_coefficient*(transform(loc)/c.visitCount)/sumPseudoReward\t\n",
      " \t\t\t\tif debug:\n",
      "-\t\t\t\t\tprint a, funcVal\n",
      "+\t\t\t\t\tprint(a, funcVal)\n",
      " \n",
      " \t\t\tif funcVal > maxFuncVal:\n",
      " \t\t\t\tmaxFuncVal = funcVal\n",
      " \t\t\t\tbestAction = a\n",
      " \t\t\t\tbestChild = c\n",
      " \t\tif bestChild == None:\t## Tiebreaker\n",
      "-\t\t\tbestAction = random.choice(v.children.keys())\n",
      "+\t\t\tbestAction = random.choice(list(v.children.keys()))\n",
      " \t\t\tbestChild = v.children[bestAction]\n",
      " \n",
      " \t\tif debug:\n",
      "-\t\t\tprint \"\"\n",
      "+\t\t\tprint(\"\")\n",
      " \t\treturn bestAction, bestChild\n",
      " \n",
      " \tdef defaultPolicy(self, v, rle, step_horizon, domain_knowledge=False):\n",
      "@@ -621,7 +621,7 @@\n",
      " \n",
      " \t\t\titers += 1\n",
      " \t\t\t\n",
      "-\t\t\tif domain_knowledge and avatar_loc in self.actionDict.keys():\n",
      "+\t\t\tif domain_knowledge and avatar_loc in list(self.actionDict.keys()):\n",
      " \t\t\t\tsample = random.choice(self.actionDict[avatar_loc])\n",
      " \t\t\telse:\n",
      " \t\t\t\tsample = random.choice([(-1,0), (1,0), (0,-1), (0,1)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored qlearner.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: qlearner.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- qlearner.py\t(original)\n",
      "+++ qlearner.py\t(refactored)\n",
      "@@ -1,4 +1,4 @@\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " \n",
      " class QLearner(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename, display, episodes=100, memory=None, \\\n",
      "@@ -29,7 +29,7 @@\n",
      " \t\telif policy == 'greedy':\n",
      " \t\t\tbestQVal, bestA, QValsAreAllEqual = self.bestSA(s, partitionWeights = [1,0,0], domainKnowledge = True)\n",
      " \t\t\tif printout:\n",
      "-\t\t\t\tprint bestQVal\n",
      "+\t\t\t\tprint(bestQVal)\n",
      " \t\t\tif QValsAreAllEqual:\n",
      " \t\t\t\treturn None\n",
      " \t\t\telse:\n",
      "@@ -79,10 +79,10 @@\n",
      " \n",
      " \t\t# print self.rle.show()\n",
      " \t\tif debug:\n",
      "-\t\t\tprint \"debugging bestSA\"\n",
      "+\t\t\tprint(\"debugging bestSA\")\n",
      " \t\t\tembed()\n",
      " \t\tfor a in actions:\n",
      "-\t\t\tif (s,a) not in self.QVals.keys():\n",
      "+\t\t\tif (s,a) not in list(self.QVals.keys()):\n",
      " \t\t\t\tself.QVals[(s,a)] = 0.\n",
      " \t\t\tsumQVal += abs(self.QVals[(s,a)])\n",
      " \t\t\tsumPseudoReward += self.getPseudoReward(s, a)\n",
      "@@ -123,8 +123,8 @@\n",
      " \t\t\t\tbestQVal = self.QVals[(s,a)]\n",
      " \t\t\t\tQValsAreAllEqual = True\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"actions array is empty. in bestSA\"\n",
      "-\t\t\t\tprint np.reshape(np.fromstring(s,dtype=float),self.rle.outdim)\n",
      "+\t\t\t\tprint(\"actions array is empty. in bestSA\")\n",
      "+\t\t\t\tprint(np.reshape(np.fromstring(s,dtype=float),self.rle.outdim))\n",
      " \t\t\t\tembed()\n",
      " \n",
      " \t\t# embed()\n",
      "@@ -136,7 +136,7 @@\n",
      " \t\ttry:\n",
      " \t\t\tself.QVals[(s,a)] = self.QVals[(s,a)] + self.alpha * (r + self.gamma*bestQVal - self.QVals[(s,a)])\n",
      " \t\texcept:\n",
      "-\t\t\tprint \"didn't find qvals[s,a]\"\n",
      "+\t\t\tprint(\"didn't find qvals[s,a]\")\n",
      " \t\t\tembed()\n",
      " \n",
      " \tdef runEpisode(self):\n",
      "@@ -157,7 +157,7 @@\n",
      " \n",
      " \t\t\t## UNCOMMENT HERE IF YOU WANT TO WATCH Q-learner learning.\n",
      " \t\t\tif self.display:\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \n",
      " \t\t\t## lower epsilon once you've found winning states.\n",
      " \t\t\tif self.anneal and rle._isDone()[1]:\n",
      "@@ -188,7 +188,7 @@\n",
      " \t\t\tif i%10==0:\n",
      " \t\t\t\ts = self.rle._getSensors().tostring()\n",
      " \t\t\t\ta = self.selectAction(s, policy='greedy', partitionWeights = self.partitionWeights)\n",
      "-\t\t\t\tprint i, self.QVals[(s,a)]\n",
      "+\t\t\t\tprint(i, self.QVals[(s,a)])\n",
      " \t\t\t\tif satisfice: ## see if values have propagated to start state; if so, return.\n",
      " \t\t\t\t\tactions = self.getBestActionsForPlayout()\n",
      " \t\t\t\t\tif len(actions)>0:\n",
      "@@ -198,8 +198,8 @@\n",
      " \t\t\t\tself.getBestActionsForPlayout(False, True)\n",
      " \t\ttime_elapsed = time.time()-t1\n",
      " \t\tself.printSummary()\n",
      "-\t\tprint \"Time to solution: {}\".format(time_elapsed)\n",
      "-\t\tprint \"\"\n",
      "+\t\tprint(\"Time to solution: {}\".format(time_elapsed))\n",
      "+\t\tprint(\"\")\n",
      " \t\treturn i\n",
      " \n",
      " \tdef getBestActionsForPlayout(self, aggressive=False, showActions = False):\n",
      "@@ -223,26 +223,26 @@\n",
      " \t\t\tactions.append(a)\n",
      " \t\t\tres = rle.step(a)\n",
      " \t\t\tif showActions:\n",
      "-\t\t\t\tprint rle.show()\n",
      "+\t\t\t\tprint(rle.show())\n",
      " \t\t\tterminal = rle._isDone()[0]\n",
      " \t\t\ts = res['observation'].tostring()\n",
      " \t\treturn actions\n",
      " \n",
      " \tdef backwardsPlayback(self):\n",
      "-\t\tlst = [(k,v) for k,v in self.QVals.iteritems()]\n",
      "+\t\tlst = [(k,v) for k,v in self.QVals.items()]\n",
      " \t\tslist = sorted(lst, key=lambda x:x[1])\n",
      " \t\tslist.reverse()\n",
      " \t\tfor l in slist:\n",
      " \t\t\tif l[1]>0:\n",
      "-\t\t\t\tprint np.reshape(np.fromstring(l[0][0],dtype=float),self.rle.outdim)\n",
      "-\t\t\t\tprint l[1]\n",
      "+\t\t\t\tprint(np.reshape(np.fromstring(l[0][0],dtype=float),self.rle.outdim))\n",
      "+\t\t\t\tprint(l[1])\n",
      " \n",
      " \tdef printSummary(self):\n",
      "-\t\tprint \"\"\n",
      "-\t\tprint \"Game: {}\".format(self.gameFilename)\n",
      "-\t\tprint \"Immovables: {}\".format(self.immovables)\n",
      "-\t\tprint \"Enemies: {}\".format(self.killerObjects)\n",
      "-\t\tprint \"Parameters: Epsilon: {}. Gamma: {}. PartitionWeights: {}\".format(self.epsilon, self.gamma, self.partitionWeights)\n",
      "+\t\tprint(\"\")\n",
      "+\t\tprint(\"Game: {}\".format(self.gameFilename))\n",
      "+\t\tprint(\"Immovables: {}\".format(self.immovables))\n",
      "+\t\tprint(\"Enemies: {}\".format(self.killerObjects))\n",
      "+\t\tprint(\"Parameters: Epsilon: {}. Gamma: {}. PartitionWeights: {}\".format(self.epsilon, self.gamma, self.partitionWeights))\n",
      " \t\treturn\n",
      " if __name__ == \"__main__\":\n",
      " \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No files need to be modified.\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP_one_hot.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP_one_hot.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP_one_hot.py\t(original)\n",
      "+++ WBP_one_hot.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " ACTIONS = [(1,0), (-1,0), (0,1), (0,-1)]\n",
      "@@ -8,16 +8,16 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename, display):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\tself.trueAtoms = set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      " \t\tself.vecSize = None\n",
      "-\t\tprint \"If we track tokens we have an additional\", 2**self.phiSize, \"array elements.\"\n",
      "+\t\tprint(\"If we track tokens we have an additional\", 2**self.phiSize, \"array elements.\")\n",
      " \n",
      " \tdef calculateAtoms(self, rle):\n",
      " \t\t## Converts rle state into a long list of atoms of length Nx2xT\n",
      "@@ -47,7 +47,7 @@\n",
      " \t\t\tvec = vec+nums\n",
      " \n",
      " \t\tif self.vecSize==None:\n",
      "-\t\t\tprint len(vec), \"atoms\"\n",
      "+\t\t\tprint(len(vec), \"atoms\")\n",
      " \t\t\tself.vecSize = len(vec)\n",
      " \t\treturn np.array(vec)\n",
      " \n",
      "@@ -62,7 +62,7 @@\n",
      " \t\tif n%2==1:\n",
      " \t\t\tdecomposition.append(0)\n",
      " \t\t\tn = n-1\n",
      "-\t\ti = len(rle._obstypes.keys())\n",
      "+\t\ti = len(list(rle._obstypes.keys()))\n",
      " \t\twhile i>0:\n",
      " \t\t\tif n>=2**i:\n",
      " \t\t\t\tdecomposition.append(i)\n",
      "@@ -88,7 +88,7 @@\n",
      " \t\treturn mergedList\n",
      " \n",
      " \tdef factorizeBoolean(self, rle, n):\n",
      "-\t\tlistLen = len(rle._obstypes.keys())+1\n",
      "+\t\tlistLen = len(list(rle._obstypes.keys()))+1\n",
      " \t\treturn self.indicesToBooleans(self.T, self.factorize(rle, n))\n",
      " \n",
      " \tdef findTrueTuples(self, node, k):\n",
      "@@ -157,7 +157,7 @@\n",
      " \t\t \telse:\n",
      " \t\t \t\treturn random.choice(bestNodes)\n",
      " \t\telse:\n",
      "-\t\t\tprint \"found 0 nodes in noveltyHeuristic\"\n",
      "+\t\t\tprint(\"found 0 nodes in noveltyHeuristic\")\n",
      " \t\t\tembed()\n",
      " \n",
      " def rewardHeuristic(lst, WBP, k, surrogateCall=False):\n",
      "@@ -173,7 +173,7 @@\n",
      " \t \telse:\n",
      " \t \t\treturn random.choice(bestNodes)\n",
      " \telse:\n",
      "-\t\tprint \"found 0 nodes in rewardHeuristic\"\n",
      "+\t\tprint(\"found 0 nodes in rewardHeuristic\")\n",
      " \t\tembed()\n",
      " \n",
      " def BFS(rle, WBP, k):\n",
      "@@ -229,7 +229,7 @@\n",
      " \t\t## This is not nec. right.\n",
      " \t\t## TODO: make heuristics return None if they're forced to tie-break but rewards and novelties are 0.\n",
      " \t\tif current is None:\n",
      "-\t\t\tprint \"current was None\"\n",
      "+\t\t\tprint(\"current was None\")\n",
      " \t\t\tembed()\n",
      " \t\t\treturn Q, visited, rejected, visitedStates\n",
      " \t\telse:\n",
      "@@ -275,12 +275,12 @@\n",
      " \t\t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\t\t\t# terminal = vrle._isDone()[0]\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t# except:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\t# embed()\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      "@@ -292,8 +292,8 @@\n",
      " \t\t\t\t# terminal = vrle._isDone()[0]\n",
      " \t\t\t\ti += 1\n",
      " \t\tif len(self.actionSeq)>0:\n",
      "-\t\t\tprint self.actionSeq[-1]\n",
      "-\t\tprint vrle.show()\n",
      "+\t\t\tprint(self.actionSeq[-1])\n",
      "+\t\tprint(vrle.show())\n",
      " \t\t# if len(vrle._game.sprite_groups['probe'])==0:\n",
      " \t\t\t# self.WBP.findAvatarInRLE(vrle) == (2,4):\n",
      " \t\t\t# embed()\n",
      "@@ -315,11 +315,11 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\tterminal = vrle._isDone()[0]\n",
      " \t\t\ti+=1\n",
      " \n",
      "@@ -375,15 +375,15 @@\n",
      " \tt1 = time.time()\n",
      " \t# last, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      " \tlast, visited, rejected, visitedStates = BFS2(rle, p, 2)\n",
      "-\tprint time.time()-t1\n",
      "-\tprint len(visited), len(rejected)\n",
      "+\tprint(time.time()-t1)\n",
      "+\tprint(len(visited), len(rejected))\n",
      " \tembed()\n",
      " \tif not hasattr(last, 'actionSeq'):\n",
      "-\t\tprint \"Failed without tracking tokens. re-trying\"\n",
      "+\t\tprint(\"Failed without tracking tokens. re-trying\")\n",
      " \t\tp.trackTokens = True\n",
      " \t\tt1 = time.time()\n",
      " \t\tlast, visited, rejected, visitedStates = BFS(rle, p, 2)\n",
      "-\t\tprint time.time()-t1\n",
      "-\t\tprint len(visited), len(rejected)\n",
      "+\t\tprint(time.time()-t1)\n",
      "+\t\tprint(len(visited), len(rejected))\n",
      " \t# embed()\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/2to3\", line 5, in <module>\n",
      "    sys.exit(main(\"lib2to3.fixes\"))\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/main.py\", line 263, in main\n",
      "    rt.refactor(args, options.write, options.doctests_only,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 690, in refactor\n",
      "    return super(MultiprocessRefactoringTool, self).refactor(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 286, in refactor\n",
      "    self.refactor_file(dir_or_file, write, doctests_only)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 731, in refactor_file\n",
      "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 326, in refactor_file\n",
      "    input, encoding = self._read_python_source(filename)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib2to3/refactor.py\", line 322, in _read_python_source\n",
      "    return f.read(), encoding\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf3 in position 1: invalid continuation byte\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored load_games.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: load_games.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- load_games.py\t(original)\n",
      "+++ load_games.py\t(refactored)\n",
      "@@ -1,11 +1,11 @@\n",
      "-from main_agent import Agent\n",
      "+from .main_agent import Agent\n",
      " from games_to_hyperparameters import *\n",
      " import time\n",
      " import dill\n",
      " import os\n",
      " from IPython import embed\n",
      " import argparse\n",
      "-from util import str2bool\n",
      "+from .util import str2bool\n",
      " \n",
      " parser = argparse.ArgumentParser(description='Process game number.')\n",
      " parser.add_argument('--game_number', type=int, default=0, help='game number')\n",
      "@@ -89,7 +89,7 @@\n",
      " \n",
      " def gen_color():\n",
      "     from vgdl.colors import colorDict\n",
      "-    color_list = colorDict.values()\n",
      "+    color_list = list(colorDict.values())\n",
      "     color_list = [c for c in color_list if c not in ['UUWSWF']]\n",
      "     for color in color_list:\n",
      "         yield color\n",
      "@@ -141,11 +141,11 @@\n",
      " \n",
      "     ##then pass this down for multiple episodes\n",
      "     gameObject = None\n",
      "-    print game_levels\n",
      "+    print(game_levels)\n",
      " \n",
      "     agent.playCurriculum(level_game_pairs=level_game_pairs, make_movie=make_movie)\n",
      " \n",
      "-    print game_levels\n",
      "+    print(game_levels)\n",
      " \n",
      "     total_time = time.time() - start_time\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored WBP6.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: WBP6.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WBP6.py\t(original)\n",
      "+++ WBP6.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from IPython import embed\n",
      "-from planner import *\n",
      "+from .planner import *\n",
      " import itertools\n",
      " \n",
      " from pygame.locals import K_SPACE, K_UP, K_DOWN, K_LEFT, K_RIGHT\n",
      "@@ -11,27 +11,27 @@\n",
      " class WBP(Planner):\n",
      " \tdef __init__(self, rle, gameString, levelString, gameFilename, display):\n",
      " \t\tPlanner.__init__(self, rle, gameString, levelString, gameFilename, display)\n",
      "-\t\tself.T = len(rle._obstypes.keys())+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      "+\t\tself.T = len(list(rle._obstypes.keys()))+1 #number of object types. Adding avatar, which is not in obstypes.\n",
      " \t\tself.vecDim = [rle.outdim[0]*rle.outdim[1], 2, self.T]\n",
      " \t\t# self.noveltyDict = []\n",
      " \t\tself.trueAtoms = set() ## set of atoms that have been true at some point thus far in the planner.\n",
      "-\t\tself.objectTypes = rle._game.sprite_groups.keys()\n",
      "+\t\tself.objectTypes = list(rle._game.sprite_groups.keys())\n",
      " \t\tself.objectTypes.sort()\n",
      "-\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in rle._game.sprite_groups.keys() if k not in ['wall', 'avatar']])\n",
      "+\t\tself.phiSize = sum([len(rle._game.sprite_groups[k]) for k in list(rle._game.sprite_groups.keys()) if k not in ['wall', 'avatar']])\n",
      " \t\tself.objIDs = {}\n",
      " \t\tself.maxNumObjects = 6\n",
      " \t\tself.trackTokens = False\n",
      " \t\tself.vecSize = None\n",
      " \t\tself.padding = 5  ##5 is arbitrary; just to make sure we don't get overlap when we add positions\n",
      "-\t\tprint \"If we track tokens we have an additional\", 2**self.phiSize, \"array elements.\"\n",
      "+\t\tprint(\"If we track tokens we have an additional\", 2**self.phiSize, \"array elements.\")\n",
      " \t\ti=1\n",
      "-\t\tfor k in rle._game.all_objects.keys():\n",
      "+\t\tfor k in list(rle._game.all_objects.keys()):\n",
      " \t\t\tself.objIDs[k] = i * (rle.outdim[0]*rle.outdim[1]+self.padding)\n",
      " \t\t\ti+=1\n",
      " \n",
      " \tdef calculateAtoms(self, rle):\n",
      " \t\tlst = []\n",
      "-\t\tfor k in rle._game.sprite_groups.keys():\n",
      "+\t\tfor k in list(rle._game.sprite_groups.keys()):\n",
      " \t\t\tfor o in rle._game.sprite_groups[k]:\n",
      " \t\t\t\tif o not in rle._game.kill_list:\n",
      " \t\t\t\t\t## turn location into vector posd2[ition (rows appended one after the other.)\n",
      "@@ -58,7 +58,7 @@\n",
      " \t\n",
      " \tdef compareDicts(self, d1,d2):\n",
      " \t\t## only tells us what is in d2 that isn't in d1, as well as differences in values between shared keys\n",
      "-\t\treturn [k for k in d2.keys() if (k not in d1.keys() or d1[k]!=d2[k])]\n",
      "+\t\treturn [k for k in list(d2.keys()) if (k not in list(d1.keys()) or d1[k]!=d2[k])]\n",
      " \n",
      " \tdef delta(self, node1, node2):\n",
      " \t\tif node1 is None:\n",
      "@@ -72,7 +72,7 @@\n",
      " \t\tnewAtoms = self.delta(node.parent, node)\n",
      " \t\t# print \"in novelty fn\"\n",
      " \t\t# embed()\n",
      "-\t\tprint 'old true atoms', len(node.state-set(newAtoms))\n",
      "+\t\tprint('old true atoms', len(node.state-set(newAtoms)))\n",
      " \t\tif len(self.trueAtoms) > 0:\n",
      " \t\t\ttrueAtoms = node.state\n",
      " \t\t\toldTrueAtoms = set(trueAtoms)-set(newAtoms)\n",
      "@@ -115,7 +115,7 @@\n",
      " \t\t \telse:\n",
      " \t\t \t\treturn random.choice(bestNodes)\n",
      " \t\telse:\n",
      "-\t\t\tprint \"found 0 nodes in noveltyHeuristic\"\n",
      "+\t\t\tprint(\"found 0 nodes in noveltyHeuristic\")\n",
      " \t\t\tembed()\n",
      " \n",
      " def rewardHeuristic(lst, WBP, k, surrogateCall=False):\n",
      "@@ -131,7 +131,7 @@\n",
      " \t \telse:\n",
      " \t \t\treturn random.choice(bestNodes)\n",
      " \telse:\n",
      "-\t\tprint \"found 0 nodes in rewardHeuristic\"\n",
      "+\t\tprint(\"found 0 nodes in rewardHeuristic\")\n",
      " \t\tembed()\n",
      " \n",
      " \n",
      "@@ -155,7 +155,7 @@\n",
      " \t\t\t\tQ.put(child)\n",
      " \t\telse:\n",
      " \t\t\trejected.append(current)\n",
      "-\tprint \"no more states in queue\"\n",
      "+\tprint(\"no more states in queue\")\n",
      " \t# embed()\n",
      " \treturn Q, visited, rejected\n",
      " \n",
      "@@ -175,7 +175,7 @@\n",
      " \t\t\tcurrent = rewardHeuristic(Q, WBP, WBP.k, surrogateCall=False)\n",
      " \t\t## This is not nec. right.\n",
      " \t\tif current is None:\n",
      "-\t\t\tprint \"got no node\"\n",
      "+\t\t\tprint(\"got no node\")\n",
      " \t\t\tembed()\n",
      " \t\t\treturn Q, visited, rejected\n",
      " \t\telse:\n",
      "@@ -217,11 +217,11 @@\n",
      " \t\t\t\t\tvrle.step(self.actionSeq[-1])\n",
      " \t\t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\texcept:\n",
      "-\t\t\t\tprint \"conditions met but copy failed\"\n",
      "+\t\t\t\tprint(\"conditions met but copy failed\")\n",
      " \t\t\t\tembed()\n",
      " \t\telse:\n",
      " \t\t\tself.reconstructed=True\n",
      "-\t\t\tprint \"copy failed; replaying from top\"\n",
      "+\t\t\tprint(\"copy failed; replaying from top\")\n",
      " \t\t\tvrle = copy.deepcopy(rle)\n",
      " \t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\ti=0\n",
      "@@ -229,17 +229,17 @@\n",
      " \t\t\t\tvrle.step(self.actionSeq[i])\n",
      " \t\t\t\tterminal, win = vrle._isDone()\n",
      " \t\t\t\ti += 1\n",
      "-\t\tprint '-------------------------------------'\n",
      "+\t\tprint('-------------------------------------')\n",
      " \t\tif len(self.actionSeq)>0:\n",
      "-\t\t\tprint [actionDict[a] for a in self.actionSeq]\n",
      "+\t\t\tprint([actionDict[a] for a in self.actionSeq])\n",
      " \t\tself.updateObjIDs(vrle)\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\tself.state = self.WBP.calculateAtoms(vrle)\n",
      " \t\tself.lastState = vrle\n",
      " \t\tself.win = win\n",
      " \t\tself.novelty = self.WBP.novelty(self, self.WBP.k, update=updateNoveltyDict)\n",
      " \t\tself.reward = vrle._game.score\n",
      "-\t\tprint 'novelty', self.novelty\n",
      "+\t\tprint('novelty', self.novelty)\n",
      " \t\t# raw_input(\"Press Enter to continue...\")\n",
      " \t\treturn win\n",
      " \n",
      "@@ -247,8 +247,8 @@\n",
      " \t\ti = 0\n",
      " \t\tfor objType in vrle._game.sprite_groups:\n",
      " \t\t\tfor s in vrle._game.sprite_groups[objType]:\n",
      "-\t\t\t\tif s.ID not in self.WBP.objIDs.keys():\n",
      "-\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(self.WBP.objIDs.keys())+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      "+\t\t\t\tif s.ID not in list(self.WBP.objIDs.keys()):\n",
      "+\t\t\t\t\tself.WBP.objIDs[s.ID] = (len(list(self.WBP.objIDs.keys()))+1) * (self.rle.outdim[0]*self.rle.outdim[1]+self.WBP.padding)\n",
      " \t\t\t\t\ti+=1\n",
      " \t\t# print \"updated {} objects\".format(i)\n",
      " \t\treturn\n",
      "@@ -263,12 +263,12 @@\n",
      " \t\tvrle = copy.deepcopy(self.rle)\n",
      " \t\tterminal = vrle._isDone()[0]\n",
      " \t\ti=0\n",
      "-\t\tprint vrle.show()\n",
      "+\t\tprint(vrle.show())\n",
      " \t\twhile not terminal:\n",
      " \t\t\ta = self.actionSeq[i]\n",
      "-\t\t\tprint actionDict[a]\n",
      "+\t\t\tprint(actionDict[a])\n",
      " \t\t\tvrle.step(a)\n",
      "-\t\t\tprint vrle.show()\n",
      "+\t\t\tprint(vrle.show())\n",
      " \t\t\t# vrle.step((0,0))\n",
      " \t\t\t# print vrle.show()\n",
      " \t\t\t# embed()\n",
      "@@ -332,10 +332,10 @@\n",
      " \tt1 = time.time()\n",
      " \tlast, visited, rejected = BFS(rle, p)\n",
      " \t# last, visited, rejected = BFS2(rle, p)\n",
      "-\tprint\n",
      "-\tprint 'time', time.time()-t1\n",
      "-\tprint 'visited', len(visited)\n",
      "-\tprint 'rejected', len(rejected)\n",
      "+\tprint()\n",
      "+\tprint('time', time.time()-t1)\n",
      "+\tprint('visited', len(visited))\n",
      "+\tprint('rejected', len(rejected))\n",
      " \t# if not hasattr(last, 'actionSeq'):\n",
      " \t# \tprint \"Failed without tracking tokens. re-trying\"\n",
      " \t# \tp.trackTokens = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to curses_example.py\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: curses_example.py\n",
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored parallel_planning.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: parallel_planning.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- parallel_planning.py\t(original)\n",
      "+++ parallel_planning.py\t(refactored)\n",
      "@@ -1,5 +1,5 @@\n",
      " from pathos.multiprocessing import ProcessingPool\n",
      "-from main_agent import Agent\n",
      "+from .main_agent import Agent\n",
      " import time\n",
      " import dill\n",
      " import os\n",
      "@@ -50,7 +50,7 @@\n",
      " \n",
      "         def gen_color():\n",
      "         \tfrom vgdl.colors import colorDict\n",
      "-        \tcolor_list = colorDict.values()\n",
      "+        \tcolor_list = list(colorDict.values())\n",
      "         \tcolor_list = [c for c in color_list if c not in ['UUWSWF']]\n",
      "         \tfor color in color_list:\n",
      "         \t\tyield color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: Refactored stateobsnonstatic.py\n",
      "RefactoringTool: Files that were modified:\n",
      "RefactoringTool: stateobsnonstatic.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- stateobsnonstatic.py\t(original)\n",
      "+++ stateobsnonstatic.py\t(refactored)\n",
      "@@ -9,9 +9,9 @@\n",
      " import pygame\n",
      " from pybrain.utilities import setAllArgs\n",
      " \n",
      "-from ontology import RotatingAvatar, BASEDIRS, GridPhysics, ShootAvatar, kill_effects\n",
      "-from core import Avatar\n",
      "-from tools import listRotate\n",
      "+from .ontology import RotatingAvatar, BASEDIRS, GridPhysics, ShootAvatar, kill_effects\n",
      "+from .core import Avatar\n",
      "+from .tools import listRotate\n",
      " from IPython import embed\n",
      " \n",
      " from collections import defaultdict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RefactoringTool: Skipping optional fixer: buffer\n",
      "RefactoringTool: Skipping optional fixer: idioms\n",
      "RefactoringTool: Skipping optional fixer: set_literal\n",
      "RefactoringTool: Skipping optional fixer: ws_comma\n",
      "RefactoringTool: No changes to colors.py\n",
      "RefactoringTool: Files that need to be modified:\n",
      "RefactoringTool: colors.py\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    call(['2to3', '-w', file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f1d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
